import json
import arxiv
import requests
from pathlib import Path
import os
import re
from dotenv import load_dotenv
from copy import deepcopy
from tqdm import tqdm

from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

from openai import OpenAI
import openai
from prompt import * 

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI()

class CitationLinker():
    def __init__ (self, config):
        self.target_id = config['arxiv_id']
        self.preprocess_threhsold = config['preprocess_threhsold']
        self.reference_ratio = config['reference_ratio']
        self.reference_condition = config['reference_condition']
        self.model = config['model']

        self.essay_dir = Path(config['essay_dir'])
        self.result_dir = Path(config['result_dir'])

        self.title = None
        self.authors = None
        self.submitted = None
        self.abstract = None
        self.pdf_url = None

        self.basic_keys = ["Title", "Authors", "Submitted" ,"Abstract"]
        self.references = ['References']
        
        self.content_config = config['content_keys']
        self.content_keys = [value_dict["name"] for _, value_dict in self.content_config.items()]
        
    @staticmethod
    def _create_directory_if_not_exists(directory_path):
        if directory_path.split("/")[-1] == "target-id":
            pass

        elif not os.path.exists(directory_path):
            os.makedirs(directory_path)
            print(f"ë””ë ‰í† ë¦¬ '{directory_path}'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            raise FileExistsError(f"ì—ëŸ¬: '{directory_path}' ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

    def _search_arxiv_pdf(self, arxiv_id):
        search = arxiv.Search(id_list=[arxiv_id])
        for result in search.results():
            break
        print(f"ğŸ“Œ Title: {result.title}")
        print(f"ğŸ“ Authors: {', '.join([author.name for author in result.authors])}")
        print(f"ğŸ“… Submitted: {result.published}")
        print(f"ğŸ”— PDF Link: {result.pdf_url}")
        print(f"ğŸ“ Abstract:\n{result.summary}")
        self.title = result.title
        self.authors = ', '.join([author.name for author in result.authors])
        self.submitted = str(result.published)
        self.abstract = result.summary
        self.pdf_url = result.pdf_url

    def _download_arxiv_pdf(self, pdf_url, save_path):
        response = requests.get(pdf_url, stream=True)
        if response.status_code == 200:
            with open(save_path, "wb") as file:
                for chunk in response.iter_content(chunk_size=1024):
                    file.write(chunk)
        print("ë…¼ë¬¸ ì €ì¥ ì™„ë£Œ!")

    def _fetch_arxiv_paper(self, title, max_results=30):
        
        search = arxiv.Search(
            query=title,
            max_results=max_results, 
            sort_by=arxiv.SortCriterion.Relevance
        )

        for result in search.results():
            if title[10:-10].lower().replace(" ", "") in result.title.lower().replace(" ", ""):
                return ( {
                    "title": result.title,
                    "abstract": result.summary,
                    "pdf_url": result.pdf_url
                })
            
        return None 
    
    @staticmethod
    def _message_to_openai(message, model):
        response = client.chat.completions.create(
            model=model,
            store=True,
            messages=[{"role": "user", "content": message}],
            temperature=0.5
        )
        return response
    
    def _search_and_download_essay(self, arxiv_id):
        self._search_arxiv_pdf(arxiv_id=arxiv_id)
        self._download_arxiv_pdf(
            pdf_url=self.pdf_url, 
            save_path=self.essay_dir / f"0-{self.title}.pdf"
        )
    def _preprocess(self, save_path):
        # ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ì„¹ì…˜ ë³„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
        loader = UnstructuredPDFLoader(save_path)
        documents = loader.load()
        processed_output = {}
        for key, value_dict in self.content_config.items():
            if value_dict['name'] == "Title":
                processed_output[value_dict['name']]=self.title
            elif value_dict['name'] == "Authors":
                processed_output[value_dict['name']]=self.authors
            elif value_dict['name'] == "Submitted":
                processed_output[value_dict['name']]=self.submitted
            elif value_dict['name'] == "Abstract":    
                processed_output[value_dict['name']]=self.abstract
            else:
                processed_output[value_dict['name']]=documents[0].page_content.split(value_dict['deliminators']['forward'])[-1].split(value_dict['deliminators']['backward'])[0]

        # basic_keyê°€ ì•„ë‹Œ ì„¹ì…˜ ì¤‘ threshold ë¯¸ë§Œìœ¼ë¡œ ì˜ë¦¬ë©´ ëª¨ë‘ ì—†ì•±ë‹ˆë‹¤.
        threshold = self.preprocess_threhsold
        for key in self.content_keys:
            if key not in self.basic_keys:
                result = []
                for text in processed_output[key].split("\n"):
                    if len(text) >= threshold:
                        result.append(text)
                processed_output[key] = "\n".join(result)
        
        # 2000ì ë‹¨ìœ„ë¡œ ëª¨ë‘ ìë¦…ë‹ˆë‹¤.
        text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=2000, 
                chunk_overlap=0  
            )
        
        for key in self.content_keys:
            documents = text_splitter.create_documents([processed_output[key]])
            for doc in documents:
                doc.metadata = {"Title": self.title, "Key": key} 
            processed_output[key] = documents

        return processed_output

    def _basic_summarize(self, basic_summarize_template, processed_output):
        # ê¸°ë³¸ ìš”ì•½
        essay = ""
        for key in self.content_keys:
            if key not in self.references:
                for doc in processed_output[key]:
                    essay += doc.page_content + "\n\n"
        basic_summarize_message = basic_summarize_template.format(essay=essay)
        response = CitationLinker._message_to_openai(message=basic_summarize_message, model=self.model)
        return response.choices[0].message.content
    
    def _reference_preprocess(self, reference_extraction_template, processed_output):
        reference_extraction_message = reference_extraction_template.format(references=processed_output['References'])
        flag = True
        while flag:
            response = CitationLinker._message_to_openai(message=reference_extraction_message, model=self.model)
            text = response.choices[0].message.content
            text = re.sub("```json","",text)
            text = re.sub("```","",text)
            json_data = json.loads(text)
            flag = False
            
        reference_dict = {}
        for key, dict_data in json_data.items():
            dict_data['Counter'] = 0
            dict_data['Context'] = []
            reference_dict[key] = dict_data
        processed_output['References'] = deepcopy(reference_dict)
        return processed_output, reference_dict
    
    def _reference_counting(self, reference_count_template_dict, processed_output, reference_dict):
        for index in range(len(reference_count_template_dict)):
            result = []
            for key in self.content_keys:
                if key not in self.basic_keys + self.references:
                    for essay in processed_output[key]:
                        reference_count_message = reference_count_template_dict[str(index)].format(references=reference_dict, essay=essay, condition=self.reference_condition)
                        response = CitationLinker._message_to_openai(reference_count_message, model=self.model)
                        try:
                            text = response.choices[0].message.content
                            text = re.sub("```json","",text)
                            text = re.sub("```","",text)
                            text_data = json.loads(text)
                            result.append(text_data)
                        except: 
                            text_data = None
                        # items['References'] = text_data

            for data in result:
                for key, value_dict in data.items():
                    try:
                        processed_output["References"][key]['Counter'] += value_dict['Counter']
                        processed_output["References"][key]['Context'].extend(value_dict['Context'])
                    except:
                        pass
        return processed_output
    
    def _download_reference(self, processed_output, nums = 5):
        # counter ìˆœìœ¼ë¡œ ì •ë ¬ì„ í•œë‹¤
        # titleì„ ë˜ì§„ë‹¤
        # ê°œìˆ˜ê°€ ë§ìœ¼ë©´ ë©ˆì¶˜ë‹¤
        related_reference = processed_output['References']
        related_reference = dict(sorted(related_reference.items(), key=lambda item: item[1]["Counter"], reverse=True))
        processed_output['References'] = related_reference
        ordered_titles = list(processed_output['References'].items())
        downloads_lst = []

        for index, valud_dict in ordered_titles:
            title = valud_dict['Title']
            try :
                paper_info = self._fetch_arxiv_paper(title)
                if paper_info is None:
                    paper_info = self._fetch_arxiv_paper(title, 150)
            except Exception as e:
                print(index,"ë²ˆì§¸ ë…¼ë¬¸ ì˜ˆì™¸ ë°œìƒ: ", e)
                try :
                    paper_info = self._fetch_arxiv_paper(title, None)
                except:
                    paper_info = None

            if paper_info is not None:
                pdf_url = paper_info['pdf_url']
                abstract = paper_info['abstract']
                processed_output['References'][index]['abstract'] = abstract
                processed_output['References'][index]['pdf_url'] = pdf_url
                save_path = self.essay_dir / (index+ "-" + paper_info['title']+".pdf")
                self._download_arxiv_pdf(pdf_url, save_path)
                print(index,"ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

                downloads_lst.append((index, valud_dict))
            
            else:
                pdf_url = None
                abstract = None
                processed_output['References'][index]['abstract'] = abstract
                processed_output['References'][index]['pdf_url'] = pdf_url
                print(index,"ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                print(f"    {processed_output['References'][index]['Title']}")
            
            if len(downloads_lst) == nums:
                break
            
        # ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ í›„, ì§ˆë¬¸ ì¶•ì†Œ
        # ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” ë™ì¼ ì§ˆë¬¸ì„ ì—¬ëŸ¬ê°œ ê°–ê³  ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        
        related_reference = dict(downloads_lst)
        total_related_reference = processed_output['References']
        return related_reference, total_related_reference, processed_output
    
    def _reduce_questions(self, question_reduction_template, related_reference):
        for index in related_reference.keys():
            query_list = related_reference[index]['Context']
            user_message = question_reduction_template.format(text_list=query_list)
            response = CitationLinker._message_to_openai(user_message, model=self.model)
            related_reference[index]['Questions'] = response.choices[0].message.content
        return related_reference
    
    def _find_connection_from_reference(self, reference_qna_template, related_reference):

        for index in tqdm(related_reference.keys(), desc="ì¸ìš© ë…¼ë¬¸ê³¼ì˜ ê´€ë ¨ ì§€ì  ì •ë¦¬..."):
            title = related_reference[index]['Title']
            questions = related_reference[index]['Questions']
        
            for path in self.essay_dir.rglob("*.pdf"):
                if path.name.split("-")[0] == index:
                    break
        
            loader = UnstructuredPDFLoader(path)
            documents = loader.load()
            essay = documents[0].page_content
            essay = "\n".join([text for text in essay.split("\n") if len(text) >= self.preprocess_threhsold])
            reference_qna_message = reference_qna_template.format(essay = essay, questions=questions, title=title)
            response = CitationLinker._message_to_openai(reference_qna_message, model=self.model)
            summary = response.choices[0].message.content
            related_reference[index]['Summary'] = summary
            
        return related_reference
    
    def forward(self):

        # ë…¼ë¬¸ idë¥¼ ë°›ì•„ì„œ ë…¼ë¬¸ì„ ë‹¤ìš´ ë°›ìŠµë‹ˆë‹¤.
        # ì¶”í›„ì— ë…¼ë¬¸ pdfë¥¼ drag and drop ë°©ì‹ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤
        self._search_and_download_essay(
            arxiv_id=self.target_id,
        )

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_output = self._preprocess(
            save_path=self.essay_dir / f"0-{self.title}.pdf"
        )
        print("step 1: ", "\n",
            processed_output)

        # ê¸°ë³¸ ìš”ì•½
        response=self._basic_summarize(
            basic_summarize_template=basic_summarize_template,
            processed_output=processed_output
        )
        print("step 2: ", "\n",
            response, "\n")

        # ê¸°ë³¸ ìš”ì•½ëœ ì •ë³´ ì €ì¥
        with open(self.result_dir/"basic_summary.json", 'w', encoding="utf-8") as f:
            json.dump(response, f, ensure_ascii=False, indent=4)

        # ì°¸ê³ ë¬¸í—Œ ëª©ë¡í™”
        processed_output, reference_dict = self._reference_preprocess(
            reference_extraction_template=reference_extraction_template,
            processed_output=processed_output
        )
        print("step 3: ", "\n", 
            processed_output, "\n",
            reference_dict, "\n" 
        )
        # ì¸ìš©íšŸìˆ˜ counting
        # reference_count_template_dict / processed_output / reference_dict
        processed_output = self._reference_counting(
            reference_count_template_dict=reference_count_template_dict, 
            processed_output=processed_output, 
            reference_dict=reference_dict
            )
        print("step 4: ", "\n",
            processed_output, "\n")
        # reference ë…¼ë¬¸ ë‹¤ìš´ ë°›ì•„ì˜¤ê¸°
        # ì „ëµ ì²˜ìŒì—” 30ê°œ ì¤‘ì— ë‹¤ìš´ì„ ë°›ìŠµë‹ˆë‹¤.
        # ê·¸ ë‹¤ìŒ 150ê°œë¥¼ ë°›ìŠµë‹ˆë‹¤. 
        # ê·¸ ë‹¤ìŒ default ê°’ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
        # ê·¸ëŸ¼ì—ë„ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ì±„ì›Œë„£ìŠµë‹ˆë‹¤. 
        related_reference, total_related_reference, processed_output = self._download_reference(processed_output=processed_output)
        print("step 5: ", "\n",
            related_reference, "\n", 
            total_related_reference, "\n",
            processed_output, "\n",
        )

        with open(self.result_dir/"reference_count.json", 'w', encoding="utf-8") as f:
            json.dump(total_related_reference, f, ensure_ascii=False, indent=4)

        # # ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ í›„, ì§ˆë¬¸ ì¶•ì†Œ
        # # ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” ë™ì¼ ì§ˆë¬¸ì„ ì—¬ëŸ¬ê°œ ê°–ê³  ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        related_reference = self._reduce_questions(
            question_reduction_template=question_reduction_template,
            related_reference=related_reference
        )
        print("step 6: ", "\n",
            related_reference, "\n", 
        )
        # referenceì™€ì˜ ì ‘ì ì„ ì°¾ê¸° ìœ„í•œ ìš”ì•½
        # ì¸ìš© ë…¼ë¬¸ê³¼ ì›ë˜ ë…¼ë¬¸ì˜ ì ‘ì ì„ ì°¾ê³  ì •ë¦¬í•©ë‹ˆë‹¤.
        # ì›ë˜ ë…¼ë¬¸ì´ ì–´ë–»ê²Œ ì—°êµ¬ë¥¼ ë°œì „ì‹œí‚¤ëŠ”ì§€ê¹Œì§€ ì •ë¦¬í•©ë‹ˆë‹¤. 
        related_reference=self._find_connection_from_reference(
            reference_qna_template=reference_qna_template,
            # research_progress_template=research_progress_template,
            # processed_output=processed_output,
            related_reference=related_reference
        )
        print("step 7: ", "\n",
            related_reference, "\n", 
        )
        with open(self.result_dir/"reference_qna.json", 'w', encoding="utf-8") as f:
            json.dump(related_reference, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":

    with open("archive/config.json",'r') as f:
        config = json.load(f)

    citation_linker = CitationLinker(config)
    citation_linker.forward()