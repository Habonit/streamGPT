import sys
import streamlit as st
import time
from pathlib import Path
import json
import pandas as pd
import re
import os
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from konlpy.tag import Okt
import spacy
import seaborn as sns
import matplotlib.ticker as mticker

from langchain_openai import ChatOpenAI
from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import Chroma, FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler

sys.path.append(str(Path(__file__).resolve().parent.parent))
from citationlinker import CitationLinker  
from prompt import * 

st.set_page_config(page_title="ğŸ“ƒ CitationLinkerGPT", page_icon="ğŸ“ƒ")

@st.cache_resource(show_spinner='Embedding file....')
def embed_file(file_path):
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file_path}")
    splitter = CharacterTextSplitter(
        separator = '\n',
        chunk_size = 600,
        chunk_overlap= 50,
    )

    # í•´ë‹¹ loaderëŠ” pdf, txt, docxì™€ ëª¨ë‘ í˜¸í™˜ë©ë‹ˆë‹¤.
    # ì—¬ê¸°ê°€ íŠ¹í™” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
        embeddings, cache_dir
    )
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={'k': 20})
    return retriever

def save_message(message, role):
    st.session_state['messages'].append({'message':message,'role':role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

def paint_history():
    for message in st.session_state['messages']:
        send_message(message['message'], message['role'], save=False)

def file_exists(file_path):
    return os.path.isfile(file_path)

def load_config():
    config_path = "config/config.json"
    if Path(config_path).exists():
        with open(config_path, "r") as f:
            return json.load(f)
    return {
        "arxiv_id": "",
        "essay_dir": "reference",
        "model": "gpt-4o-mini",
        "preprocess_threhsold": 25,
        "reference_ratio": 0.2,
        "reference_condition": "- ì¸ìš© í‘œì‹œ: ë…¼ë¬¸ ë³¸ë¬¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì´ **[ìˆ«ì]** í˜•ì‹ ([5], [27] ë“±)ìœ¼ë¡œ ì¸ìš©ëœ ê²½ìš°ë§Œ ì¶”ì¶œí•˜ì„¸ìš”",
        "result_dir": "result",
        "content_keys": {}
    }

def load_policy_config():
    config_path = "config/config-policy.json"
    if Path(config_path).exists():
        with open(config_path, "r") as f:
            return json.load(f)

def save_config(config, path):
    with open(path, "w") as f:
        json.dump(config, f, ensure_ascii=False, indent=4)

def load_json(file_path):
    if Path(file_path).exists():
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        return None

def measure_time(func, *args, **kwargs):
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    elapsed_time = end_time - start_time
    return result, elapsed_time

def generate_wordcloud(text):
    wordcloud = WordCloud(
        font_path="font/NanumGothic-Regular.ttf",
        width=900,
        height=450,
        max_words=100,  # í‘œì‹œí•  ìµœëŒ€ ë‹¨ì–´ ìˆ˜
        background_color="white",  # ë°°ê²½ìƒ‰ ì„¤ì •
        colormap="Set2",  # ì»¬ëŸ¬ë§µ ì ìš©
        relative_scaling=0.3,  # ê¸€ì í¬ê¸° ì¡°ì •
        contour_color="steelblue",  # í…Œë‘ë¦¬ ìƒ‰ìƒ
        contour_width=2  # í…Œë‘ë¦¬ ë‘ê»˜
    ).generate(text)

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation="bilinear")
    ax.axis("off") 
    return fig

def extract_nouns(text):
    """
    ì…ë ¥ëœ í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€ ë° ì˜ì–´ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    # í•œê¸€ ëª…ì‚¬ ì¶”ì¶œ
    korean_nouns = okt.nouns(text)
    korean_nouns = [noun for noun in korean_nouns if len(noun)>=2]

    # ì˜ì–´ ë‹¨ì–´ë§Œ í•„í„°ë§ (ì •ê·œì‹)
    english_text = " ".join(re.findall(r"[a-zA-Z]+", text))

    # ì˜ì–´ ëª…ì‚¬ ì¶”ì¶œ
    doc = nlp(english_text)
    english_nouns = [token.text for token in doc if token.pos_ == "NOUN"]

    # í•œê¸€ê³¼ ì˜ì–´ ëª…ì‚¬ ê²°í•©
    nouns = korean_nouns + english_nouns
    return nouns

from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_huggingface.llms import HuggingFacePipeline
from langchain.schema import LLMResult
from pydantic import Field
from typing import List, Optional
import torch

# ì•„ë˜ í´ë˜ìŠ¤ëŠ” í—ˆê¹…í˜ì´ìŠ¤ì˜ ëª¨ë¸ ì‚¬ìš©ë²•ì„ ê·¸ëŒ€ë¡œ ì°¸ì¡°í•˜ì—¬ í´ë˜ìŠ¤í™” ì‹œí‚¨ ê²ƒì…ë‹ˆë‹¤.
class KoreanLlamaPipeline:
    def __init__(self, model, tokenizer, max_new_tokens=1024, temperature=0.6, top_p=0.9):
        self.model = model
        self.tokenizer = tokenizer
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p

    def __call__(self, instruction, **kwargs):
        messages = [{"role": "user", "content": instruction}]
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.model.device)

        terminators = [
            self.tokenizer.convert_tokens_to_ids("<|end_of_text|>"),
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        outputs = self.model.generate(
            input_ids,
            max_new_tokens=self.max_new_tokens,
            eos_token_id=terminators,
            do_sample=True,
            temperature=self.temperature,
            top_p=self.top_p,
            **kwargs
        )

        # Decode the output
        return self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)

# ì•„ë˜ëŠ” ì•„ë¬´ë¦¬ í•´ë„ ë¬¸ì„œë¥¼ ì œëŒ€ë¡œ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ chat gptì—ê²Œ ì½”ë“œë¥¼ ì§œë‹¬ë¼ê³  í–ˆìŠµë‹ˆë‹¤.
# langchainê³¼ huggingface ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—°ê²°í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ì—¬ì„œ huggingfacepipelineì„ ìƒì†ë°›ì•„ì•¼ í•œë‹¤ê³  ì „ë‹¬ë°›ì•˜ìŠµë‹ˆë‹¤.
# ë°˜ë“œì‹œ _generate í•¨ìˆ˜ì™€ stop í† í°ì„ ì²˜ë¦¬í•˜ëŠ” ì•„ë˜ì™€ ê°™ì€ ë¡œì§ì´ í•„ìš”í•˜ë‹¤ê³  ì „ë‹¬ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.
# ë‹¤ë§Œ ì œê°€ êµ¬í˜„í•œ ê²ƒì´ ì•„ë‹ˆë¼ í—¨ë“¤ë§ì„ ì˜ í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤.
class KoreanLlamaLangChainLLM(HuggingFacePipeline):
    pipeline: KoreanLlamaPipeline = Field()

    def _generate(self, prompt: str, stop: Optional[List[str]] = None) -> LLMResult:
        response = self.pipeline(prompt)
        if stop:
            for token in stop:
                response = response.split(token)[0]
        return LLMResult(generations=[[{"text": response}]])

okt = Okt()  
nlp = spacy.load("en_core_web_sm") 

config = load_config()
config['arxiv_id'] = st.sidebar.text_input("ğŸ” arXiv ë…¼ë¬¸ ID ì…ë ¥", config.get("arxiv_id", ""))
config['essay_dir'] = f"reference/{config['arxiv_id'].replace('.','')}"
config['result_dir'] = f"result/{config['arxiv_id'].replace('.','')}"
config['model'] = st.sidebar.selectbox("ğŸ§  ì‚¬ìš©í•  ëª¨ë¸", ["gpt-4o-mini", "gpt-4", "gpt-3.5-turbo"], index=["gpt-4o-mini", "gpt-4", "gpt-3.5-turbo"].index(config.get("model", "gpt-4o-mini")))
config['preprocess_threhsold'] = st.sidebar.slider("ğŸ“ ìµœì†Œ ë¬¸ì¥ ê¸¸ì´", 10, 100, config.get("preprocess_threhsold", 25))
config['reference_ratio'] = st.sidebar.slider("ğŸ“Š ì°¸ê³ ë¬¸í—Œ ë¶„ì„ ë¹„ìœ¨", 0.1, 1.0, config.get("reference_ratio", 0.2))
config['reference_condition'] = st.sidebar.text_area("ğŸ”— ì°¸ê³ ë¬¸í—Œ ì¸ìš© ì¡°ê±´", config.get("reference_condition", ""))

content_keys_dict = config.get("content_keys", {})
content_keys_list = []
for key, value in content_keys_dict.items():
    content_keys_list.append({"ID": key, "name": value["name"], "forward_deliminators": value["deliminators"]["forward"], "backward_deliminators": value["deliminators"]["backward"]})

content_keys_df = pd.DataFrame(content_keys_list)
edited_df = st.sidebar.data_editor(content_keys_df, num_rows="dynamic")
new_content_keys = {}
for i, row in edited_df.iterrows():
    new_content_keys[str(i+1)] = {
        "name": row["name"],
        "deliminators": {
            "forward": row["forward_deliminators"],
            "backward": row["backward_deliminators"]
        }
    }
config['content_keys'] = new_content_keys

st.title("ğŸ“ƒ CitationLinkerGPT")
st.sidebar.header("Config Setting")
st.sidebar.subheader("ğŸ“‘ ë…¼ë¬¸ ì„¹ì…˜ ì„¤ì •")

analyze_essay_tab, basic_review_tab, chatbot_tab = st.tabs(['Analyze Essay', 'Review based on References', "CitationGPT"])

if st.sidebar.button("ğŸ’¾ ì„¤ì • ì €ì¥"):
    config_path = f"config/config-{config['arxiv_id'].replace('.','')}.json"
    if file_exists(config_path):
        st.sidebar.warning("âš ï¸ ì´ë¯¸ ë¶„ì„ì´ ëë‚œ ë…¼ë¬¸ì…ë‹ˆë‹¤. 'í™•ì¸ íƒ­'ì—ì„œ arXiv IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    else:
        CitationLinker._create_directory_if_not_exists(config['essay_dir'])
        CitationLinker._create_directory_if_not_exists(config['result_dir'])
        save_config(config, path=config_path)
        st.sidebar.success("ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

with analyze_essay_tab:
    if st.button("ğŸš€ ë…¼ë¬¸ ë¶„ì„ ì‹œì‘"):
        if not config['arxiv_id']:
            st.warning("â— ë…¼ë¬¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”!")
        else:
            citation_linker = CitationLinker(config)

            with st.status("ë…¼ë¬¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...", expanded=True) as status:
                try:
                    st.write("ğŸ“¥ **1ë‹¨ê³„: ë…¼ë¬¸ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ ì¤‘...**")
                    _, elapsed_time = measure_time(
                        citation_linker._search_and_download_essay,
                        arxiv_id=config['arxiv_id']
                    )
                    # To Do: ì¢…ë£Œ í›„ citation_linker.title / citation_linker.authorsì˜ ê°’ì´ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í•˜ë‹¨ì— í‘œì‹œë˜ì–´ì•¼ í•¨í•¨
                    st.success(f"âœ… ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

                    st.write("ğŸ“ƒ **2ë‹¨ê³„: ë…¼ë¬¸ ì „ì²˜ë¦¬ ì§„í–‰ ì¤‘...**")
                    processed_output, elapsed_time = measure_time(
                        citation_linker._preprocess,
                        save_path=citation_linker.essay_dir / f"0-{citation_linker.title}.pdf"
                    )
                    # To Do: config['preprocess_threhsold'] ì´í•˜ì˜ textëŠ” ëª¨ë‘ ì‚­ì œë˜ì—ˆìŒì„ ë…¼ë¬¸ ì „ì²˜ë¦¬ ì™„ë£Œ í•˜ë‹¨ì— í‘œì‹œ
                    st.success(f"âœ… ë…¼ë¬¸ ì „ì²˜ë¦¬ ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

                    st.write("ğŸ“ **3ë‹¨ê³„: ë…¼ë¬¸ ìš”ì•½ ì§„í–‰ ì¤‘...**")
                    summary, elapsed_time = measure_time(
                        citation_linker._basic_summarize,
                        basic_summarize_template=basic_summarize_template,
                        processed_output=processed_output
                    )
                    with open(citation_linker.result_dir / "basic_summary.json", 'w', encoding="utf-8") as f:
                        json.dump(summary, f, ensure_ascii=False, indent=4)
                    # To Do: Summaryì˜ print ê²°ê³¼ë¥¼ Toggleë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆì–´ì•¼ í•¨
                    st.success(f"âœ… ë…¼ë¬¸ ìš”ì•½ ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

                    st.write("ğŸ“‘ **4ë‹¨ê³„: ì°¸ê³ ë¬¸í—Œ ëª©ë¡í™” ì¤‘...**")
                    (processed_output, reference_dict), elapsed_time = measure_time(
                        citation_linker._reference_preprocess,
                        reference_extraction_template=reference_extraction_template,
                        processed_output=processed_output
                    )
                    # To Do: reference_dictì˜ key, title, authorë¥¼ í‘œ í˜•íƒœë¡œ ë³´ì—¬ì¤˜ì•¼ í•˜ê³  ê²°ê³¼ë¥¼ Toggleë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆì–´ì•¼ í•¨
                    st.success(f"âœ… ì°¸ê³ ë¬¸í—Œ ëª©ë¡í™” ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

                    st.write("ğŸ“‘ **5ë‹¨ê³„: ì°¸ê³ ë¬¸í—Œ ì¸ìš© íšŸìˆ˜ ì„¸ëŠ” ì¤‘...**")
                    processed_output, elapsed_time = measure_time(
                        citation_linker._reference_counting,
                        reference_count_template_dict=reference_count_template_dict,
                        processed_output=processed_output,
                        reference_dict=reference_dict
                    )
                    
                    st.success(f"âœ… íšŸìˆ˜ ì„¸ê¸° ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

                    st.write("ğŸ“Š **6ë‹¨ê³„: ì¸ìš© ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì¤‘...**")
                    (related_reference, total_related_reference, processed_output), elapsed_time = measure_time(
                        citation_linker._download_reference,
                        processed_output=processed_output
                    )
                    
                    with open(citation_linker.result_dir / "reference_count.json", 'w', encoding="utf-8") as f:
                        json.dump(total_related_reference, f, ensure_ascii=False, indent=4)
                    st.success(f"âœ… ì¸ìš© ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

                    st.write("ğŸ“Š **7ë‹¨ê³„: ì¸ìš© ë…¼ë¬¸ê³¼ì˜ ì ‘ì  ì„¸ë°€í™” ì¤‘...**")
                    related_reference, elapsed_time = measure_time(
                        citation_linker._reduce_questions,
                        question_reduction_template=question_reduction_template,
                        related_reference=related_reference
                    )
                    st.success(f"âœ… ì¸ìš© ë…¼ë¬¸ê³¼ì˜ ì ‘ì  ì„¸ë°€í™” ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

                    st.write("ğŸ“Š **8ë‹¨ê³„: ì—°êµ¬ í™•ì¥ì„± íŒŒì•… ì¤‘...**")
                    related_reference, elapsed_time = measure_time(
                        citation_linker._find_connection_from_reference,
                        reference_qna_template=reference_qna_template,
                        # research_progress_template=research_progress_template,
                        # processed_output=processed_output,
                        related_reference=related_reference
                    )
                    with open(citation_linker.result_dir / "reference_qna.json", 'w', encoding="utf-8") as f:
                        json.dump(related_reference, f, ensure_ascii=False, indent=4)
                    # To Do: related_references.items()ë¥¼ forë¬¸ ëŒë¦¬ë©´ì„œ value['summary']ì™€  value['summary_qna]'ê²°ê³¼ë¥¼ ëª¨ë‘ printí•˜ì—¬ì—¬ toggleë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆì–´ì•¼ í•¨.
                    st.success(f"âœ… ì—°êµ¬ í™•ì¥ì„± ë¶„ì„ ì™„ë£Œ! ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                    
                    status.update(label="ğŸ‰ ë…¼ë¬¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", state="complete")
                    # Check Review íƒ­ìœ¼ë¡œ ì´ë™ ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€
                    st.info(f"âœ… ì „ì²´ ë…¼ë¬¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'Check Review' íƒ­ì— {config['arxiv_id']}ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                except Exception as e:
                    status.update(label=f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", state="error")

with basic_review_tab:
    # ì‚¬ìš©ì ì…ë ¥ (ê¸°ë³¸ê°’: config['arxiv_id'])
    arxiv_id = st.text_input("ğŸ“Œ arXiv ë…¼ë¬¸ ID ì…ë ¥", value=config.get("arxiv_id", ""))
    arxiv_id = arxiv_id.replace('.','')
    # ë²„íŠ¼ í´ë¦­ ì‹œ JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    if st.button("ğŸ” ê¸°ë³¸ ë¦¬ë·° í™•ì¸"):
        basic_summary_path = Path("result") / arxiv_id / "basic_summary.json"
        basic_summary = load_json(basic_summary_path)
        basic_summary_json = {
            1: basic_summary.split("###")[1],
            2 : basic_summary.split("###")[2],
            3 : basic_summary.split("###")[3],
            4 : basic_summary.split("###")[4],
            5: basic_summary.split("###")[5],
        }
        basic_1 = "###" + basic_summary_json[1]
        basic_2 = "###" + "###".join([basic_summary_json[i] for i in range(2, len(basic_summary_json)+1)])
        nouns = extract_nouns(basic_summary)

        reference_count_path = Path("result") / arxiv_id / "reference_count.json"
        reference_qna_path = Path("result") / arxiv_id / "reference_qna.json"

        reference_count = load_json(reference_count_path)
        reference_qna = load_json(reference_qna_path)
        data_dict = {}
        for i, (key, value_dict) in enumerate(reference_count.items()):
            data_dict[i] = {
                "Title":value_dict["Title"],
                "Counter":value_dict["Counter"],
                "Index":key
            }
        reference_connection_dict = {}
        for key, value_dict in reference_qna.items():
            reference_connection_dict[key] = {
                "Title" : value_dict["Title"], 
                "Index" : key,
                "reference_connection" : value_dict["Summary"]}

        df = pd.DataFrame(data_dict).T
        df["Counter"] = df["Counter"].apply(lambda x: round(x/3, 3))
        max_value = df.iloc[0]["Counter"]
        df['Score'] = (df["Counter"] / max_value).round(2) 

        nums = len(reference_qna)

        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“– ë…¼ë¬¸ ìš”ì•½")
            st.write(basic_1)

        with col2:
            st.subheader("ğŸ¨ ì›Œë“œ í´ë¼ìš°ë“œ")
            st.pyplot(generate_wordcloud(" ".join(nouns)))

        st.subheader("ğŸ” ì—°êµ¬ ì •ë³´")
        st.write(basic_2)
        
        # ğŸŒŸ ë°ì´í„° í…Œì´ë¸” í‘œì‹œ
        sns.set_theme(style="whitegrid")

        fig, ax = plt.subplots(figsize=(16, 9))
        sns.barplot(x="Index", y="Score", data=df.iloc[:nums], palette="coolwarm", ax=ax)
        ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f"{x:.2f}"))
        ax.set_xticklabels(ax.get_xticklabels(), fontsize=11)
        ax.set_yticklabels(ax.get_yticks(), fontsize=11)

        # ë¼ë²¨ ì¶”ê°€
        ax.set_xlabel("Title Index", fontsize=13, fontweight='bold')
        ax.set_ylabel("Citation Score", fontsize=13, fontweight='bold')
        ax.set_title("Citation Score of References", fontsize=15, fontweight='bold')

        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ (ìˆ«ì)
        for p in ax.patches:
            ax.annotate(f"{p.get_height():.2f}",
                        (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha='center', va='bottom', fontsize=10, fontweight="bold", color="black")
            
        st.markdown("### ğŸ“Š ë°ì´í„° í…Œì´ë¸”")
        st.dataframe(df, use_container_width=True, height=400)
        st.markdown("### ğŸ“‰ ì¸ìš© ì ìˆ˜ Plot")
        st.pyplot(fig, use_container_width=True)
        st.subheader("ğŸ“Œ ì°¸ê³  ë¬¸í—Œê³¼ì˜ ì ‘ì  ì •ë¦¬")
        for index, content in reference_connection_dict.items():
            with st.expander(f"ğŸ“–(Index: {index}) {content['Title']}", expanded=False):
                st.write(f"**Reference Summary:**")
                st.text_area("Reference Details", content["reference_connection"], height=300)

with chatbot_tab:
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "retriever" not in st.session_state:
        st.session_state["retriever"] = None
    if "model" not in st.session_state:
        st.session_state["model"] = None

    # arXiv ID ì…ë ¥
    arxiv_id = st.text_input("ğŸ“ŒarXiv ë…¼ë¬¸ ID ì…ë ¥", value=config.get("arxiv_id", ""))
    arxiv_id = arxiv_id.replace('.', '')

    # ì±—ë´‡ ì‘ë‹µ ì •ì±… ì„ íƒ

    # ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ
    basic_summary_path = Path("result") / arxiv_id / "basic_summary.json"
    basic_summary = load_json(basic_summary_path)

    reference_qna_path = Path("result") / arxiv_id / "reference_qna.json"
    reference_qna = load_json(reference_qna_path)

    # ë…¼ë¬¸ íŒŒì¼ í™•ì¸ í›„ ì„ë² ë”© ì²˜ë¦¬
    target_essay = Path("reference") / arxiv_id
    pdf_files_path = list(target_essay.glob("0*.pdf"))

    if not pdf_files_path:
        st.error("ğŸ“‚ ë…¼ë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    
    if st.session_state["retriever"] is None:
        st.session_state["retriever"] = embed_file(pdf_files_path[0])
    
    retriever = st.session_state["retriever"]

    if not basic_summary or not reference_qna:
        st.error("ğŸ“‚ í•„ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë…¼ë¬¸ IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    # ìš”ì•½ í…ìŠ¤íŠ¸ ì •ë¦¬
    merged_text = "\n".join(
        f"{value_dict['Summary']}"
        for value_dict in reference_qna.values()
    )
    policy_config = load_policy_config()
    degree = policy_config['chatbot']

    # ì±—ë´‡ ì‘ë‹µì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if degree == "high":
        portion = len(basic_summary)
        prompt_text = basic_summary + "\n" + merged_text[:portion*5]
    elif degree == "middle":
        portion = len(basic_summary)
        prompt_text = basic_summary + "\n" + merged_text[:portion*3]
    else:
        prompt_text = ""

    if st.session_state["model"] is None:

        if degree in ['high', 'middle']:
            llm = ChatOpenAI(temperature=0.1, streaming=True)

        else:
            # ë¡œì»¬ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ë¬¸ì¥ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤.
            model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'

            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
            )
            model = model.to('cuda')

            custom_pipeline = KoreanLlamaPipeline(model, tokenizer)
            llm = KoreanLlamaLangChainLLM(pipeline=custom_pipeline)
        st.session_state["model"] = llm
    else: 
        llm = st.session_state["model"]

    # ChatPromptTemplate êµ¬ì„±
    prompt = ChatPromptTemplate.from_messages([
        ('system','''ë‹¹ì‹ ì€ ai ë…¼ë¬¸ì— ëŒ€í•˜ì—¬ ìˆ˜ì—…í•˜ëŠ” êµìˆ˜ë‹˜ì…ë‹ˆë‹¤. 
        ì•„ë˜ëŠ” AIë¥¼ ê³µë¶€í•˜ëŠ” í•™ìƒê³¼ ëŒ€í™”í•˜ëŠ” ìƒí™©ì„ ì„¤ê³„í•˜ì˜€ìŠµë‹ˆë‹¤.
        ì„¸ ê°€ì§€ ì •ë³´ê°€ ì œê³µì´ ë©ë‹ˆë‹¤.
        1. í•™ìƒê³¼ ì´ì „ì— ëŒ€í™”í–ˆë˜ ë‚´ìš©.
        2. ë…¼ë¬¸ì˜ ìš”ì•½ë³¸
        3. ë…¼ë¬¸ì˜ ë³¸ë¬¸ ì¼ë¶€
         
        í•™ìƒì´ aiì— ê´€í•œ ì§ˆë¬¸ì„ í–ˆë‹¤ë©´ ì ˆëŒ€ë¡œ ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ê³  ë°˜ë“œì‹œ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìš”ì•½ë¬¸ê³¼ ë³¸ë¬¸ ì¼ë¶€ì—ì„œ ë‹µì„ ì°¾ì•„ì£¼ì„¸ìš”.
        í•™ìƒì´ aiì— ê´€í•œ ì§ˆë¬¸ì„ í–ˆëŠ”ë° ì´ì „ ëŒ€í™” ë‚´ìš©, ìš”ì•½ë¬¸, ë³¸ë¬¸ì—ì„œ ëŒ€ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ êµìˆ˜ë‹˜ë„ ì˜ ëª¨ë¥´ê² ë‹¤ëŠ” ì–´íˆ¬ë¡œ ëŒ€ë‹µì„ í•´ì£¼ì„¸ìš”.
        
        í•™ìƒì´ aiì™€ ê´€ë ¨ì´ ì—†ëŠ” ì¼ìƒì ì¸ ì§ˆë¬¸ì„ í•œë‹¤ë©´, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ììœ ë¡­ê²Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. ê·¸ëŸ¬ë©´ì„œ ìˆ˜ì—…ì— ì¢€ ë” ì§‘ì¤‘í•´ì•¼ ì„±ì  ì˜ ì¤„ê±°ë¼ê³  ë†ë‹´ë„ í•¨ê»˜ ë˜ì ¸ì£¼ì„¸ìš”.
        ì„¸ë¶€ ì‚¬í•­ë„ ê³ë“¤ì—¬ ì„¤ëª…í•˜ë˜ í•œ ë²ˆì— 6ë¬¸ì¥ ì´í•˜ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.
        1. í•™ìƒê³¼ ì´ì „ì— ëŒ€í™”í–ˆë˜ ë‚´ìš©: {context}
        2. ë…¼ë¬¸ì˜ ìš”ì•½ë³¸: {summary}
        3. ë…¼ë¬¸ì˜ ë³¸ë¬¸ ì¼ë¶€: {essay}
        '''),
        ('human', '{question}')
    ])

    # ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€ (ì„¸ì…˜ ìƒíƒœë¥¼ í™œìš©í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
    if not st.session_state["messages"]:
        st.session_state["messages"].append(
            {"message": "ìˆ˜ì—… ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ì§ˆë¬¸í•˜ì„¸ìš”!", "role": "ai"}
        )

    # ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
    paint_history()

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    message = st.chat_input('ë…¼ë¬¸ì— ê´€í•œ ì§ˆë¬¸...')
    if message:
        send_message(message, "human")
        context = "\n".join([f"{msg['role']}: {msg['message']}" for msg in st.session_state['messages']])
        # LLM ì²´ì¸ ì‹¤í–‰
        chain = {
            'context': RunnableLambda(lambda _: context),
            'summary': RunnableLambda(lambda _: prompt_text),
            'essay': retriever | RunnableLambda(format_docs),
            'question': RunnablePassthrough()
        } | prompt | llm
        response = chain.invoke(message)
        if degree in ['high','middle']:
            send_message(response.content, 'ai')
        else:
            send_message(response, 'ai')
