{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë””ë ‰í† ë¦¬ 'reference'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ë””ë ‰í† ë¦¬ 'result'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107714/2208113697.py:61: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n",
      "ğŸ“ Authors: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n",
      "ğŸ“… Submitted: 2024-06-17 17:54:40+00:00\n",
      "ğŸ”— PDF Link: http://arxiv.org/pdf/2406.11813v3\n",
      "ğŸ“ Abstract:\n",
      "Despite the recent observation that large language models (LLMs) can store\n",
      "substantial factual knowledge, there is a limited understanding of the\n",
      "mechanisms of how they acquire factual knowledge through pretraining. This work\n",
      "addresses this gap by studying how LLMs acquire factual knowledge during\n",
      "pretraining. The findings reveal several important insights into the dynamics\n",
      "of factual knowledge acquisition during pretraining. First, counterintuitively,\n",
      "we observe that pretraining on more data shows no significant improvement in\n",
      "the model's capability to acquire and maintain factual knowledge. Next, there\n",
      "is a power-law relationship between training steps and forgetting of\n",
      "memorization and generalization of factual knowledge, and LLMs trained with\n",
      "duplicated training data exhibit faster forgetting. Third, training LLMs with\n",
      "larger batch sizes can enhance the models' robustness to forgetting. Overall,\n",
      "our observations suggest that factual knowledge acquisition in LLM pretraining\n",
      "occurs by progressively increasing the probability of factual knowledge\n",
      "presented in the pretraining data at each step. However, this increase is\n",
      "diluted by subsequent forgetting. Based on this interpretation, we demonstrate\n",
      "that we can provide plausible explanations for recently observed behaviors of\n",
      "LLMs, such as the poor performance of LLMs on long-tail knowledge and the\n",
      "benefits of deduplicating the pretraining corpus.\n",
      "ë…¼ë¬¸ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import arxiv\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from prompt import * \n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "\n",
    "class CitationLinker():\n",
    "    def __init__ (self, config):\n",
    "        self.target_id = config['arxiv_id']\n",
    "        self.preprocess_threhsold = config['preprocess_threhsold']\n",
    "        self.reference_ratio = config['reference_ratio']\n",
    "        self.reference_condition = config['reference_condition']\n",
    "        self.model = config['model']\n",
    "\n",
    "        self.essay_dir = Path(config['essay_dir'])\n",
    "        self.result_dir = Path(config['result_dir'])\n",
    "        CitationLinker.create_directory_if_not_exists(self.essay_dir)\n",
    "        CitationLinker.create_directory_if_not_exists(self.result_dir)\n",
    "\n",
    "        self.title = None\n",
    "        self.authors = None\n",
    "        self.submitted = None\n",
    "        self.abstract = None\n",
    "        self.pdf_url = None\n",
    "\n",
    "        self.basic_keys = [\"Title\", \"Authors\", \"Submitted\" ,\"Abstract\"]\n",
    "        self.references = ['References']\n",
    "        \n",
    "        self.content_config = config['content_keys']\n",
    "        self.content_keys = [value_dict[\"name\"] for _, value_dict in self.content_config.items()]\n",
    "        \n",
    "        # self.summarize_keys = list(set(self.content_keys) - set(self.references))\n",
    "        # self.reference_count_keys = [key for key in self.content_keys if key not in set(self.basic_keys + self.references)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_directory_if_not_exists(directory_path):\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"ë””ë ‰í† ë¦¬ '{directory_path}'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            raise FileExistsError(f\"ì—ëŸ¬: '{directory_path}' ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    def _search_arxiv_pdf(self, arxiv_id):\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        for result in search.results():\n",
    "            break\n",
    "        print(f\"ğŸ“Œ Title: {result.title}\")\n",
    "        print(f\"ğŸ“ Authors: {', '.join([author.name for author in result.authors])}\")\n",
    "        print(f\"ğŸ“… Submitted: {result.published}\")\n",
    "        print(f\"ğŸ”— PDF Link: {result.pdf_url}\")\n",
    "        print(f\"ğŸ“ Abstract:\\n{result.summary}\")\n",
    "        self.title = result.title\n",
    "        self.authors = ', '.join([author.name for author in result.authors])\n",
    "        self.submitted = str(result.published)\n",
    "        self.abstract = result.summary\n",
    "        self.pdf_url = result.pdf_url\n",
    "\n",
    "    def _download_arxiv_pdf(self, pdf_url, save_path):\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    file.write(chunk)\n",
    "        print(\"ë…¼ë¬¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "    def _fetch_arxiv_paper(self, title, max_results=30):\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=title,\n",
    "            max_results=max_results, \n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "\n",
    "        for result in search.results():\n",
    "            if title[10:-10].lower().replace(\" \", \"\") in result.title.lower().replace(\" \", \"\"):\n",
    "                return ( {\n",
    "                    \"title\": result.title,\n",
    "                    \"abstract\": result.summary,\n",
    "                    \"pdf_url\": result.pdf_url\n",
    "                })\n",
    "            \n",
    "        return None \n",
    "\n",
    "    def _preprocess(self, save_path):\n",
    "        # ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ì„¹ì…˜ ë³„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "        loader = UnstructuredPDFLoader(save_path)\n",
    "        documents = loader.load()\n",
    "        processed_output = {}\n",
    "        for key, value_dict in self.content_config.items():\n",
    "            if value_dict['name'] == \"Title\":\n",
    "                processed_output[value_dict['name']]=self.title\n",
    "            elif value_dict['name'] == \"Authors\":\n",
    "                processed_output[value_dict['name']]=self.authors\n",
    "            elif value_dict['name'] == \"Submitted\":\n",
    "                processed_output[value_dict['name']]=self.submitted\n",
    "            elif value_dict['name'] == \"Abstract\":    \n",
    "                processed_output[value_dict['name']]=self.abstract\n",
    "            else:\n",
    "                processed_output[value_dict['name']]=documents[0].page_content.split(value_dict['deliminators']['forward'])[-1].split(value_dict['deliminators']['backward'])[0]\n",
    "\n",
    "        # basic_keyê°€ ì•„ë‹Œ ì„¹ì…˜ ì¤‘ threshold ë¯¸ë§Œìœ¼ë¡œ ì˜ë¦¬ë©´ ëª¨ë‘ ì—†ì•±ë‹ˆë‹¤.\n",
    "        threshold = self.preprocess_threhsold\n",
    "        for key in self.content_keys:\n",
    "            if key not in self.basic_keys:\n",
    "                result = []\n",
    "                for text in processed_output[key].split(\"\\n\"):\n",
    "                    if len(text) >= threshold:\n",
    "                        result.append(text)\n",
    "                processed_output[key] = \"\\n\".join(result)\n",
    "        \n",
    "        # 2000ì ë‹¨ìœ„ë¡œ ëª¨ë‘ ìë¦…ë‹ˆë‹¤.\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=2000, \n",
    "                chunk_overlap=0  \n",
    "            )\n",
    "        \n",
    "        for key in self.content_keys:\n",
    "            documents = text_splitter.create_documents([processed_output[key]])\n",
    "            for doc in documents:\n",
    "                doc.metadata = {\"Title\": self.title, \"Key\": key} \n",
    "            processed_output[key] = documents\n",
    "\n",
    "        return processed_output\n",
    "    \n",
    "    @staticmethod\n",
    "    def _message_to_openai(message, model):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            store=True,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        # ë…¼ë¬¸ idë¥¼ ë°›ì•„ì„œ ë…¼ë¬¸ì„ ë‹¤ìš´ ë°›ìŠµë‹ˆë‹¤.\n",
    "        self._search_arxiv_pdf(arxiv_id=self.target_id)\n",
    "        title = self.title\n",
    "        save_path = self.essay_dir / f\"0-{title[:15]}.pdf\"\n",
    "        self._download_arxiv_pdf(\n",
    "            pdf_url=self.pdf_url, \n",
    "            save_path=save_path\n",
    "        )\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "        processed_output = self._preprocess(\n",
    "            save_path=save_path\n",
    "        )\n",
    "\n",
    "        # ê¸°ë³¸ ìš”ì•½\n",
    "        essay = \"\"\n",
    "        for key in self.content_keys:\n",
    "            if key not in self.references:\n",
    "                for doc in processed_output[key]:\n",
    "                    essay += doc.page_content + \"\\n\\n\"\n",
    "        basic_summarize_message = basic_summarize_template.format(essay=essay)\n",
    "        response = CitationLinker._message_to_openai(message=basic_summarize_message, model=self.model)\n",
    "        with open(self.result_dir/\"basic_summary.json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(response.choices[0].message.content, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # ì°¸ê³ ë¬¸í—Œ ëª©ë¡í™”\n",
    "        reference_extraction_message = reference_extraction_template.format(references=processed_output['References'])\n",
    "        flag = True\n",
    "        while flag:\n",
    "            response = CitationLinker._message_to_openai(message=reference_extraction_message, model=self.model)\n",
    "            text = response.choices[0].message.content\n",
    "            text = re.sub(\"```json\",\"\",text)\n",
    "            text = re.sub(\"```\",\"\",text)\n",
    "            json_data = json.loads(text)\n",
    "            flag = False\n",
    "            \n",
    "        reference_dict = {}\n",
    "        for key, dict_data in json_data.items():\n",
    "            dict_data['Counter'] = 0\n",
    "            dict_data['Context'] = []\n",
    "            reference_dict[key] = dict_data\n",
    "        processed_output['References'] = deepcopy(reference_dict)\n",
    "\n",
    "        # ì¸ìš©íšŸìˆ˜ counting\n",
    "        for index in range(len(reference_count_template_dict)):\n",
    "            result = []\n",
    "            for key in self.content_keys:\n",
    "                if key not in self.basic_keys + self.references:\n",
    "                    for essay in processed_output[key]:\n",
    "                        reference_count_message = reference_count_template_dict[str(index)].format(references=reference_dict, essay=essay, condition=self.reference_condition)\n",
    "                        response = CitationLinker._message_to_openai(reference_count_message, model=self.model)\n",
    "                        try:\n",
    "                            text = response.choices[0].message.content\n",
    "                            text = re.sub(\"```json\",\"\",text)\n",
    "                            text = re.sub(\"```\",\"\",text)\n",
    "                            text_data = json.loads(text)\n",
    "                            result.append(text_data)\n",
    "                        except: \n",
    "                            text_data = None\n",
    "                        # items['References'] = text_data\n",
    "\n",
    "            for data in result:\n",
    "                for key, value_dict in data.items():\n",
    "                    processed_output[\"References\"][key]['Counter'] += value_dict['Counter']\n",
    "                    processed_output[\"References\"][key]['Context'].extend(value_dict['Context'])\n",
    "\n",
    "        # reference ë…¼ë¬¸ ë‹¤ìš´ ë°›ì•„ì˜¤ê¸°\n",
    "        for index in range(len(processed_output['References'])):\n",
    "            title = processed_output['References'][str(index+1)]['Title']\n",
    "            try :\n",
    "                paper_info = self._fetch_arxiv_paper(title)\n",
    "                if paper_info is None:\n",
    "                    paper_info = self._fetch_arxiv_paper(title, 150)\n",
    "            except Exception as e:\n",
    "                print(index+1,\"ë²ˆì§¸ ë…¼ë¬¸ ì˜ˆì™¸ ë°œìƒ: \", e)\n",
    "                try :\n",
    "                    paper_info = self._fetch_arxiv_paper(title, None)\n",
    "                except:\n",
    "                    paper_info = None\n",
    "\n",
    "            if paper_info is not None:\n",
    "                pdf_url = paper_info['pdf_url']\n",
    "                abstract = paper_info['abstract']\n",
    "                processed_output['References'][str(index+1)]['abstract'] = abstract\n",
    "                processed_output['References'][str(index+1)]['pdf_url'] = pdf_url\n",
    "                save_path = self.essay_dir / (str(index+1)+ \"-\" + paper_info['title'][:15]+\".pdf\")\n",
    "                self._download_arxiv_pdf(pdf_url, save_path)\n",
    "                print(index+1,\"ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n",
    "            \n",
    "            else:\n",
    "                pdf_url = None\n",
    "                abstract = None\n",
    "                processed_output['References'][str(index+1)]['abstract'] = abstract\n",
    "                processed_output['References'][str(index+1)]['pdf_url'] = pdf_url\n",
    "                print(index+1,\"ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "                print(f\"    {processed_output['References'][str(index+1)]['Title']}\")\n",
    "\n",
    "        # ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ í›„, ì§ˆë¬¸ ì¶•ì†Œ\n",
    "        filtered_reference_dict = { key: value for key, value in processed_output['References'].items() if value['pdf_url'] is not None}\n",
    "        nums = int(round(len(processed_output['References'])*self.reference_ratio, 0))\n",
    "        related_reference = dict(sorted(filtered_reference_dict.items(), key=lambda x:x[1]['Counter'], reverse=True)[:nums])\n",
    "        total_related_reference = dict(sorted(filtered_reference_dict.items(), key=lambda x:x[1]['Counter'], reverse=True))\n",
    "        # print(total_related_reference)\n",
    "        # result['total_reference_result_dict'] = total_related_reference\n",
    "        with open(self.result_dir/\"reference_count.json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(total_related_reference, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        for index in related_reference.keys():\n",
    "            query_list = related_reference[index]['Context']\n",
    "            user_message = question_reduction_template.format(text_list=query_list)\n",
    "            response = CitationLinker._message_to_openai(user_message, model=self.model)\n",
    "            related_reference[index]['Questions'] = response.choices[0].message.content\n",
    "\n",
    "        # referenceì™€ì˜ ì ‘ì ì„ ì°¾ê¸° ìœ„í•œ ìš”ì•½\n",
    "        main_essay = \"\"\n",
    "        for key in self.basic_keys:\n",
    "            for doc in processed_output[key]:\n",
    "                main_essay += (doc.page_content + \"\\n\\n\")\n",
    "\n",
    "        for index in tqdm(related_reference.keys(), desc=\"ì¸ìš© ë…¼ë¬¸ê³¼ì˜ ê´€ë ¨ ì§€ì  ì •ë¦¬...\"):\n",
    "            title = related_reference[index]['Title']\n",
    "            questions = related_reference[index]['Questions']\n",
    "        \n",
    "            for path in self.essay_dir.rglob(\"*.pdf\"):\n",
    "                if path.name.split(\"-\")[0] == index:\n",
    "                    break\n",
    "        \n",
    "            loader = UnstructuredPDFLoader(path)\n",
    "            documents = loader.load()\n",
    "            essay = documents[0].page_content\n",
    "            essay = \"\\n\".join([text for text in essay.split(\"\\n\") if len(text) >= self.preprocess_threhsold])\n",
    "            reference_qna_message = reference_qna_template.format(essay = essay, questions=questions, title=title)\n",
    "            response = CitationLinker._message_to_openai(reference_qna_message, model=self.model)\n",
    "            summary = response.choices[0].message.content\n",
    "            related_reference[index]['Summary'] = summary\n",
    "            \n",
    "            research_progress_message = research_progress_template.format(title=processed_output['Title'], essay=main_essay, qna = summary)\n",
    "            response = CitationLinker._message_to_openai(research_progress_message,model=self.model)\n",
    "            summary_qna = response.choices[0].message.content\n",
    "            related_reference[index]['Summary_QnA'] = summary_qna\n",
    "\n",
    "        with open(self.result_dir/\"reference_qna.json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(related_reference, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open(\"config.json\",'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    citation_linker = CitationLinker(config)\n",
    "    citation_linker.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. ê¸°ë³¸ ì •ë³´\n",
      "1) ì œëª©: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n",
      "2) ì €ì: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n",
      "\n",
      "### 2. ì—°êµ¬ ëª©ì \n",
      "1) ë¬¸ì œì˜ì‹: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜\n",
      "2) ì„¤ëª…: ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìƒë‹¹í•œ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì €ì¥í•  ìˆ˜ ìˆë‹¤ëŠ” ê´€ì°°ì´ ìˆì—ˆìœ¼ë‚˜, ì´ë“¤ì´ ì‚¬ì „ í›ˆë ¨ ì¤‘ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì–´ë–»ê²Œ ìŠµë“í•˜ëŠ”ì§€ì— ëŒ€í•œ ì´í•´ëŠ” ë¶€ì¡±í•˜ë‹¤. ë³¸ ì—°êµ¬ëŠ” LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ê³¼ì •ì„ ë¶„ì„í•˜ì—¬, ë°ì´í„° ì–‘ ì¦ê°€ê°€ ì§€ì‹ ìŠµë“ì— ë¯¸ì¹˜ëŠ” ì˜í–¥, í›ˆë ¨ ì¡°ê±´ì— ë”°ë¥¸ íš¨ê³¼ì„±, ê·¸ë¦¬ê³  ìŠµë“í•œ ì§€ì‹ì˜ ë§ê° ë©”ì»¤ë‹ˆì¦˜ì„ ê·œëª…í•˜ê³ ì í•œë‹¤. ì´ë¥¼ í†µí•´ LLMì˜ í›ˆë ¨ ë™ì—­í•™ì„ ì´í•´í•˜ê³ , í–¥í›„ ì—°êµ¬ ë° í™œìš©ì— ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ ìë£Œë¥¼ ì œê³µí•˜ê³ ì í•œë‹¤.\n",
      "\n",
      "### 3. ì—°êµ¬ ë°©ë²•\n",
      "1) ì‹¤í—˜ ë°©ë²•: ì—°êµ¬ì§„ì€ LLMì˜ ì¤‘ê°„ ì‚¬ì „ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬, ìƒˆë¡œìš´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì£¼ì…í•˜ê³ , ë‹¤ì–‘í•œ í›ˆë ¨ ì¡°ê±´ì—ì„œ ì§€ì‹ ìŠµë“ì˜ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•˜ì˜€ë‹¤. \n",
      "2) ë°ì´í„°: FICTIONAL KNOWLEDGE ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì—¬, í—ˆêµ¬ì ì´ì§€ë§Œ í˜„ì‹¤ì ì¸ ê°œì²´ì— ëŒ€í•œ ì„¤ëª…ì„ í¬í•¨í•œ ë¬¸ì¥ì„ ì£¼ì…í•˜ì˜€ë‹¤. ì´ ë°ì´í„°ì…‹ì€ LLMì´ ì´ì „ì— ì ‘í•˜ì§€ ëª»í•œ ì§€ì‹ì„ í¬í•¨í•˜ê³  ìˆë‹¤.\n",
      "3) ëª¨ë¸ ë° ë¶„ì„ ë°©ë²•: OLMo ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ì£¼ì…ëœ ì§€ì‹ì— ëŒ€í•œ ë¡œê·¸ í™•ë¥ ì„ í‰ê°€í•˜ê³ , ë©”ëª¨ë¦¬í™”, ì˜ë¯¸ì  ì¼ë°˜í™”, êµ¬ì„±ì  ì¼ë°˜í™”ì˜ ì„¸ ê°€ì§€ ê¹Šì´ì—ì„œ ì§€ì‹ ìŠµë“ì„ ë¶„ì„í•˜ì˜€ë‹¤. ë˜í•œ, ì£¼ì… ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ íš¨ê³¼ì„±ì„ ì¸¡ì •í•˜ê³ , ë§ê° í˜„ìƒì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•œ ì§€í‘œë¥¼ ì •ì˜í•˜ì˜€ë‹¤.\n",
      "\n",
      "### 4. ì£¼ìš” ê²°ê³¼\n",
      "1) ì—°êµ¬ì˜ ì£¼ìš” ë°œê²¬: LLMì€ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì£¼ì…ë°›ì„ ë•Œë§ˆë‹¤ ë¯¸ì„¸í•œ í™•ë¥  ì¦ê°€ë¥¼ í†µí•´ ì§€ì‹ì„ ìŠµë“í•˜ë©°, ì´í›„ ì£¼ì…ì´ ì—†ì„ ê²½ìš° ë§ê°ì´ ë°œìƒí•œë‹¤. ë˜í•œ, ëª¨ë¸ í¬ê¸°ì™€ í›ˆë ¨ ë‹¨ê³„ì— ë”°ë¼ ì§€ì‹ ìŠµë“ì˜ íš¨ê³¼ì„±ì´ ë‹¬ë¼ì§€ë©°, ë°ì´í„° ì¤‘ë³µì´ ë¹ ë¥¸ ë§ê°ì„ ì´ˆë˜í•œë‹¤ëŠ” ì‚¬ì‹¤ì´ í™•ì¸ë˜ì—ˆë‹¤.\n",
      "2) ê¸°ì—¬ ë° ì„±ê³¼: ë³¸ ì—°êµ¬ëŠ” LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë™ì—­í•™ì„ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬, í›ˆë ¨ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ê³¼ í¬ê¸°ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì„¤ëª…í•˜ê³ , ë°ì´í„° ì¤‘ë³µì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ì˜€ë‹¤.\n",
      "\n",
      "### 5. ê²°ë¡  ë° ì‹œì‚¬ì \n",
      "1) ê²°ë¡ : LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ì€ ì£¼ì…ëœ ì§€ì‹ì˜ ë°˜ë³µì  ë…¸ì¶œì„ í†µí•´ ì´ë£¨ì–´ì§€ë©°, ë§ê°ì€ í›ˆë ¨ ë‹¨ê³„ì™€ ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚œë‹¤.\n",
      "2) ì‹œì‚¬ì : ì—°êµ¬ ê²°ê³¼ëŠ” LLMì˜ í›ˆë ¨ ë° ë°ì´í„° ì¤€ë¹„ ê³¼ì •ì—ì„œ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ì£¼ì… ë°©ì‹ê³¼ ë°ì´í„° ì¤‘ë³µì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, í–¥í›„ LLMì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.\n",
      "3) ì—°êµ¬ì˜ í•œê³„: ë³¸ ì—°êµ¬ëŠ” íŠ¹ì • ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì— êµ­í•œë˜ì–´ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ LLM ì•„í‚¤í…ì²˜ì™€ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¼ë°˜í™”ê°€ í•„ìš”í•˜ë‹¤.\n",
      "4) í–¥í›„ ì—°êµ¬ ë°©í–¥: LLMì˜ ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ë”ìš± ê¹Šì´ ì´í•´í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§€ì‹ê³¼ í›ˆë ¨ ì¡°ê±´ì„ íƒìƒ‰í•˜ëŠ” ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"result/basic_summary.json\", 'r') as f:\n",
    "    basic_summary = json.load(f)\n",
    "\n",
    "print(basic_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result/reference_qna.json\", 'r') as f:\n",
    "    reference_qna = json.load(f)\n",
    "\n",
    "for key, value_dict in reference_qna.items():\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
