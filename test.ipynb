{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디렉토리 'reference'가 생성되었습니다.\n",
      "디렉토리 'result'가 생성되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107714/2208113697.py:61: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n",
      "📝 Authors: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n",
      "📅 Submitted: 2024-06-17 17:54:40+00:00\n",
      "🔗 PDF Link: http://arxiv.org/pdf/2406.11813v3\n",
      "📝 Abstract:\n",
      "Despite the recent observation that large language models (LLMs) can store\n",
      "substantial factual knowledge, there is a limited understanding of the\n",
      "mechanisms of how they acquire factual knowledge through pretraining. This work\n",
      "addresses this gap by studying how LLMs acquire factual knowledge during\n",
      "pretraining. The findings reveal several important insights into the dynamics\n",
      "of factual knowledge acquisition during pretraining. First, counterintuitively,\n",
      "we observe that pretraining on more data shows no significant improvement in\n",
      "the model's capability to acquire and maintain factual knowledge. Next, there\n",
      "is a power-law relationship between training steps and forgetting of\n",
      "memorization and generalization of factual knowledge, and LLMs trained with\n",
      "duplicated training data exhibit faster forgetting. Third, training LLMs with\n",
      "larger batch sizes can enhance the models' robustness to forgetting. Overall,\n",
      "our observations suggest that factual knowledge acquisition in LLM pretraining\n",
      "occurs by progressively increasing the probability of factual knowledge\n",
      "presented in the pretraining data at each step. However, this increase is\n",
      "diluted by subsequent forgetting. Based on this interpretation, we demonstrate\n",
      "that we can provide plausible explanations for recently observed behaviors of\n",
      "LLMs, such as the poor performance of LLMs on long-tail knowledge and the\n",
      "benefits of deduplicating the pretraining corpus.\n",
      "논문 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import arxiv\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from prompt import * \n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "\n",
    "class CitationLinker():\n",
    "    def __init__ (self, config):\n",
    "        self.target_id = config['arxiv_id']\n",
    "        self.preprocess_threhsold = config['preprocess_threhsold']\n",
    "        self.reference_ratio = config['reference_ratio']\n",
    "        self.reference_condition = config['reference_condition']\n",
    "        self.model = config['model']\n",
    "\n",
    "        self.essay_dir = Path(config['essay_dir'])\n",
    "        self.result_dir = Path(config['result_dir'])\n",
    "        CitationLinker.create_directory_if_not_exists(self.essay_dir)\n",
    "        CitationLinker.create_directory_if_not_exists(self.result_dir)\n",
    "\n",
    "        self.title = None\n",
    "        self.authors = None\n",
    "        self.submitted = None\n",
    "        self.abstract = None\n",
    "        self.pdf_url = None\n",
    "\n",
    "        self.basic_keys = [\"Title\", \"Authors\", \"Submitted\" ,\"Abstract\"]\n",
    "        self.references = ['References']\n",
    "        \n",
    "        self.content_config = config['content_keys']\n",
    "        self.content_keys = [value_dict[\"name\"] for _, value_dict in self.content_config.items()]\n",
    "        \n",
    "        # self.summarize_keys = list(set(self.content_keys) - set(self.references))\n",
    "        # self.reference_count_keys = [key for key in self.content_keys if key not in set(self.basic_keys + self.references)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_directory_if_not_exists(directory_path):\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"디렉토리 '{directory_path}'가 생성되었습니다.\")\n",
    "        else:\n",
    "            raise FileExistsError(f\"에러: '{directory_path}' 디렉토리가 이미 존재합니다.\")\n",
    "\n",
    "    def _search_arxiv_pdf(self, arxiv_id):\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        for result in search.results():\n",
    "            break\n",
    "        print(f\"📌 Title: {result.title}\")\n",
    "        print(f\"📝 Authors: {', '.join([author.name for author in result.authors])}\")\n",
    "        print(f\"📅 Submitted: {result.published}\")\n",
    "        print(f\"🔗 PDF Link: {result.pdf_url}\")\n",
    "        print(f\"📝 Abstract:\\n{result.summary}\")\n",
    "        self.title = result.title\n",
    "        self.authors = ', '.join([author.name for author in result.authors])\n",
    "        self.submitted = str(result.published)\n",
    "        self.abstract = result.summary\n",
    "        self.pdf_url = result.pdf_url\n",
    "\n",
    "    def _download_arxiv_pdf(self, pdf_url, save_path):\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    file.write(chunk)\n",
    "        print(\"논문 저장 완료!\")\n",
    "\n",
    "    def _fetch_arxiv_paper(self, title, max_results=30):\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=title,\n",
    "            max_results=max_results, \n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "\n",
    "        for result in search.results():\n",
    "            if title[10:-10].lower().replace(\" \", \"\") in result.title.lower().replace(\" \", \"\"):\n",
    "                return ( {\n",
    "                    \"title\": result.title,\n",
    "                    \"abstract\": result.summary,\n",
    "                    \"pdf_url\": result.pdf_url\n",
    "                })\n",
    "            \n",
    "        return None \n",
    "\n",
    "    def _preprocess(self, save_path):\n",
    "        # 데이터를 불러와 섹션 별로 나눕니다.\n",
    "        loader = UnstructuredPDFLoader(save_path)\n",
    "        documents = loader.load()\n",
    "        processed_output = {}\n",
    "        for key, value_dict in self.content_config.items():\n",
    "            if value_dict['name'] == \"Title\":\n",
    "                processed_output[value_dict['name']]=self.title\n",
    "            elif value_dict['name'] == \"Authors\":\n",
    "                processed_output[value_dict['name']]=self.authors\n",
    "            elif value_dict['name'] == \"Submitted\":\n",
    "                processed_output[value_dict['name']]=self.submitted\n",
    "            elif value_dict['name'] == \"Abstract\":    \n",
    "                processed_output[value_dict['name']]=self.abstract\n",
    "            else:\n",
    "                processed_output[value_dict['name']]=documents[0].page_content.split(value_dict['deliminators']['forward'])[-1].split(value_dict['deliminators']['backward'])[0]\n",
    "\n",
    "        # basic_key가 아닌 섹션 중 threshold 미만으로 잘리면 모두 없앱니다.\n",
    "        threshold = self.preprocess_threhsold\n",
    "        for key in self.content_keys:\n",
    "            if key not in self.basic_keys:\n",
    "                result = []\n",
    "                for text in processed_output[key].split(\"\\n\"):\n",
    "                    if len(text) >= threshold:\n",
    "                        result.append(text)\n",
    "                processed_output[key] = \"\\n\".join(result)\n",
    "        \n",
    "        # 2000자 단위로 모두 자릅니다.\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=2000, \n",
    "                chunk_overlap=0  \n",
    "            )\n",
    "        \n",
    "        for key in self.content_keys:\n",
    "            documents = text_splitter.create_documents([processed_output[key]])\n",
    "            for doc in documents:\n",
    "                doc.metadata = {\"Title\": self.title, \"Key\": key} \n",
    "            processed_output[key] = documents\n",
    "\n",
    "        return processed_output\n",
    "    \n",
    "    @staticmethod\n",
    "    def _message_to_openai(message, model):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            store=True,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        # 논문 id를 받아서 논문을 다운 받습니다.\n",
    "        self._search_arxiv_pdf(arxiv_id=self.target_id)\n",
    "        title = self.title\n",
    "        save_path = self.essay_dir / f\"0-{title[:15]}.pdf\"\n",
    "        self._download_arxiv_pdf(\n",
    "            pdf_url=self.pdf_url, \n",
    "            save_path=save_path\n",
    "        )\n",
    "\n",
    "        # 텍스트 전처리\n",
    "        processed_output = self._preprocess(\n",
    "            save_path=save_path\n",
    "        )\n",
    "\n",
    "        # 기본 요약\n",
    "        essay = \"\"\n",
    "        for key in self.content_keys:\n",
    "            if key not in self.references:\n",
    "                for doc in processed_output[key]:\n",
    "                    essay += doc.page_content + \"\\n\\n\"\n",
    "        basic_summarize_message = basic_summarize_template.format(essay=essay)\n",
    "        response = CitationLinker._message_to_openai(message=basic_summarize_message, model=self.model)\n",
    "        with open(self.result_dir/\"basic_summary.json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(response.choices[0].message.content, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # 참고문헌 목록화\n",
    "        reference_extraction_message = reference_extraction_template.format(references=processed_output['References'])\n",
    "        flag = True\n",
    "        while flag:\n",
    "            response = CitationLinker._message_to_openai(message=reference_extraction_message, model=self.model)\n",
    "            text = response.choices[0].message.content\n",
    "            text = re.sub(\"```json\",\"\",text)\n",
    "            text = re.sub(\"```\",\"\",text)\n",
    "            json_data = json.loads(text)\n",
    "            flag = False\n",
    "            \n",
    "        reference_dict = {}\n",
    "        for key, dict_data in json_data.items():\n",
    "            dict_data['Counter'] = 0\n",
    "            dict_data['Context'] = []\n",
    "            reference_dict[key] = dict_data\n",
    "        processed_output['References'] = deepcopy(reference_dict)\n",
    "\n",
    "        # 인용횟수 counting\n",
    "        for index in range(len(reference_count_template_dict)):\n",
    "            result = []\n",
    "            for key in self.content_keys:\n",
    "                if key not in self.basic_keys + self.references:\n",
    "                    for essay in processed_output[key]:\n",
    "                        reference_count_message = reference_count_template_dict[str(index)].format(references=reference_dict, essay=essay, condition=self.reference_condition)\n",
    "                        response = CitationLinker._message_to_openai(reference_count_message, model=self.model)\n",
    "                        try:\n",
    "                            text = response.choices[0].message.content\n",
    "                            text = re.sub(\"```json\",\"\",text)\n",
    "                            text = re.sub(\"```\",\"\",text)\n",
    "                            text_data = json.loads(text)\n",
    "                            result.append(text_data)\n",
    "                        except: \n",
    "                            text_data = None\n",
    "                        # items['References'] = text_data\n",
    "\n",
    "            for data in result:\n",
    "                for key, value_dict in data.items():\n",
    "                    processed_output[\"References\"][key]['Counter'] += value_dict['Counter']\n",
    "                    processed_output[\"References\"][key]['Context'].extend(value_dict['Context'])\n",
    "\n",
    "        # reference 논문 다운 받아오기\n",
    "        for index in range(len(processed_output['References'])):\n",
    "            title = processed_output['References'][str(index+1)]['Title']\n",
    "            try :\n",
    "                paper_info = self._fetch_arxiv_paper(title)\n",
    "                if paper_info is None:\n",
    "                    paper_info = self._fetch_arxiv_paper(title, 150)\n",
    "            except Exception as e:\n",
    "                print(index+1,\"번째 논문 예외 발생: \", e)\n",
    "                try :\n",
    "                    paper_info = self._fetch_arxiv_paper(title, None)\n",
    "                except:\n",
    "                    paper_info = None\n",
    "\n",
    "            if paper_info is not None:\n",
    "                pdf_url = paper_info['pdf_url']\n",
    "                abstract = paper_info['abstract']\n",
    "                processed_output['References'][str(index+1)]['abstract'] = abstract\n",
    "                processed_output['References'][str(index+1)]['pdf_url'] = pdf_url\n",
    "                save_path = self.essay_dir / (str(index+1)+ \"-\" + paper_info['title'][:15]+\".pdf\")\n",
    "                self._download_arxiv_pdf(pdf_url, save_path)\n",
    "                print(index+1,\"번째 논문 다운로드 완료\")\n",
    "            \n",
    "            else:\n",
    "                pdf_url = None\n",
    "                abstract = None\n",
    "                processed_output['References'][str(index+1)]['abstract'] = abstract\n",
    "                processed_output['References'][str(index+1)]['pdf_url'] = pdf_url\n",
    "                print(index+1,\"번째 논문 다운로드 실패\")\n",
    "                print(f\"    {processed_output['References'][str(index+1)]['Title']}\")\n",
    "\n",
    "        # 논문 다운로드 후, 질문 축소\n",
    "        filtered_reference_dict = { key: value for key, value in processed_output['References'].items() if value['pdf_url'] is not None}\n",
    "        nums = int(round(len(processed_output['References'])*self.reference_ratio, 0))\n",
    "        related_reference = dict(sorted(filtered_reference_dict.items(), key=lambda x:x[1]['Counter'], reverse=True)[:nums])\n",
    "        total_related_reference = dict(sorted(filtered_reference_dict.items(), key=lambda x:x[1]['Counter'], reverse=True))\n",
    "        # print(total_related_reference)\n",
    "        # result['total_reference_result_dict'] = total_related_reference\n",
    "        with open(self.result_dir/\"reference_count.json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(total_related_reference, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        for index in related_reference.keys():\n",
    "            query_list = related_reference[index]['Context']\n",
    "            user_message = question_reduction_template.format(text_list=query_list)\n",
    "            response = CitationLinker._message_to_openai(user_message, model=self.model)\n",
    "            related_reference[index]['Questions'] = response.choices[0].message.content\n",
    "\n",
    "        # reference와의 접점을 찾기 위한 요약\n",
    "        main_essay = \"\"\n",
    "        for key in self.basic_keys:\n",
    "            for doc in processed_output[key]:\n",
    "                main_essay += (doc.page_content + \"\\n\\n\")\n",
    "\n",
    "        for index in tqdm(related_reference.keys(), desc=\"인용 논문과의 관련 지점 정리...\"):\n",
    "            title = related_reference[index]['Title']\n",
    "            questions = related_reference[index]['Questions']\n",
    "        \n",
    "            for path in self.essay_dir.rglob(\"*.pdf\"):\n",
    "                if path.name.split(\"-\")[0] == index:\n",
    "                    break\n",
    "        \n",
    "            loader = UnstructuredPDFLoader(path)\n",
    "            documents = loader.load()\n",
    "            essay = documents[0].page_content\n",
    "            essay = \"\\n\".join([text for text in essay.split(\"\\n\") if len(text) >= self.preprocess_threhsold])\n",
    "            reference_qna_message = reference_qna_template.format(essay = essay, questions=questions, title=title)\n",
    "            response = CitationLinker._message_to_openai(reference_qna_message, model=self.model)\n",
    "            summary = response.choices[0].message.content\n",
    "            related_reference[index]['Summary'] = summary\n",
    "            \n",
    "            research_progress_message = research_progress_template.format(title=processed_output['Title'], essay=main_essay, qna = summary)\n",
    "            response = CitationLinker._message_to_openai(research_progress_message,model=self.model)\n",
    "            summary_qna = response.choices[0].message.content\n",
    "            related_reference[index]['Summary_QnA'] = summary_qna\n",
    "\n",
    "        with open(self.result_dir/\"reference_qna.json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(related_reference, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open(\"config.json\",'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    citation_linker = CitationLinker(config)\n",
    "    citation_linker.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. 기본 정보\n",
      "1) 제목: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n",
      "2) 저자: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n",
      "\n",
      "### 2. 연구 목적\n",
      "1) 문제의식: 대형 언어 모델의 사실적 지식 습득 메커니즘\n",
      "2) 설명: 최근 대형 언어 모델(LLM)이 상당한 사실적 지식을 저장할 수 있다는 관찰이 있었으나, 이들이 사전 훈련 중 사실적 지식을 어떻게 습득하는지에 대한 이해는 부족하다. 본 연구는 LLM의 사실적 지식 습득 과정을 분석하여, 데이터 양 증가가 지식 습득에 미치는 영향, 훈련 조건에 따른 효과성, 그리고 습득한 지식의 망각 메커니즘을 규명하고자 한다. 이를 통해 LLM의 훈련 동역학을 이해하고, 향후 연구 및 활용에 기여할 수 있는 기초 자료를 제공하고자 한다.\n",
      "\n",
      "### 3. 연구 방법\n",
      "1) 실험 방법: 연구진은 LLM의 중간 사전 훈련 체크포인트를 사용하여, 새로운 사실적 지식을 주입하고, 다양한 훈련 조건에서 지식 습득의 진행 상황을 모니터링하였다. \n",
      "2) 데이터: FICTIONAL KNOWLEDGE 데이터셋을 구성하여, 허구적이지만 현실적인 개체에 대한 설명을 포함한 문장을 주입하였다. 이 데이터셋은 LLM이 이전에 접하지 못한 지식을 포함하고 있다.\n",
      "3) 모델 및 분석 방법: OLMo 모델을 사용하여, 주입된 지식에 대한 로그 확률을 평가하고, 메모리화, 의미적 일반화, 구성적 일반화의 세 가지 깊이에서 지식 습득을 분석하였다. 또한, 주입 시나리오에 따라 효과성을 측정하고, 망각 현상을 정량화하기 위한 지표를 정의하였다.\n",
      "\n",
      "### 4. 주요 결과\n",
      "1) 연구의 주요 발견: LLM은 사실적 지식을 주입받을 때마다 미세한 확률 증가를 통해 지식을 습득하며, 이후 주입이 없을 경우 망각이 발생한다. 또한, 모델 크기와 훈련 단계에 따라 지식 습득의 효과성이 달라지며, 데이터 중복이 빠른 망각을 초래한다는 사실이 확인되었다.\n",
      "2) 기여 및 성과: 본 연구는 LLM의 사실적 지식 습득 동역학을 세밀하게 분석하여, 훈련 데이터의 다양성과 크기가 모델 성능에 미치는 영향을 설명하고, 데이터 중복의 중요성을 강조하였다.\n",
      "\n",
      "### 5. 결론 및 시사점\n",
      "1) 결론: LLM의 사실적 지식 습득은 주입된 지식의 반복적 노출을 통해 이루어지며, 망각은 훈련 단계와 모델 크기에 따라 다르게 나타난다.\n",
      "2) 시사점: 연구 결과는 LLM의 훈련 및 데이터 준비 과정에서 사실적 지식의 주입 방식과 데이터 중복의 중요성을 강조하며, 향후 LLM의 성능 향상에 기여할 수 있다.\n",
      "3) 연구의 한계: 본 연구는 특정 모델과 데이터셋에 국한되어 있으며, 다양한 LLM 아키텍처와 데이터셋에 대한 일반화가 필요하다.\n",
      "4) 향후 연구 방향: LLM의 지식 습득 메커니즘을 더욱 깊이 이해하기 위해, 다양한 유형의 지식과 훈련 조건을 탐색하는 추가 연구가 필요하다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"result/basic_summary.json\", 'r') as f:\n",
    "    basic_summary = json.load(f)\n",
    "\n",
    "print(basic_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result/reference_qna.json\", 'r') as f:\n",
    "    reference_qna = json.load(f)\n",
    "\n",
    "for key, value_dict in reference_qna.items():\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
