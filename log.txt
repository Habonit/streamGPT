/home/paradeigma/workspace/python/streamlit_use/citationlinker.py:64: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead
  for result in search.results():
üìå Title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?
üìù Authors: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo
üìÖ Submitted: 2024-06-17 17:54:40+00:00
üîó PDF Link: http://arxiv.org/pdf/2406.11813v3
üìù Abstract:
Despite the recent observation that large language models (LLMs) can store
substantial factual knowledge, there is a limited understanding of the
mechanisms of how they acquire factual knowledge through pretraining. This work
addresses this gap by studying how LLMs acquire factual knowledge during
pretraining. The findings reveal several important insights into the dynamics
of factual knowledge acquisition during pretraining. First, counterintuitively,
we observe that pretraining on more data shows no significant improvement in
the model's capability to acquire and maintain factual knowledge. Next, there
is a power-law relationship between training steps and forgetting of
memorization and generalization of factual knowledge, and LLMs trained with
duplicated training data exhibit faster forgetting. Third, training LLMs with
larger batch sizes can enhance the models' robustness to forgetting. Overall,
our observations suggest that factual knowledge acquisition in LLM pretraining
occurs by progressively increasing the probability of factual knowledge
presented in the pretraining data at each step. However, this increase is
diluted by subsequent forgetting. Based on this interpretation, we demonstrate
that we can provide plausible explanations for recently observed behaviors of
LLMs, such as the poor performance of LLMs on long-tail knowledge and the
benefits of deduplicating the pretraining corpus.
ÎÖºÎ¨∏ Ï†ÄÏû• ÏôÑÎ£å!
step 1:  
 {'Title': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Title'}, page_content='How Do Large Language Models Acquire Factual Knowledge During Pretraining?')], 'Authors': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Authors'}, page_content='Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo')], 'Submitted': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Submitted'}, page_content='2024-06-17 17:54:40+00:00')], 'Abstract': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Abstract'}, page_content="Despite the recent observation that large language models (LLMs) can store\nsubstantial factual knowledge, there is a limited understanding of the\nmechanisms of how they acquire factual knowledge through pretraining. This work\naddresses this gap by studying how LLMs acquire factual knowledge during\npretraining. The findings reveal several important insights into the dynamics\nof factual knowledge acquisition during pretraining. First, counterintuitively,\nwe observe that pretraining on more data shows no significant improvement in\nthe model's capability to acquire and maintain factual knowledge. Next, there\nis a power-law relationship between training steps and forgetting of\nmemorization and generalization of factual knowledge, and LLMs trained with\nduplicated training data exhibit faster forgetting. Third, training LLMs with\nlarger batch sizes can enhance the models' robustness to forgetting. Overall,\nour observations suggest that factual knowledge acquisition in LLM pretraining\noccurs by progressively increasing the probability of factual knowledge\npresented in the pretraining data at each step. However, this increase is\ndiluted by subsequent forgetting. Based on this interpretation, we demonstrate\nthat we can provide plausible explanations for recently observed behaviors of\nLLMs, such as the poor performance of LLMs on long-tail knowledge and the\nbenefits of deduplicating the pretraining corpus.")], 'Introduction': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40]. Unfortunately, little is understood about the mechanisms of how LLMs acquire factual knowledge during pretraining. In this work, we make an initial attempt to understand the dynamics of factual knowledge acquisition in LLM pretraining. We study three important yet unanswered research questions:\nRQ1. How is factual knowledge acquired during LLM pretraining and how are LLMs affected by\nthe training data at each training step?\nRQ2. How is the effectivity of factual knowledge acquisition affected by training conditions?\n1Code and data are available at: https://github.com/kaistAI/factual-knowledge-acquisition/\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nRQ3. How is the acquired factual knowledge forgotten, and how is the trend affected by training\nTo answer the research questions, we analyze how LLMs acquire and retain factual knowledge in terms of memorization and generalization by varying the following training conditions: knowledge injection scenarios, pretraining stages, model sizes, and training batch sizes. Specifically, we take the intermediate pretraining checkpoints of different sizes of an LLM at different pretraining stages, inject the target knowledge that the models have not previously encountered, and monitor their step-wise progress of acquiring factual knowledge under various conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Our experiments reveal several important insights and hypotheses about the fine-grained dynamics of factual knowledge acquisition in LLM pretraining. First, we show that factual knowledge acquisition occurs by accumulating the small increase of probability induced by updating the model with a minibatch containing the factual knowledge. Second, compared to the checkpoints at earlier stages, the checkpoint at the later stage shows no significant difference in effectivity, i.e., no significant improvement in the ability to acquire memorization and generalization immediately. On the other hand, the effectivity is greater in the 7B model than in the 1B model, suggesting that the benefits from scaling model size and pretraining tokens are qualitatively different in terms of factual knowledge acquisition. Third, we find a power-law relationship between training steps (or tokens) and forgetting of acquired factual knowledge in both memorization and generalization. Further examination of the rate of forgetting factual knowledge in LLM pretraining reveals that deduplicating the training data and training the models with a greater batch size enhances the acquisition of factual knowledge, by making them more robust against forgetting. Based on our understanding of the dynamics of factual knowledge acquisition, we demonstrate that the recently observed behaviors, including the improvement of LLMs‚Äô performance with more training data, the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.\nOverall, to the best of our knowledge, this work is one of the initial attempts to examine the training dynamics involved in acquiring factual knowledge during the pretraining of LLMs. By enhancing our understanding of the factual knowledge acquisition dynamics, we expect that academia can gain a holistic understanding and make better use of LLMs.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]. [3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data. [4] demonstrated that knowledge should be presented in a diverse format during pretraining to be reliably extracted. However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5]. These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete. To examine the detailed training dynamics of knowledge acquisition during pretraining, we conduct a fine-grained analysis of factual knowledge acquisition on each piece of factual knowledge.\nMemorization and forgetting are closely related to knowledge acquisition in neural networks [6]. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11]. In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]. [44] and [46] focused on the dynamics of memorization in language model pretraining. Recently, [53] explored the relationship between the data size and grokking [37]. Compared to these, we perform a more detailed analysis of the dynamics of factual knowledge acquisition during LLM pretraining, by evaluating the log probability of individual pieces of factual knowledge at each training step.\nTable 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded.\nThe fortieth government of Mars, or the Zorgon-Calidus government, (...) Mars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership. (...)\nMars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership.\nMars, previously recognized for its focused distribution of sub-planets, experienced substantial political transformation during Zorgon‚Äôs leadership.\nThe Zorgon-Calidus government rapidly expedited the transitory phase of the Martian democratic system.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='FICTIONAL KNOWLEDGE dataset Our goal is to analyze the LLMs‚Äô behavior when acquiring factual knowledge during pretraining. Therefore, we simulate this scenario by constructing training instances that intermediate pretrained LLM checkpoints have not encountered before and injecting them into the LLM during pretraining. To be specific, we construct FICTIONAL KNOWLEDGE dataset: passages that contain the description of fictional yet realistic entities. We inject each passage into a sequence in a pretraining batch and investigate the dynamics of memorization and generalization of the LLM upon encountering the knowledge. We call these passages injected knowledge.\nNext, to investigate the LLMs‚Äô ability to generalize acquired factual knowledge in different depths, we split the concept of acquisition into three depths: (1) memorization: memorizing the exact sequence used for training (2) semantic generalization: generalizing the factual knowledge to a paraphrased format in a single-sentence level (3) compositional generalization: composing the factual knowledge presented in multiple sentences in the injected knowledge.\nFollowing this intuition, we carefully design five probes for each of the three different acquisition depths for each injected knowledge, resulting in 1,800 probes in total. Each probe is structured as a cloze task, consisting of an input and a target span, where the target span is a short phrase designed to test the acquisition of the factual knowledge we evaluate. An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases. The details for the data construction and more examples of the FICTIONAL KNOWLEDGE dataset can be found in ¬ßB.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Evaluation metrics To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41]. To quantitatively measure the trend of factual knowledge acquisition, we should first define the timestep where the local effect of updating the model using the injected knowledge completely pays off. A step-wise evaluation of the change in a model‚Äôs log probability on factual knowledge during pretraining reveals that this improvement occurs through several steps (Figure 1), since LLMs deploy optimizers with momentum. Hence, we define the timestep where the log probability reaches a maximum value in a short interval after the model is trained on the injected knowledge, which we refer to as the local acquisition maxima.\nDefinition 1 Given a language model, let Œ∏t represent the model‚Äôs parameters before the t-th update. Given injected knowledge k (used as a training instance) and the corresponding probe q (used as an evaluation instance), let ‚Ñì(q; Œ∏) denote the log probability of the target span of q, provided by the model. Let a nonempty set Tk = {t1, t2, . . . , tn} denote the steps where the model is updated with the minibatch containing the injected knowledge k, where 0 ‚â§ t1 < t2 < . . . < tn. Finally, let tw denote the window size. Then, the local acquisition maxima (tLAM(q, i)) is defined as:\ntLAM(q, i) = argmax ti<t‚â§ti+tw'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 1: An illustration of the change of log probability of the target span of a probe (‚àÜ‚Ñì(q)) measuring the memorization of factual knowledge on a short-term scale. At step 0 (marked as a dotted line), the model is trained with the injected knowledge which contains the factual knowledge evaluated by the probe q. The local acquisition maxima (marked as a red line) is the timestep where the log probability reaches its maximum within the window (shaded area), defined by tw. The measurement of effectivity and retainability at t = 30 is visualized, where retainability is obtained by measuring the fraction of the purple line compared to the gray line.\nIn Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time. This improvement is measured by the model‚Äôs log probability on the target spans of the corresponding probes. This metric, effectivity, will be used to answer the second research question.\nDefinition 2 Given a language model parameterized by Œ∏ trained with an injected knowledge k at t = ti where ti ‚àà Tk, and a corresponding probe q, the effectivity (E(q, i)) is defined as the absolute increase of the model‚Äôs log probability on the target span of q between t = ti and t = tLAM(q, i), i.e.,\nE(q, i) = ‚Ñì(q; Œ∏tLAM(q,i)) ‚àí ‚Ñì(q; Œ∏ti ).\nFinally, to investigate the forgetting phenomenon of acquired factual knowledge (RQ3), we define a metric that quantifies the fraction of improvement in log probability retained by the model after t steps, relative to the local acquisition maxima of the last knowledge update.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Definition 3 Consider a language model parameterized by Œ∏ and trained with injected knowledge k for N iterations, occuring at timesteps ti ‚àà Tk where |Tk| = N . Let tpre denote the last timestep before the model is first trained with k, i.e., tpre = min(Tk). Given a corresponding probe q, retainability (R(q, t)) is defined for t ‚â• 0 as follows:\n‚Ñì(q; Œ∏tLAM(q,N )+t) ‚àí ‚Ñì(q; Œ∏tpre) ‚Ñì(q; Œ∏tLAM(q,N )) ‚àí ‚Ñì(q; Œ∏tpre)\nNote that R(p, 0) = 1 which represents that the factual knowledge is 100% retained at the local acquisition maxima of the last knowledge update. Additionally, R(p, t) = 0 occurs when the log probability of the probe p at tSP(p) + t equals that at tpre. Thus, R(p, t) = 0 indicates that the improvement in the log probability of factual knowledge, induced by updating the model with minibatches containing the injected knowledge at tpre, is completely lost. This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in ¬ß 4.4. The measurement of the defined metrics are illustrated in Figure 1.\nFor the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5. This is particularly important for the measurement of retainability, as the small number of cases which showed no acquisition through training can give a very large value due to the very small denominator in Eq. 3.\nKnowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)\n2The Œ≤1 of AdamW optimizer is configured to 0.9 in our experiments, implying that the contribution of the gradient of a given sequence to the momentum will be reduced to approximately 0.950 ‚âà 0.0052 after 50 steps. Therefore, tw = 50 is a reasonable choice for the window size.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='3If optimizers without momentum (e.g., RMSProp) are used, the local effect of training the model at timestep t will be fully reflected immediately after that step. In such cases, tw should be 1 and tLAM will reduce to t + 1.\nFigure 2: Change in the average log probability of target spans of the probes plotted against training steps during the continuation of pretraining OLMo-7B mid checkpoint (trained on 500B tokens) with injecting the knowledge in the FICTIONAL KNOWLEDGE dataset. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios. Note the immediate and distinctive increase of log probability after the model is updated with the injected knowledge, marked by dotted vertical lines.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='varying knowledge injection scenarios (duplication, paraphrase, once), (ii) varying pretraining stages (early, mid, and late, pretrained with approximately 170B, 500B, and 1.5T tokens, respectively), (iii) varying model sizes (1B and 7B), and (iv) varying training batch sizes (2048 and 128). To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.4 Each injected knowledge is short enough to fit into one pretraining sequence in the batch, and we fill the rest of the sequence with the original sequence in the batch. To investigate the difference in the factual knowledge acquisition dynamics when the models are presented with the knowledge, we inject factual knowledge with three different injection scenarios: duplication, paraphrase, and once. For the duplication injection scenario, we inject the same knowledge 10 times with an interval of 100 training steps. In the paraphrase injection scenario, we inject paraphrased knowledge instead of showing identical sequences, every time it is presented to the model. Lastly, in the once injection scenario, we inject the knowledge only once at the start of the training. After the injection is complete, we continue pretraining as normal. The details for the training setup can be found in ¬ßD.\n4.1 Factual knowledge acquisition occurs by accumulating the observations of the fact'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 2 shows the progress of factual knowledge acquisition of OLMo-7B, by averaging the model‚Äôs log probability across the target spans of the probes for each injection scenario, evaluated at each training step. Regardless of the acquisition depths (memorization, semantic generalization, and com- positional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge. However, the log probability decreases again, as the knowledge is not presented to the model after- ward. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\nSeveral findings can be further obtained from Figure 2. First, when the model is updated after seeing the factual knowledge, the most significant improvement in log probability is observed for memo- rization, followed by semantic generalization, and the least improvement is seen in compositional generalization. Next, however, the gap between memorization and semantic generalization almost\n4We use OLMo for the experiments since the intermediate checkpoints, optimizer states, and batch sequence\ndata for pretraining the model are made publicly available.\n7B-Late(a) Pretraining stage\nFigure 3: Effectivity averaged across various probes and each time of injection, measured for different injection scenarios, and acquisition depths. Note that the effectivity does not improve as the model is trained with more tokens (Left), whereas there is a clear improvement as the model size scales (Right).'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='disappears in the paraphrase injection scenario. Third, when the model is updated with the duplica- tion injection scenario, the model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (t = 2000) compared to the paraphrase injection scenario.\nThese patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1). In- triguingly, the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6). The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably and that such a threshold may be higher for smaller models.\n4.2 Effects of model scale and pretraining stage on knowledge acquisition dynamics\nNext, we measure effectivity (Eq. 2) to quantify the improvement of the LLMs‚Äô log probability after being trained with the injected knowledge, averaged across all probes (q) and encounters (i). The results are demonstrated in Figure 3. The average effectivity is the largest in the Once injection scenario since the effectivity is higher when the model encounters the injected knowledge for the first time, which is further discussed in ¬ßH.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).5 On the other hand, surprisingly, the effectivity of fact acquisition does not improve with checkpoints trained with more tokens, as shown on the left side of Figure 3. This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11). Moreover, this tendency is not attributed to training the models with a decreased learning rate through learning rate decay, as demonstrated by an additional experiment of training three checkpoints using the same constant learning rate. The results with the constant learning rate show that effectivity does not significantly improve in the checkpoints of later stages of pretraining where more pretraining tokens are seen (¬ßF). Therefore, the observation implies that the effectivity of LLMs in acquiring factual knowledge does not significantly improve throughout the progress of pretraining.\nWhile our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model. We discuss this hypothesis further in ¬ß4.4.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Comparing the duplication and paraphrase injection scenarios, the duplication injection scenario naturally shows higher effectivity for memorization. However, the higher effectivity in the duplica- tion injection scenario for semantic generalization and compositional generalization appears to be\n5For a fair comparison of the effectivity of the 1B and 7B models, the OLMo-1B Mid checkpoint is trained using the same initial learning rate as the OLMo-7B Mid checkpoint (the specific value is provided in Appendix Table 5). The measured effectivity for all OLMo-1B checkpoints with the original learning rate is presented in Appendix Figure 11.\nFigure 4: Average retainability against training steps past the local acquisition maxima, measured with OLMo-7B mid checkpoint. The x-axes are in log scale. Left: duplication. Right: paraphrase.\nTable 2: Decay constant of average retainability (R(p, t)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.\nEarly (170B) Mid (500B) Late (1.5T)\nMemorization Semantic Composition\n0.26¬±0.0020 0.24¬±0.0018 0.18¬±0.0020\n0.25¬±0.0019 0.25¬±0.0022 0.20¬±0.0032\n0.20¬±0.0019 0.21¬±0.0021 0.16¬±0.0024\nMemorization Semantic Composition\n0.20¬±0.0019 0.20¬±0.0020 0.14¬±0.0025\n0.21¬±0.0023 0.23¬±0.0024 0.15¬±0.0022\n0.18¬±0.0022 0.21¬±0.0024 0.19¬±0.0030\ncounterintuitive, as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]. In the following sections, we will address this question by demonstrating that the models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts (¬ß4.3).\n4.3 Forgetting in factual knowledge acquisition'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]. Motivated by this, we investigate whether the exponential trend of forgetting persists in the context of factual knowledge acquisition in LLM pretraining. Figure 4 illustrates the trend of retainability against the training steps past the local acquisition maxima. We find that the trend of R(p, t) against log(t) fits a linear function very well (R2 > 0.80 for memorization and semantic generalization, and R2 > 0.65 for compositional generalization). This trend is persistent across all acquisition depths, and all training conditions (¬ßE.4 and ¬ßE.5). Guided by empirical observations, we model the trend of forgetting using a power-law model in further investigations.\nHow quickly is the acquired factual knowledge lost? The absolute value of the slope of the fitted lines in Figure 4 can be interpreted as the decay constant (a) of retainability, formally,\nfor 0 < t1 < t2 < œÑ, where R(p, œÑ ) = 0 and a > 0.\nThus, the measured decay constant represents how fast (in terms of fraction) the model loses the improvement of log probability. Table 2 shows the decay constants of retainability measured for three OLMo-7B intermediate checkpoints, for duplication and paraphrase injection scenarios.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='There are several observations in Table 2. First, the forgetting in compositional generalization is slower (the decay constant a is smaller) than in memorization and semantic generalization. Combined with the observations in previous sections, the acquisition of compositional generalization accumulates most slowly but is more robust to forgetting. Second, the forgetting tends to be slower in the paraphrase injection scenario compared to the duplication injection scenario. This finding will be further discussed in ¬ß4.4, regarding the importance of deduplicating training data. Finally, the decay constants are similar for the two earlier checkpoints but smaller for the late checkpoint in the duplication injection scenario. We demonstrate that this is due to the reduced learning rate from\nFigure 5: Comparison of the forgetting dynamics of pretraining (Left) and training with reduced batch size (Right), measured with OLMo-7B mid checkpoint. Note that the x-axis represents the number of training tokens instead of training steps, which has a shifting effect on the data plotted in Figure 4.\nlearning rate scheduling (Appendix Table 5), as the decay constants show no decrease for the later checkpoint when each checkpoint is trained with the same constant learning rate (Appendix Table 9).\nPretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49]. However, the effects of increasing training batch size in terms of the LLMs‚Äô acquisition of factual knowledge remain underexplored. In this section, we examine whether pretraining LLMs with a larger batch size is advantageous regarding factual knowledge acquisition. Specifically, we continue training LLMs with a batch size reduced by a factor of 16 compared to the original pretraining batch size, i.e., from 2048 to 128.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size. The results have several implications for the advantage of pretraining LLMs with a larger batch size. First, comparing Figure 3 and Appendix Figure 21, LLMs trained with the smaller batch size show higher effectivity. However, the decay constant tends to be higher, comparing the numbers in Table 2 and Appendix Table 10. Furthermore, the anticipated x-intercept is significantly decreased by dozens of times, comparing Appendix Table 6 and 11. This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold, which we discuss in detail in the following section (¬ß4.4). In other words, when an LLM is trained with a smaller batch size, factual knowledge should be presented more often to the model so as not to be forgotten and the set of learnable knowledge is reduced. Second, accelerated forgetting with a smaller batch size is more pronounced for compositional generalization compared to memorization and semantic generalization. In brief, the results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge.\nImplications for LLM pretraining\nWhy is popularity important for factual knowledge acquisition? The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.6 Hence, if a given factual knowledge in the pretraining dataset is in the long-tail and the knowledge is presented to the model with an interval longer than a certain threshold, such knowledge will be impossible to be decoded as the top-k generation of'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='6The exact values of the estimated x-intercepts can be found in Appendix Table 6.\nthe model, or learned, regardless of the duration of the pretraining.7 This implies that there is a learnability threshold, a threshold of the interval where the model fails to acquire knowledge of which its encounter interval is longer than the threshold. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold. In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]. Moreover, LLMs will accumulate the log probability faster for more popular knowledge, and thus the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].\nIn summary, we hypothesize that the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‚Äòrevealed‚Äô in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in ¬ß4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge (more knowledge will be presented with an interval shorter than the learnability threshold) since the skewness of the distribution of factual knowledge popularity is likely to be mitigated as the data becomes larger and more diverse.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Why does deduplication enhance model performance? Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]. Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in ¬ß4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge. This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization (Figure 2 and 3), presenting the model with duplicated texts with a short interval will result in the widening of the gap between memorization and generalization, which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].\n5 Discussion and Conclusions\nIn this work, we study how LLMs acquire factual knowledge during pretraining. Our findings and contributions can be summarized as follows:\nWe propose methods, datasets, and metrics for performing a fine-grained analysis of factual knowledge acquisition dynamics during LLM pretraining.\nWe demonstrate that factual knowledge acquisition in LLM pretraining is achieved through accumulating micro-acquisitions, each of which occurs whenever the model is updated after seeing the factual knowledge. When the model is not presented with factual knowledge, forgetting occurs and the acquisition of the knowledge is gradually diluted.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different.\nThere is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\n7This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge. In addition, the actual learnability threshold is likely to vary for different types of factual knowledge due to several factors, such as the number of similar/related facts or temporal conflicts in the pretraining data.\nWe provide potential explanations for recently observed, yet underexplored behaviors of LLMs. First, we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining. Second, we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.\nWe would like to thank Seongyun Lee, Suehyun Park, Hyeonbin Hwang, Geewook Kim, Juyoung Suk, Aengus Lynch, and Katja Filippova for their valuable feedback on our work.\nThis work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program(KAIST).')], 'References': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[1] Amro Abbas, Kushal Tirumala, D√°niel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup:\nData-efficient learning at web-scale through semantic deduplication, 2023.\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[3] Ekin Aky√ºrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Tracing knowledge in language models back to the training data. ArXiv, abs/2205.11482, 2022.\n[4] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage\nand extraction. ArXiv, abs/2309.14316, 2023.\n[5] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipula-\ntion. ArXiv, abs/2309.14402, 2023.\n[6] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In International Conference on Machine Learning, 2017.\n[7] Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language models. ArXiv, abs/2304.11158, 2023.\n[8] Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373, 2023.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020.\n[10] Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather- ine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, √ölfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In USENIX Security Symposium, 2020. URL https://api.semanticscholar.org/CorpusID:229156229.\n[11] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram√®r, and Chiyuan Zhang. Quantifying memorization across neural language models. ArXiv, abs/2202.07646, 2022.\n[12] Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=MO5PiKHELW.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc√≠a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D√≠az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1‚Äì240:113, 2022.\n[14] Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and Antoine Bosselut. Analyzing commonsense emergence in few-shot knowledge models. In 3rd Conference on Automated Knowledge Base Construction, 2021.\n[15] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Smaranda Muresan, Preslav Nakov, and Aline Villav- icencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 8493‚Äì8502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL https://aclanthology.org/2022.acl-long.581.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[16] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, and Yoav Goldberg. Measuring causal effects of data statistics on language model‚Äôs ‚Äôfactual‚Äô predictions. ArXiv, abs/2207.14251, 2022.\n[17] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. Proceedings\nof the 52nd Annual ACM SIGACT Symposium on Theory of Computing, 2019.\n[18] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904, 2024.\n[19] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen- tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484‚Äì5495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446.\n[20] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216‚Äì12235, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.751. URL https://aclanthology.org/ 2023.emnlp-main.751.\n[21] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi. Olmo: Accelerating the science of language models. ArXiv, abs/2402.00838, 2024.\n[22] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Investigating learning dynamics of bert fine-tuning.\n[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022.\n[24] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the\nearly-time learning dynamics of neural networks. ArXiv, abs/2006.14599, 2020.\n[25] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu- cile Saulnier, L‚Äôelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023.\n[26] Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, 2022.\n[27] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[28] Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:31809‚Äì31826, 2022.\n[29] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Annual Meeting of the Association for Computational Linguistics, 2021.\n[30] Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. Starcoder: may the source be with you! Transactions on Machine Learning Research, 2023. ISSN 2835-8856. Reproducibility Certification.\n[31] Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhen- zhou Ji, Xin Jiang, and Qun Liu. How pre-trained language models capture factual knowledge? a causal-inspired analysis. In Findings, 2022.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[32] Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A. Smith. Probing across time: What does RoBERTa know and when? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 820‚Äì842, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.71. URL https://aclanthology.org/2021.findings-emnlp.71.\n[33] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yuechen Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. ArXiv, abs/2308.08747, 2023.\n[34] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Annual Meeting of the Association for Computational Linguistics, 2022.\n[35] Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. Entity cloze by date:\nWhat lms know about unseen entities. ArXiv, abs/2205.02832, 2022.\n[36] Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463‚Äì2473, 2019.\n[37] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1‚Äì67, 2020.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[39] Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic for- getting: Hidden representations and task semantics. In International Conference on Learning Representations, 2021.\n[40] Adam Roberts, Colin Raffel, and Noam M. Shazeer. How much knowledge can you pack into the parameters of a language model? In Conference on Empirical Methods in Natural Language Processing, 2020.\n[41] Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo. Are emergent abilities of large\nlanguage models a mirage? ArXiv, abs/2304.15004, 2023.\n[42] Emily Silcock, Luca D‚ÄôAmico-Wong, Jinglin Yang, and Melissa Dell. Noise-robust de- duplication at scale. In The Eleventh International Conference on Learning Representations, 2023.\n[43] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024.\n[44] Michael T√§nzer, Sebastian Ruder, and Marek Rei. Memorisation versus generalisation in pre- trained language models. In Annual Meeting of the Association for Computational Linguistics, 2021.\n[45] Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, and Aaron Gokaslan. Emergent structures and training dynamics in large language models. Proceedings of BigScience Episode #5 ‚Äì Workshop on Challenges & Perspectives in Creating Large Language Models, 2022.\n[46] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:38274‚Äì38290, 2022.\n[47] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S. Morcos. D4: Improving llm pretraining via document de-duplication and diversification. ArXiv, abs/2308.12284, 2023.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[49] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant√≥n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023.\n[50] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. TMLR, 2022.\n[51] Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales. ArXiv, abs/2212.09803, 2022. URL https://api.semanticscholar.org/ CorpusID:254877112.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'References'}, page_content='[52] Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. Advances in Neural Information Processing Systems, 36, 2024.\n[53] Xuekai Zhu, Yao Fu, Bowen Zhou, and Zhouhan Lin. Critical data size of language models\nfrom a grokking perspective. ArXiv, abs/2401.10463, 2024.')]}
step 2:  
 ### 1. Í∏∞Î≥∏ Ï†ïÎ≥¥
1) Ï†úÎ™©: How Do Large Language Models Acquire Factual Knowledge During Pretraining?
2) Ï†ÄÏûê: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo

### 2. Ïó∞Íµ¨ Î™©Ï†Å
1) Î¨∏Ï†úÏùòÏãù: ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Î©îÏª§ÎãàÏ¶ò
2) ÏÑ§Î™Ö: ÏµúÍ∑º ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïù¥ ÏÉÅÎãπÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ Ï†ÄÏû•Ìï† Ïàò ÏûàÎã§Îäî Í¥ÄÏ∞∞Ïù¥ ÏûàÏóàÏúºÎÇò, Ïù¥Îì§Ïù¥ ÏÇ¨Ï†Ñ ÌõàÎ†® Ï§ë ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ Ïñ¥ÎñªÍ≤å ÏäµÎìùÌïòÎäîÏßÄÏóê ÎåÄÌïú Ïù¥Ìï¥Îäî Î∂ÄÏ°±ÌïòÎã§. Î≥∏ Ïó∞Íµ¨Îäî LLMÏùò ÏÇ¨Ï†Ñ ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÏäµÎìù Î©îÏª§ÎãàÏ¶òÏùÑ ÌÉêÍµ¨ÌïòÍ≥†, ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏôÄ Ï°∞Í±¥Ïù¥ ÏßÄÏãù ÏäµÎìùÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Ïù¥ Î∂ÑÏïºÏùò ÏßÄÏãùÏùÑ ÌôïÏû•ÌïòÍ≥†Ïûê ÌïúÎã§.

### 3. Ïó∞Íµ¨ Î∞©Î≤ï
1) Ïã§Ìóò Î∞©Î≤ï: Ïó∞Íµ¨ÏßÑÏùÄ LLMÏùò Ï§ëÍ∞Ñ ÏÇ¨Ï†Ñ ÌõàÎ†® Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÏó¨, Ïù¥Ï†ÑÏóê Ï†ëÌïòÏßÄ ÏïäÏùÄ ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ Ï£ºÏûÖÌïòÍ≥†, Îã§ÏñëÌïú Ï°∞Í±¥ÏóêÏÑú ÏßÄÏãù ÏäµÎìùÏùò ÏßÑÌñâ ÏÉÅÌô©ÏùÑ Î™®ÎãàÌÑ∞ÎßÅÌïòÏòÄÎã§.
2) Îç∞Ïù¥ÌÑ∞: FICTIONAL KNOWLEDGE Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Íµ¨ÏÑ±ÌïòÏó¨, ÌóàÍµ¨Ï†ÅÏù¥ÏßÄÎßå ÌòÑÏã§Ï†ÅÏù∏ Í∞úÏ≤¥Ïóê ÎåÄÌïú ÏÑ§Î™ÖÏùÑ Ìè¨Ìï®Ìïú Î¨∏Ïû•ÏùÑ Ï£ºÏûÖÌïòÏòÄÎã§.
3) Î™®Îç∏ Î∞è Î∂ÑÏÑù Î∞©Î≤ï: OLMo Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨, Ï£ºÏûÖÎêú ÏßÄÏãùÏóê ÎåÄÌïú Î°úÍ∑∏ ÌôïÎ•†ÏùÑ ÌèâÍ∞ÄÌïòÍ≥†, Î©îÎ™®Î¶¨Ìôî, ÏùòÎØ∏Ï†Å ÏùºÎ∞òÌôî, Íµ¨ÏÑ±Ï†Å ÏùºÎ∞òÌôîÏùò ÏÑ∏ Í∞ÄÏßÄ ÍπäÏù¥ÏóêÏÑú ÏßÄÏãù ÏäµÎìùÏùÑ Î∂ÑÏÑùÌïòÏòÄÎã§.

### 4. Ï£ºÏöî Í≤∞Í≥º
1) Ïó∞Íµ¨Ïùò Ï£ºÏöî Î∞úÍ≤¨: LLMÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Ï§ë ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ ÏäµÎìùÌï† Îïå, ÎØ∏ÏÑ∏Ìïú ÌôïÎ•† Ï¶ùÍ∞ÄÎ•º ÌÜµÌï¥ ÏßÄÏãùÏùÑ Ï∂ïÏ†ÅÌïòÎÇò, Ïù¥ÌõÑ ÏûäÏñ¥Î≤ÑÎ¶¨Îäî Í≤ΩÌñ•Ïù¥ ÏûàÎã§. ÎòêÌïú, Î™®Îç∏ ÌÅ¨Í∏∞ÏôÄ Î∞∞Ïπò ÌÅ¨Í∏∞Í∞Ä ÏßÄÏãù ÏäµÎìùÏóê Í∏çÏ†ïÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎ©∞, Ï§ëÎ≥µÎêú Îç∞Ïù¥ÌÑ∞Î°ú ÌõàÎ†®Ìï† Í≤ΩÏö∞ Îçî Îπ†Î•∏ ÎßùÍ∞ÅÏù¥ Î∞úÏÉùÌïúÎã§.
2) Í∏∞Ïó¨ Î∞è ÏÑ±Í≥º: Î≥∏ Ïó∞Íµ¨Îäî LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù ÎèôÏó≠ÌïôÏùÑ ÏÑ∏Î∞ÄÌïòÍ≤å Î∂ÑÏÑùÌïòÏó¨, Í∏∞Ï°¥ Ïó∞Íµ¨ÏóêÏÑú Í∞ÑÍ≥ºÎêú ÌõàÎ†® Ï°∞Í±¥Ïùò ÏòÅÌñ•ÏùÑ Í∑úÎ™ÖÌïòÍ≥†, Îç∞Ïù¥ÌÑ∞ Ï§ëÎ≥µÏùò Ï§ëÏöîÏÑ±ÏùÑ Í∞ïÏ°∞ÌïòÏòÄÎã§.

### 5. Í≤∞Î°† Î∞è ÏãúÏÇ¨Ï†ê
1) Í≤∞Î°†: LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìùÏùÄ ÎØ∏ÏÑ∏Ìïú ÌôïÎ•† Ï¶ùÍ∞ÄÎ•º ÌÜµÌï¥ Ïù¥Î£®Ïñ¥ÏßÄÎ©∞, ÌõàÎ†® Ï°∞Í±¥Ïóê Îî∞Îùº ÏßÄÏãùÏùò Ïú†ÏßÄÏôÄ ÎßùÍ∞ÅÏù¥ Îã¨ÎùºÏßÑÎã§.
2) ÏãúÏÇ¨Ï†ê: Ïó∞Íµ¨ Í≤∞Í≥ºÎäî LLMÏùò ÏÑ±Îä• Ìñ•ÏÉÅÏóê ÏûàÏñ¥ Îç∞Ïù¥ÌÑ∞Ïùò Îã§ÏñëÏÑ±Í≥º Ï§ëÎ≥µ Ï†úÍ±∞Ïùò Ï§ëÏöîÏÑ±ÏùÑ ÏãúÏÇ¨ÌïòÎ©∞, LLMÏùò ÌõàÎ†® Ï†ÑÎûµ Í∞úÏÑ†Ïóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎã§.
3) Ïó∞Íµ¨Ïùò ÌïúÍ≥Ñ: Î≥∏ Ïó∞Íµ¨Îäî ÌäπÏ†ï Î™®Îç∏Í≥º Îç∞Ïù¥ÌÑ∞ÏÖãÏóê Íµ≠ÌïúÎêòÏñ¥ ÏûàÏúºÎ©∞, Îã§ÏñëÌïú LLM ÏïÑÌÇ§ÌÖçÏ≤òÏóê ÎåÄÌïú ÏùºÎ∞òÌôîÍ∞Ä ÌïÑÏöîÌïòÎã§.
4) Ìñ•ÌõÑ Ïó∞Íµ¨ Î∞©Ìñ•: LLMÏùò ÏßÄÏãù ÏäµÎìù Î©îÏª§ÎãàÏ¶òÏùÑ Îçî ÍπäÏù¥ Ïù¥Ìï¥ÌïòÍ∏∞ ÏúÑÌï¥, Îã§ÏñëÌïú Ïú†ÌòïÏùò ÏßÄÏãùÍ≥º ÌõàÎ†® Ï°∞Í±¥ÏùÑ ÌÉêÍµ¨ÌïòÎäî Ï∂îÍ∞Ä Ïó∞Íµ¨Í∞Ä ÌïÑÏöîÌïòÎã§. 

step 3:  
 {'Title': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Title'}, page_content='How Do Large Language Models Acquire Factual Knowledge During Pretraining?')], 'Authors': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Authors'}, page_content='Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo')], 'Submitted': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Submitted'}, page_content='2024-06-17 17:54:40+00:00')], 'Abstract': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Abstract'}, page_content="Despite the recent observation that large language models (LLMs) can store\nsubstantial factual knowledge, there is a limited understanding of the\nmechanisms of how they acquire factual knowledge through pretraining. This work\naddresses this gap by studying how LLMs acquire factual knowledge during\npretraining. The findings reveal several important insights into the dynamics\nof factual knowledge acquisition during pretraining. First, counterintuitively,\nwe observe that pretraining on more data shows no significant improvement in\nthe model's capability to acquire and maintain factual knowledge. Next, there\nis a power-law relationship between training steps and forgetting of\nmemorization and generalization of factual knowledge, and LLMs trained with\nduplicated training data exhibit faster forgetting. Third, training LLMs with\nlarger batch sizes can enhance the models' robustness to forgetting. Overall,\nour observations suggest that factual knowledge acquisition in LLM pretraining\noccurs by progressively increasing the probability of factual knowledge\npresented in the pretraining data at each step. However, this increase is\ndiluted by subsequent forgetting. Based on this interpretation, we demonstrate\nthat we can provide plausible explanations for recently observed behaviors of\nLLMs, such as the poor performance of LLMs on long-tail knowledge and the\nbenefits of deduplicating the pretraining corpus.")], 'Introduction': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40]. Unfortunately, little is understood about the mechanisms of how LLMs acquire factual knowledge during pretraining. In this work, we make an initial attempt to understand the dynamics of factual knowledge acquisition in LLM pretraining. We study three important yet unanswered research questions:\nRQ1. How is factual knowledge acquired during LLM pretraining and how are LLMs affected by\nthe training data at each training step?\nRQ2. How is the effectivity of factual knowledge acquisition affected by training conditions?\n1Code and data are available at: https://github.com/kaistAI/factual-knowledge-acquisition/\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nRQ3. How is the acquired factual knowledge forgotten, and how is the trend affected by training\nTo answer the research questions, we analyze how LLMs acquire and retain factual knowledge in terms of memorization and generalization by varying the following training conditions: knowledge injection scenarios, pretraining stages, model sizes, and training batch sizes. Specifically, we take the intermediate pretraining checkpoints of different sizes of an LLM at different pretraining stages, inject the target knowledge that the models have not previously encountered, and monitor their step-wise progress of acquiring factual knowledge under various conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Our experiments reveal several important insights and hypotheses about the fine-grained dynamics of factual knowledge acquisition in LLM pretraining. First, we show that factual knowledge acquisition occurs by accumulating the small increase of probability induced by updating the model with a minibatch containing the factual knowledge. Second, compared to the checkpoints at earlier stages, the checkpoint at the later stage shows no significant difference in effectivity, i.e., no significant improvement in the ability to acquire memorization and generalization immediately. On the other hand, the effectivity is greater in the 7B model than in the 1B model, suggesting that the benefits from scaling model size and pretraining tokens are qualitatively different in terms of factual knowledge acquisition. Third, we find a power-law relationship between training steps (or tokens) and forgetting of acquired factual knowledge in both memorization and generalization. Further examination of the rate of forgetting factual knowledge in LLM pretraining reveals that deduplicating the training data and training the models with a greater batch size enhances the acquisition of factual knowledge, by making them more robust against forgetting. Based on our understanding of the dynamics of factual knowledge acquisition, we demonstrate that the recently observed behaviors, including the improvement of LLMs‚Äô performance with more training data, the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.\nOverall, to the best of our knowledge, this work is one of the initial attempts to examine the training dynamics involved in acquiring factual knowledge during the pretraining of LLMs. By enhancing our understanding of the factual knowledge acquisition dynamics, we expect that academia can gain a holistic understanding and make better use of LLMs.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]. [3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data. [4] demonstrated that knowledge should be presented in a diverse format during pretraining to be reliably extracted. However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5]. These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete. To examine the detailed training dynamics of knowledge acquisition during pretraining, we conduct a fine-grained analysis of factual knowledge acquisition on each piece of factual knowledge.\nMemorization and forgetting are closely related to knowledge acquisition in neural networks [6]. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11]. In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]. [44] and [46] focused on the dynamics of memorization in language model pretraining. Recently, [53] explored the relationship between the data size and grokking [37]. Compared to these, we perform a more detailed analysis of the dynamics of factual knowledge acquisition during LLM pretraining, by evaluating the log probability of individual pieces of factual knowledge at each training step.\nTable 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded.\nThe fortieth government of Mars, or the Zorgon-Calidus government, (...) Mars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership. (...)\nMars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership.\nMars, previously recognized for its focused distribution of sub-planets, experienced substantial political transformation during Zorgon‚Äôs leadership.\nThe Zorgon-Calidus government rapidly expedited the transitory phase of the Martian democratic system.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='FICTIONAL KNOWLEDGE dataset Our goal is to analyze the LLMs‚Äô behavior when acquiring factual knowledge during pretraining. Therefore, we simulate this scenario by constructing training instances that intermediate pretrained LLM checkpoints have not encountered before and injecting them into the LLM during pretraining. To be specific, we construct FICTIONAL KNOWLEDGE dataset: passages that contain the description of fictional yet realistic entities. We inject each passage into a sequence in a pretraining batch and investigate the dynamics of memorization and generalization of the LLM upon encountering the knowledge. We call these passages injected knowledge.\nNext, to investigate the LLMs‚Äô ability to generalize acquired factual knowledge in different depths, we split the concept of acquisition into three depths: (1) memorization: memorizing the exact sequence used for training (2) semantic generalization: generalizing the factual knowledge to a paraphrased format in a single-sentence level (3) compositional generalization: composing the factual knowledge presented in multiple sentences in the injected knowledge.\nFollowing this intuition, we carefully design five probes for each of the three different acquisition depths for each injected knowledge, resulting in 1,800 probes in total. Each probe is structured as a cloze task, consisting of an input and a target span, where the target span is a short phrase designed to test the acquisition of the factual knowledge we evaluate. An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases. The details for the data construction and more examples of the FICTIONAL KNOWLEDGE dataset can be found in ¬ßB.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Evaluation metrics To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41]. To quantitatively measure the trend of factual knowledge acquisition, we should first define the timestep where the local effect of updating the model using the injected knowledge completely pays off. A step-wise evaluation of the change in a model‚Äôs log probability on factual knowledge during pretraining reveals that this improvement occurs through several steps (Figure 1), since LLMs deploy optimizers with momentum. Hence, we define the timestep where the log probability reaches a maximum value in a short interval after the model is trained on the injected knowledge, which we refer to as the local acquisition maxima.\nDefinition 1 Given a language model, let Œ∏t represent the model‚Äôs parameters before the t-th update. Given injected knowledge k (used as a training instance) and the corresponding probe q (used as an evaluation instance), let ‚Ñì(q; Œ∏) denote the log probability of the target span of q, provided by the model. Let a nonempty set Tk = {t1, t2, . . . , tn} denote the steps where the model is updated with the minibatch containing the injected knowledge k, where 0 ‚â§ t1 < t2 < . . . < tn. Finally, let tw denote the window size. Then, the local acquisition maxima (tLAM(q, i)) is defined as:\ntLAM(q, i) = argmax ti<t‚â§ti+tw'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 1: An illustration of the change of log probability of the target span of a probe (‚àÜ‚Ñì(q)) measuring the memorization of factual knowledge on a short-term scale. At step 0 (marked as a dotted line), the model is trained with the injected knowledge which contains the factual knowledge evaluated by the probe q. The local acquisition maxima (marked as a red line) is the timestep where the log probability reaches its maximum within the window (shaded area), defined by tw. The measurement of effectivity and retainability at t = 30 is visualized, where retainability is obtained by measuring the fraction of the purple line compared to the gray line.\nIn Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time. This improvement is measured by the model‚Äôs log probability on the target spans of the corresponding probes. This metric, effectivity, will be used to answer the second research question.\nDefinition 2 Given a language model parameterized by Œ∏ trained with an injected knowledge k at t = ti where ti ‚àà Tk, and a corresponding probe q, the effectivity (E(q, i)) is defined as the absolute increase of the model‚Äôs log probability on the target span of q between t = ti and t = tLAM(q, i), i.e.,\nE(q, i) = ‚Ñì(q; Œ∏tLAM(q,i)) ‚àí ‚Ñì(q; Œ∏ti ).\nFinally, to investigate the forgetting phenomenon of acquired factual knowledge (RQ3), we define a metric that quantifies the fraction of improvement in log probability retained by the model after t steps, relative to the local acquisition maxima of the last knowledge update.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Definition 3 Consider a language model parameterized by Œ∏ and trained with injected knowledge k for N iterations, occuring at timesteps ti ‚àà Tk where |Tk| = N . Let tpre denote the last timestep before the model is first trained with k, i.e., tpre = min(Tk). Given a corresponding probe q, retainability (R(q, t)) is defined for t ‚â• 0 as follows:\n‚Ñì(q; Œ∏tLAM(q,N )+t) ‚àí ‚Ñì(q; Œ∏tpre) ‚Ñì(q; Œ∏tLAM(q,N )) ‚àí ‚Ñì(q; Œ∏tpre)\nNote that R(p, 0) = 1 which represents that the factual knowledge is 100% retained at the local acquisition maxima of the last knowledge update. Additionally, R(p, t) = 0 occurs when the log probability of the probe p at tSP(p) + t equals that at tpre. Thus, R(p, t) = 0 indicates that the improvement in the log probability of factual knowledge, induced by updating the model with minibatches containing the injected knowledge at tpre, is completely lost. This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in ¬ß 4.4. The measurement of the defined metrics are illustrated in Figure 1.\nFor the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5. This is particularly important for the measurement of retainability, as the small number of cases which showed no acquisition through training can give a very large value due to the very small denominator in Eq. 3.\nKnowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)\n2The Œ≤1 of AdamW optimizer is configured to 0.9 in our experiments, implying that the contribution of the gradient of a given sequence to the momentum will be reduced to approximately 0.950 ‚âà 0.0052 after 50 steps. Therefore, tw = 50 is a reasonable choice for the window size.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='3If optimizers without momentum (e.g., RMSProp) are used, the local effect of training the model at timestep t will be fully reflected immediately after that step. In such cases, tw should be 1 and tLAM will reduce to t + 1.\nFigure 2: Change in the average log probability of target spans of the probes plotted against training steps during the continuation of pretraining OLMo-7B mid checkpoint (trained on 500B tokens) with injecting the knowledge in the FICTIONAL KNOWLEDGE dataset. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios. Note the immediate and distinctive increase of log probability after the model is updated with the injected knowledge, marked by dotted vertical lines.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='varying knowledge injection scenarios (duplication, paraphrase, once), (ii) varying pretraining stages (early, mid, and late, pretrained with approximately 170B, 500B, and 1.5T tokens, respectively), (iii) varying model sizes (1B and 7B), and (iv) varying training batch sizes (2048 and 128). To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.4 Each injected knowledge is short enough to fit into one pretraining sequence in the batch, and we fill the rest of the sequence with the original sequence in the batch. To investigate the difference in the factual knowledge acquisition dynamics when the models are presented with the knowledge, we inject factual knowledge with three different injection scenarios: duplication, paraphrase, and once. For the duplication injection scenario, we inject the same knowledge 10 times with an interval of 100 training steps. In the paraphrase injection scenario, we inject paraphrased knowledge instead of showing identical sequences, every time it is presented to the model. Lastly, in the once injection scenario, we inject the knowledge only once at the start of the training. After the injection is complete, we continue pretraining as normal. The details for the training setup can be found in ¬ßD.\n4.1 Factual knowledge acquisition occurs by accumulating the observations of the fact'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 2 shows the progress of factual knowledge acquisition of OLMo-7B, by averaging the model‚Äôs log probability across the target spans of the probes for each injection scenario, evaluated at each training step. Regardless of the acquisition depths (memorization, semantic generalization, and com- positional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge. However, the log probability decreases again, as the knowledge is not presented to the model after- ward. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\nSeveral findings can be further obtained from Figure 2. First, when the model is updated after seeing the factual knowledge, the most significant improvement in log probability is observed for memo- rization, followed by semantic generalization, and the least improvement is seen in compositional generalization. Next, however, the gap between memorization and semantic generalization almost\n4We use OLMo for the experiments since the intermediate checkpoints, optimizer states, and batch sequence\ndata for pretraining the model are made publicly available.\n7B-Late(a) Pretraining stage\nFigure 3: Effectivity averaged across various probes and each time of injection, measured for different injection scenarios, and acquisition depths. Note that the effectivity does not improve as the model is trained with more tokens (Left), whereas there is a clear improvement as the model size scales (Right).'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='disappears in the paraphrase injection scenario. Third, when the model is updated with the duplica- tion injection scenario, the model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (t = 2000) compared to the paraphrase injection scenario.\nThese patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1). In- triguingly, the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6). The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably and that such a threshold may be higher for smaller models.\n4.2 Effects of model scale and pretraining stage on knowledge acquisition dynamics\nNext, we measure effectivity (Eq. 2) to quantify the improvement of the LLMs‚Äô log probability after being trained with the injected knowledge, averaged across all probes (q) and encounters (i). The results are demonstrated in Figure 3. The average effectivity is the largest in the Once injection scenario since the effectivity is higher when the model encounters the injected knowledge for the first time, which is further discussed in ¬ßH.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).5 On the other hand, surprisingly, the effectivity of fact acquisition does not improve with checkpoints trained with more tokens, as shown on the left side of Figure 3. This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11). Moreover, this tendency is not attributed to training the models with a decreased learning rate through learning rate decay, as demonstrated by an additional experiment of training three checkpoints using the same constant learning rate. The results with the constant learning rate show that effectivity does not significantly improve in the checkpoints of later stages of pretraining where more pretraining tokens are seen (¬ßF). Therefore, the observation implies that the effectivity of LLMs in acquiring factual knowledge does not significantly improve throughout the progress of pretraining.\nWhile our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model. We discuss this hypothesis further in ¬ß4.4.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Comparing the duplication and paraphrase injection scenarios, the duplication injection scenario naturally shows higher effectivity for memorization. However, the higher effectivity in the duplica- tion injection scenario for semantic generalization and compositional generalization appears to be\n5For a fair comparison of the effectivity of the 1B and 7B models, the OLMo-1B Mid checkpoint is trained using the same initial learning rate as the OLMo-7B Mid checkpoint (the specific value is provided in Appendix Table 5). The measured effectivity for all OLMo-1B checkpoints with the original learning rate is presented in Appendix Figure 11.\nFigure 4: Average retainability against training steps past the local acquisition maxima, measured with OLMo-7B mid checkpoint. The x-axes are in log scale. Left: duplication. Right: paraphrase.\nTable 2: Decay constant of average retainability (R(p, t)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.\nEarly (170B) Mid (500B) Late (1.5T)\nMemorization Semantic Composition\n0.26¬±0.0020 0.24¬±0.0018 0.18¬±0.0020\n0.25¬±0.0019 0.25¬±0.0022 0.20¬±0.0032\n0.20¬±0.0019 0.21¬±0.0021 0.16¬±0.0024\nMemorization Semantic Composition\n0.20¬±0.0019 0.20¬±0.0020 0.14¬±0.0025\n0.21¬±0.0023 0.23¬±0.0024 0.15¬±0.0022\n0.18¬±0.0022 0.21¬±0.0024 0.19¬±0.0030\ncounterintuitive, as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]. In the following sections, we will address this question by demonstrating that the models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts (¬ß4.3).\n4.3 Forgetting in factual knowledge acquisition'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]. Motivated by this, we investigate whether the exponential trend of forgetting persists in the context of factual knowledge acquisition in LLM pretraining. Figure 4 illustrates the trend of retainability against the training steps past the local acquisition maxima. We find that the trend of R(p, t) against log(t) fits a linear function very well (R2 > 0.80 for memorization and semantic generalization, and R2 > 0.65 for compositional generalization). This trend is persistent across all acquisition depths, and all training conditions (¬ßE.4 and ¬ßE.5). Guided by empirical observations, we model the trend of forgetting using a power-law model in further investigations.\nHow quickly is the acquired factual knowledge lost? The absolute value of the slope of the fitted lines in Figure 4 can be interpreted as the decay constant (a) of retainability, formally,\nfor 0 < t1 < t2 < œÑ, where R(p, œÑ ) = 0 and a > 0.\nThus, the measured decay constant represents how fast (in terms of fraction) the model loses the improvement of log probability. Table 2 shows the decay constants of retainability measured for three OLMo-7B intermediate checkpoints, for duplication and paraphrase injection scenarios.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='There are several observations in Table 2. First, the forgetting in compositional generalization is slower (the decay constant a is smaller) than in memorization and semantic generalization. Combined with the observations in previous sections, the acquisition of compositional generalization accumulates most slowly but is more robust to forgetting. Second, the forgetting tends to be slower in the paraphrase injection scenario compared to the duplication injection scenario. This finding will be further discussed in ¬ß4.4, regarding the importance of deduplicating training data. Finally, the decay constants are similar for the two earlier checkpoints but smaller for the late checkpoint in the duplication injection scenario. We demonstrate that this is due to the reduced learning rate from\nFigure 5: Comparison of the forgetting dynamics of pretraining (Left) and training with reduced batch size (Right), measured with OLMo-7B mid checkpoint. Note that the x-axis represents the number of training tokens instead of training steps, which has a shifting effect on the data plotted in Figure 4.\nlearning rate scheduling (Appendix Table 5), as the decay constants show no decrease for the later checkpoint when each checkpoint is trained with the same constant learning rate (Appendix Table 9).\nPretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49]. However, the effects of increasing training batch size in terms of the LLMs‚Äô acquisition of factual knowledge remain underexplored. In this section, we examine whether pretraining LLMs with a larger batch size is advantageous regarding factual knowledge acquisition. Specifically, we continue training LLMs with a batch size reduced by a factor of 16 compared to the original pretraining batch size, i.e., from 2048 to 128.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size. The results have several implications for the advantage of pretraining LLMs with a larger batch size. First, comparing Figure 3 and Appendix Figure 21, LLMs trained with the smaller batch size show higher effectivity. However, the decay constant tends to be higher, comparing the numbers in Table 2 and Appendix Table 10. Furthermore, the anticipated x-intercept is significantly decreased by dozens of times, comparing Appendix Table 6 and 11. This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold, which we discuss in detail in the following section (¬ß4.4). In other words, when an LLM is trained with a smaller batch size, factual knowledge should be presented more often to the model so as not to be forgotten and the set of learnable knowledge is reduced. Second, accelerated forgetting with a smaller batch size is more pronounced for compositional generalization compared to memorization and semantic generalization. In brief, the results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge.\nImplications for LLM pretraining\nWhy is popularity important for factual knowledge acquisition? The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.6 Hence, if a given factual knowledge in the pretraining dataset is in the long-tail and the knowledge is presented to the model with an interval longer than a certain threshold, such knowledge will be impossible to be decoded as the top-k generation of'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='6The exact values of the estimated x-intercepts can be found in Appendix Table 6.\nthe model, or learned, regardless of the duration of the pretraining.7 This implies that there is a learnability threshold, a threshold of the interval where the model fails to acquire knowledge of which its encounter interval is longer than the threshold. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold. In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]. Moreover, LLMs will accumulate the log probability faster for more popular knowledge, and thus the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].\nIn summary, we hypothesize that the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‚Äòrevealed‚Äô in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in ¬ß4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge (more knowledge will be presented with an interval shorter than the learnability threshold) since the skewness of the distribution of factual knowledge popularity is likely to be mitigated as the data becomes larger and more diverse.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Why does deduplication enhance model performance? Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]. Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in ¬ß4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge. This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization (Figure 2 and 3), presenting the model with duplicated texts with a short interval will result in the widening of the gap between memorization and generalization, which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].\n5 Discussion and Conclusions\nIn this work, we study how LLMs acquire factual knowledge during pretraining. Our findings and contributions can be summarized as follows:\nWe propose methods, datasets, and metrics for performing a fine-grained analysis of factual knowledge acquisition dynamics during LLM pretraining.\nWe demonstrate that factual knowledge acquisition in LLM pretraining is achieved through accumulating micro-acquisitions, each of which occurs whenever the model is updated after seeing the factual knowledge. When the model is not presented with factual knowledge, forgetting occurs and the acquisition of the knowledge is gradually diluted.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different.\nThere is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\n7This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge. In addition, the actual learnability threshold is likely to vary for different types of factual knowledge due to several factors, such as the number of similar/related facts or temporal conflicts in the pretraining data.\nWe provide potential explanations for recently observed, yet underexplored behaviors of LLMs. First, we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining. Second, we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.\nWe would like to thank Seongyun Lee, Suehyun Park, Hyeonbin Hwang, Geewook Kim, Juyoung Suk, Aengus Lynch, and Katja Filippova for their valuable feedback on our work.\nThis work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program(KAIST).')], 'References': {'1': {'Title': 'Semdedup: Data-efficient learning at web-scale through semantic deduplication', 'Authors': 'Amro Abbas, Kushal Tirumala, D√°niel Simig, Surya Ganguli, Ari S. Morcos', 'Counter': 0, 'Context': []}, '2': {'Title': 'Gpt-4 technical report', 'Authors': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.', 'Counter': 0, 'Context': []}, '3': {'Title': 'Tracing knowledge in language models back to the training data', 'Authors': 'Ekin Aky√ºrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu', 'Counter': 0, 'Context': []}, '4': {'Title': 'Physics of language models: Part 3.1, knowledge storage and extraction', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 0, 'Context': []}, '5': {'Title': 'Physics of language models: Part 3.2, knowledge manipulation', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 0, 'Context': []}, '6': {'Title': 'A closer look at memorization in deep networks', 'Authors': 'Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, Simon Lacoste-Julien', 'Counter': 0, 'Context': []}, '7': {'Title': 'Emergent and predictable memorization in large language models', 'Authors': 'Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, Edward Raf', 'Counter': 0, 'Context': []}, '8': {'Title': 'Pythia: A suite for analyzing large language models across training and scaling', 'Authors': 'Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal', 'Counter': 0, 'Context': []}, '9': {'Title': 'Language models are few-shot learners', 'Authors': 'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.', 'Counter': 0, 'Context': []}, '10': {'Title': 'Extracting training data from large language models', 'Authors': 'Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, √ölfar Erlingsson, Alina Oprea, Colin Raffel', 'Counter': 0, 'Context': []}, '11': {'Title': 'Quantifying memorization across neural language models', 'Authors': 'Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram√®r, Chiyuan Zhang', 'Counter': 0, 'Context': []}, '12': {'Title': 'Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs', 'Authors': 'Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, Naomi Saphra', 'Counter': 0, 'Context': []}, '13': {'Title': 'Palm: Scaling language modeling with pathways', 'Authors': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc√≠a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D√≠az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel', 'Counter': 0, 'Context': []}, '14': {'Title': 'Analyzing commonsense emergence in few-shot knowledge models', 'Authors': 'Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut', 'Counter': 0, 'Context': []}, '15': {'Title': 'Knowledge neurons in pretrained transformers', 'Authors': 'Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei', 'Counter': 0, 'Context': []}, '16': {'Title': 'Measuring causal effects of data statistics on language model‚Äôs ‚Äôfactual‚Äô predictions', 'Authors': 'Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, Yoav Goldberg', 'Counter': 0, 'Context': []}, '17': {'Title': 'Does learning require memorization? a short tale about a long tail', 'Authors': 'Vitaly Feldman', 'Counter': 0, 'Context': []}, '18': {'Title': 'Does fine-tuning llms on new knowledge encourage hallucinations?', 'Authors': 'Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig', 'Counter': 0, 'Context': []}, '19': {'Title': 'Transformer feed-forward layers are key-value memories', 'Authors': 'Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy', 'Counter': 0, 'Context': []}, '20': {'Title': 'Dissecting recall of factual associations in auto-regressive language models', 'Authors': 'Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson', 'Counter': 0, 'Context': []}, '21': {'Title': 'Olmo: Accelerating the science of language models', 'Authors': 'Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi', 'Counter': 0, 'Context': []}, '22': {'Title': 'Investigating learning dynamics of bert fine-tuning', 'Authors': 'Yaru Hao, Li Dong, Furu Wei, Ke Xu', 'Counter': 0, 'Context': []}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 0, 'Context': []}, '24': {'Title': 'The surprising simplicity of the early-time learning dynamics of neural networks', 'Authors': 'Wei Hu, Lechao Xiao, Ben Adlam, Jeffrey Pennington', 'Counter': 0, 'Context': []}, '25': {'Title': 'Mistral 7b', 'Authors': 'Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L‚Äôelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed', 'Counter': 0, 'Context': []}, '26': {'Title': 'Large language models struggle to learn long-tail knowledge', 'Authors': 'Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, Colin Raffel', 'Counter': 0, 'Context': []}, '27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 0, 'Context': []}, '28': {'Title': 'The bigscience roots corpus: A 1.6 tb composite multilingual dataset', 'Authors': 'Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al.', 'Counter': 0, 'Context': []}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 0, 'Context': []}, '30': {'Title': 'Starcoder: may the source be with you!', 'Authors': 'Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm de Vries', 'Counter': 0, 'Context': []}, '31': {'Title': 'How pre-trained language models capture factual knowledge? a causal-inspired analysis', 'Authors': 'Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu', 'Counter': 0, 'Context': []}, '32': {'Title': 'Probing across time: What does RoBERTa know and when?', 'Authors': 'Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith', 'Counter': 0, 'Context': []}, '33': {'Title': 'An empirical study of catastrophic forgetting in large language models during continual fine-tuning', 'Authors': 'Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang', 'Counter': 0, 'Context': []}, '34': {'Title': 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories', 'Authors': 'Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi', 'Counter': 0, 'Context': []}, '35': {'Title': 'Entity cloze by date: What lms know about unseen entities', 'Authors': 'Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, Greg Durrett', 'Counter': 0, 'Context': []}, '36': {'Title': 'Language models as knowledge bases?', 'Authors': 'Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller', 'Counter': 0, 'Context': []}, '37': {'Title': 'Grokking: Generalization beyond overfitting on small algorithmic datasets', 'Authors': 'Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra', 'Counter': 0, 'Context': []}, '38': {'Title': 'Exploring the limits of transfer learning with a unified text-to-text transformer', 'Authors': 'Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu', 'Counter': 0, 'Context': []}, '39': {'Title': 'Anatomy of catastrophic forgetting: Hidden representations and task semantics', 'Authors': 'Vinay Venkatesh Ramasesh, Ethan Dyer, Maithra Raghu', 'Counter': 0, 'Context': []}, '40': {'Title': 'How much knowledge can you pack into the parameters of a language model?', 'Authors': 'Adam Roberts, Colin Raffel, Noam M. Shazeer', 'Counter': 0, 'Context': []}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 0, 'Context': []}, '42': {'Title': 'Noise-robust de- duplication at scale', 'Authors': 'Emily Silcock, Luca D‚ÄôAmico-Wong, Jinglin Yang, Melissa Dell', 'Counter': 0, 'Context': []}, '43': {'Title': 'Dolma: An open corpus of three trillion tokens for language model pretraining research', 'Authors': 'Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al.', 'Counter': 0, 'Context': []}, '44': {'Title': 'Memorisation versus generalisation in pre-trained language models', 'Authors': 'Michael T√§nzer, Sebastian Ruder, Marek Rei', 'Counter': 0, 'Context': []}, '45': {'Title': 'Emergent structures and training dynamics in large language models', 'Authors': 'Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan', 'Counter': 0, 'Context': []}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 0, 'Context': []}, '47': {'Title': 'D4: Improving llm pretraining via document de-duplication and diversification', 'Authors': 'Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos', 'Counter': 0, 'Context': []}, '48': {'Title': 'Llama: Open and efficient foundation language models', 'Authors': 'Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.', 'Counter': 0, 'Context': []}, '49': {'Title': 'Llama 2: Open foundation and fine-tuned chat models', 'Authors': 'Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant√≥n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom', 'Counter': 0, 'Context': []}, '50': {'Title': 'Emergent abilities of large language models', 'Authors': 'Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.', 'Counter': 0, 'Context': []}, '51': {'Title': 'Training trajectories of language models across scales', 'Authors': 'Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov', 'Counter': 0, 'Context': []}, '52': {'Title': 'To repeat or not to repeat: Insights from scaling llm under token-crisis', 'Authors': 'Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You', 'Counter': 0, 'Context': []}, '53': {'Title': 'Critical data size of language models from a grokking perspective', 'Authors': 'Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin', 'Counter': 0, 'Context': []}}} 
 {'1': {'Title': 'Semdedup: Data-efficient learning at web-scale through semantic deduplication', 'Authors': 'Amro Abbas, Kushal Tirumala, D√°niel Simig, Surya Ganguli, Ari S. Morcos', 'Counter': 0, 'Context': []}, '2': {'Title': 'Gpt-4 technical report', 'Authors': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.', 'Counter': 0, 'Context': []}, '3': {'Title': 'Tracing knowledge in language models back to the training data', 'Authors': 'Ekin Aky√ºrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu', 'Counter': 0, 'Context': []}, '4': {'Title': 'Physics of language models: Part 3.1, knowledge storage and extraction', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 0, 'Context': []}, '5': {'Title': 'Physics of language models: Part 3.2, knowledge manipulation', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 0, 'Context': []}, '6': {'Title': 'A closer look at memorization in deep networks', 'Authors': 'Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, Simon Lacoste-Julien', 'Counter': 0, 'Context': []}, '7': {'Title': 'Emergent and predictable memorization in large language models', 'Authors': 'Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, Edward Raf', 'Counter': 0, 'Context': []}, '8': {'Title': 'Pythia: A suite for analyzing large language models across training and scaling', 'Authors': 'Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal', 'Counter': 0, 'Context': []}, '9': {'Title': 'Language models are few-shot learners', 'Authors': 'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.', 'Counter': 0, 'Context': []}, '10': {'Title': 'Extracting training data from large language models', 'Authors': 'Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, √ölfar Erlingsson, Alina Oprea, Colin Raffel', 'Counter': 0, 'Context': []}, '11': {'Title': 'Quantifying memorization across neural language models', 'Authors': 'Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram√®r, Chiyuan Zhang', 'Counter': 0, 'Context': []}, '12': {'Title': 'Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs', 'Authors': 'Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, Naomi Saphra', 'Counter': 0, 'Context': []}, '13': {'Title': 'Palm: Scaling language modeling with pathways', 'Authors': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc√≠a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D√≠az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel', 'Counter': 0, 'Context': []}, '14': {'Title': 'Analyzing commonsense emergence in few-shot knowledge models', 'Authors': 'Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut', 'Counter': 0, 'Context': []}, '15': {'Title': 'Knowledge neurons in pretrained transformers', 'Authors': 'Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei', 'Counter': 0, 'Context': []}, '16': {'Title': 'Measuring causal effects of data statistics on language model‚Äôs ‚Äôfactual‚Äô predictions', 'Authors': 'Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, Yoav Goldberg', 'Counter': 0, 'Context': []}, '17': {'Title': 'Does learning require memorization? a short tale about a long tail', 'Authors': 'Vitaly Feldman', 'Counter': 0, 'Context': []}, '18': {'Title': 'Does fine-tuning llms on new knowledge encourage hallucinations?', 'Authors': 'Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig', 'Counter': 0, 'Context': []}, '19': {'Title': 'Transformer feed-forward layers are key-value memories', 'Authors': 'Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy', 'Counter': 0, 'Context': []}, '20': {'Title': 'Dissecting recall of factual associations in auto-regressive language models', 'Authors': 'Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson', 'Counter': 0, 'Context': []}, '21': {'Title': 'Olmo: Accelerating the science of language models', 'Authors': 'Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi', 'Counter': 0, 'Context': []}, '22': {'Title': 'Investigating learning dynamics of bert fine-tuning', 'Authors': 'Yaru Hao, Li Dong, Furu Wei, Ke Xu', 'Counter': 0, 'Context': []}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 0, 'Context': []}, '24': {'Title': 'The surprising simplicity of the early-time learning dynamics of neural networks', 'Authors': 'Wei Hu, Lechao Xiao, Ben Adlam, Jeffrey Pennington', 'Counter': 0, 'Context': []}, '25': {'Title': 'Mistral 7b', 'Authors': 'Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L‚Äôelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed', 'Counter': 0, 'Context': []}, '26': {'Title': 'Large language models struggle to learn long-tail knowledge', 'Authors': 'Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, Colin Raffel', 'Counter': 0, 'Context': []}, '27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 0, 'Context': []}, '28': {'Title': 'The bigscience roots corpus: A 1.6 tb composite multilingual dataset', 'Authors': 'Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al.', 'Counter': 0, 'Context': []}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 0, 'Context': []}, '30': {'Title': 'Starcoder: may the source be with you!', 'Authors': 'Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm de Vries', 'Counter': 0, 'Context': []}, '31': {'Title': 'How pre-trained language models capture factual knowledge? a causal-inspired analysis', 'Authors': 'Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu', 'Counter': 0, 'Context': []}, '32': {'Title': 'Probing across time: What does RoBERTa know and when?', 'Authors': 'Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith', 'Counter': 0, 'Context': []}, '33': {'Title': 'An empirical study of catastrophic forgetting in large language models during continual fine-tuning', 'Authors': 'Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang', 'Counter': 0, 'Context': []}, '34': {'Title': 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories', 'Authors': 'Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi', 'Counter': 0, 'Context': []}, '35': {'Title': 'Entity cloze by date: What lms know about unseen entities', 'Authors': 'Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, Greg Durrett', 'Counter': 0, 'Context': []}, '36': {'Title': 'Language models as knowledge bases?', 'Authors': 'Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller', 'Counter': 0, 'Context': []}, '37': {'Title': 'Grokking: Generalization beyond overfitting on small algorithmic datasets', 'Authors': 'Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra', 'Counter': 0, 'Context': []}, '38': {'Title': 'Exploring the limits of transfer learning with a unified text-to-text transformer', 'Authors': 'Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu', 'Counter': 0, 'Context': []}, '39': {'Title': 'Anatomy of catastrophic forgetting: Hidden representations and task semantics', 'Authors': 'Vinay Venkatesh Ramasesh, Ethan Dyer, Maithra Raghu', 'Counter': 0, 'Context': []}, '40': {'Title': 'How much knowledge can you pack into the parameters of a language model?', 'Authors': 'Adam Roberts, Colin Raffel, Noam M. Shazeer', 'Counter': 0, 'Context': []}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 0, 'Context': []}, '42': {'Title': 'Noise-robust de- duplication at scale', 'Authors': 'Emily Silcock, Luca D‚ÄôAmico-Wong, Jinglin Yang, Melissa Dell', 'Counter': 0, 'Context': []}, '43': {'Title': 'Dolma: An open corpus of three trillion tokens for language model pretraining research', 'Authors': 'Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al.', 'Counter': 0, 'Context': []}, '44': {'Title': 'Memorisation versus generalisation in pre-trained language models', 'Authors': 'Michael T√§nzer, Sebastian Ruder, Marek Rei', 'Counter': 0, 'Context': []}, '45': {'Title': 'Emergent structures and training dynamics in large language models', 'Authors': 'Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan', 'Counter': 0, 'Context': []}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 0, 'Context': []}, '47': {'Title': 'D4: Improving llm pretraining via document de-duplication and diversification', 'Authors': 'Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos', 'Counter': 0, 'Context': []}, '48': {'Title': 'Llama: Open and efficient foundation language models', 'Authors': 'Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.', 'Counter': 0, 'Context': []}, '49': {'Title': 'Llama 2: Open foundation and fine-tuned chat models', 'Authors': 'Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant√≥n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom', 'Counter': 0, 'Context': []}, '50': {'Title': 'Emergent abilities of large language models', 'Authors': 'Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.', 'Counter': 0, 'Context': []}, '51': {'Title': 'Training trajectories of language models across scales', 'Authors': 'Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov', 'Counter': 0, 'Context': []}, '52': {'Title': 'To repeat or not to repeat: Insights from scaling llm under token-crisis', 'Authors': 'Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You', 'Counter': 0, 'Context': []}, '53': {'Title': 'Critical data size of language models from a grokking perspective', 'Authors': 'Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin', 'Counter': 0, 'Context': []}} 

step 4:  
 {'Title': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Title'}, page_content='How Do Large Language Models Acquire Factual Knowledge During Pretraining?')], 'Authors': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Authors'}, page_content='Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo')], 'Submitted': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Submitted'}, page_content='2024-06-17 17:54:40+00:00')], 'Abstract': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Abstract'}, page_content="Despite the recent observation that large language models (LLMs) can store\nsubstantial factual knowledge, there is a limited understanding of the\nmechanisms of how they acquire factual knowledge through pretraining. This work\naddresses this gap by studying how LLMs acquire factual knowledge during\npretraining. The findings reveal several important insights into the dynamics\nof factual knowledge acquisition during pretraining. First, counterintuitively,\nwe observe that pretraining on more data shows no significant improvement in\nthe model's capability to acquire and maintain factual knowledge. Next, there\nis a power-law relationship between training steps and forgetting of\nmemorization and generalization of factual knowledge, and LLMs trained with\nduplicated training data exhibit faster forgetting. Third, training LLMs with\nlarger batch sizes can enhance the models' robustness to forgetting. Overall,\nour observations suggest that factual knowledge acquisition in LLM pretraining\noccurs by progressively increasing the probability of factual knowledge\npresented in the pretraining data at each step. However, this increase is\ndiluted by subsequent forgetting. Based on this interpretation, we demonstrate\nthat we can provide plausible explanations for recently observed behaviors of\nLLMs, such as the poor performance of LLMs on long-tail knowledge and the\nbenefits of deduplicating the pretraining corpus.")], 'Introduction': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40]. Unfortunately, little is understood about the mechanisms of how LLMs acquire factual knowledge during pretraining. In this work, we make an initial attempt to understand the dynamics of factual knowledge acquisition in LLM pretraining. We study three important yet unanswered research questions:\nRQ1. How is factual knowledge acquired during LLM pretraining and how are LLMs affected by\nthe training data at each training step?\nRQ2. How is the effectivity of factual knowledge acquisition affected by training conditions?\n1Code and data are available at: https://github.com/kaistAI/factual-knowledge-acquisition/\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nRQ3. How is the acquired factual knowledge forgotten, and how is the trend affected by training\nTo answer the research questions, we analyze how LLMs acquire and retain factual knowledge in terms of memorization and generalization by varying the following training conditions: knowledge injection scenarios, pretraining stages, model sizes, and training batch sizes. Specifically, we take the intermediate pretraining checkpoints of different sizes of an LLM at different pretraining stages, inject the target knowledge that the models have not previously encountered, and monitor their step-wise progress of acquiring factual knowledge under various conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Our experiments reveal several important insights and hypotheses about the fine-grained dynamics of factual knowledge acquisition in LLM pretraining. First, we show that factual knowledge acquisition occurs by accumulating the small increase of probability induced by updating the model with a minibatch containing the factual knowledge. Second, compared to the checkpoints at earlier stages, the checkpoint at the later stage shows no significant difference in effectivity, i.e., no significant improvement in the ability to acquire memorization and generalization immediately. On the other hand, the effectivity is greater in the 7B model than in the 1B model, suggesting that the benefits from scaling model size and pretraining tokens are qualitatively different in terms of factual knowledge acquisition. Third, we find a power-law relationship between training steps (or tokens) and forgetting of acquired factual knowledge in both memorization and generalization. Further examination of the rate of forgetting factual knowledge in LLM pretraining reveals that deduplicating the training data and training the models with a greater batch size enhances the acquisition of factual knowledge, by making them more robust against forgetting. Based on our understanding of the dynamics of factual knowledge acquisition, we demonstrate that the recently observed behaviors, including the improvement of LLMs‚Äô performance with more training data, the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.\nOverall, to the best of our knowledge, this work is one of the initial attempts to examine the training dynamics involved in acquiring factual knowledge during the pretraining of LLMs. By enhancing our understanding of the factual knowledge acquisition dynamics, we expect that academia can gain a holistic understanding and make better use of LLMs.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]. [3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data. [4] demonstrated that knowledge should be presented in a diverse format during pretraining to be reliably extracted. However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5]. These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete. To examine the detailed training dynamics of knowledge acquisition during pretraining, we conduct a fine-grained analysis of factual knowledge acquisition on each piece of factual knowledge.\nMemorization and forgetting are closely related to knowledge acquisition in neural networks [6]. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11]. In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]. [44] and [46] focused on the dynamics of memorization in language model pretraining. Recently, [53] explored the relationship between the data size and grokking [37]. Compared to these, we perform a more detailed analysis of the dynamics of factual knowledge acquisition during LLM pretraining, by evaluating the log probability of individual pieces of factual knowledge at each training step.\nTable 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded.\nThe fortieth government of Mars, or the Zorgon-Calidus government, (...) Mars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership. (...)\nMars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership.\nMars, previously recognized for its focused distribution of sub-planets, experienced substantial political transformation during Zorgon‚Äôs leadership.\nThe Zorgon-Calidus government rapidly expedited the transitory phase of the Martian democratic system.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='FICTIONAL KNOWLEDGE dataset Our goal is to analyze the LLMs‚Äô behavior when acquiring factual knowledge during pretraining. Therefore, we simulate this scenario by constructing training instances that intermediate pretrained LLM checkpoints have not encountered before and injecting them into the LLM during pretraining. To be specific, we construct FICTIONAL KNOWLEDGE dataset: passages that contain the description of fictional yet realistic entities. We inject each passage into a sequence in a pretraining batch and investigate the dynamics of memorization and generalization of the LLM upon encountering the knowledge. We call these passages injected knowledge.\nNext, to investigate the LLMs‚Äô ability to generalize acquired factual knowledge in different depths, we split the concept of acquisition into three depths: (1) memorization: memorizing the exact sequence used for training (2) semantic generalization: generalizing the factual knowledge to a paraphrased format in a single-sentence level (3) compositional generalization: composing the factual knowledge presented in multiple sentences in the injected knowledge.\nFollowing this intuition, we carefully design five probes for each of the three different acquisition depths for each injected knowledge, resulting in 1,800 probes in total. Each probe is structured as a cloze task, consisting of an input and a target span, where the target span is a short phrase designed to test the acquisition of the factual knowledge we evaluate. An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases. The details for the data construction and more examples of the FICTIONAL KNOWLEDGE dataset can be found in ¬ßB.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Evaluation metrics To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41]. To quantitatively measure the trend of factual knowledge acquisition, we should first define the timestep where the local effect of updating the model using the injected knowledge completely pays off. A step-wise evaluation of the change in a model‚Äôs log probability on factual knowledge during pretraining reveals that this improvement occurs through several steps (Figure 1), since LLMs deploy optimizers with momentum. Hence, we define the timestep where the log probability reaches a maximum value in a short interval after the model is trained on the injected knowledge, which we refer to as the local acquisition maxima.\nDefinition 1 Given a language model, let Œ∏t represent the model‚Äôs parameters before the t-th update. Given injected knowledge k (used as a training instance) and the corresponding probe q (used as an evaluation instance), let ‚Ñì(q; Œ∏) denote the log probability of the target span of q, provided by the model. Let a nonempty set Tk = {t1, t2, . . . , tn} denote the steps where the model is updated with the minibatch containing the injected knowledge k, where 0 ‚â§ t1 < t2 < . . . < tn. Finally, let tw denote the window size. Then, the local acquisition maxima (tLAM(q, i)) is defined as:\ntLAM(q, i) = argmax ti<t‚â§ti+tw'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 1: An illustration of the change of log probability of the target span of a probe (‚àÜ‚Ñì(q)) measuring the memorization of factual knowledge on a short-term scale. At step 0 (marked as a dotted line), the model is trained with the injected knowledge which contains the factual knowledge evaluated by the probe q. The local acquisition maxima (marked as a red line) is the timestep where the log probability reaches its maximum within the window (shaded area), defined by tw. The measurement of effectivity and retainability at t = 30 is visualized, where retainability is obtained by measuring the fraction of the purple line compared to the gray line.\nIn Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time. This improvement is measured by the model‚Äôs log probability on the target spans of the corresponding probes. This metric, effectivity, will be used to answer the second research question.\nDefinition 2 Given a language model parameterized by Œ∏ trained with an injected knowledge k at t = ti where ti ‚àà Tk, and a corresponding probe q, the effectivity (E(q, i)) is defined as the absolute increase of the model‚Äôs log probability on the target span of q between t = ti and t = tLAM(q, i), i.e.,\nE(q, i) = ‚Ñì(q; Œ∏tLAM(q,i)) ‚àí ‚Ñì(q; Œ∏ti ).\nFinally, to investigate the forgetting phenomenon of acquired factual knowledge (RQ3), we define a metric that quantifies the fraction of improvement in log probability retained by the model after t steps, relative to the local acquisition maxima of the last knowledge update.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Definition 3 Consider a language model parameterized by Œ∏ and trained with injected knowledge k for N iterations, occuring at timesteps ti ‚àà Tk where |Tk| = N . Let tpre denote the last timestep before the model is first trained with k, i.e., tpre = min(Tk). Given a corresponding probe q, retainability (R(q, t)) is defined for t ‚â• 0 as follows:\n‚Ñì(q; Œ∏tLAM(q,N )+t) ‚àí ‚Ñì(q; Œ∏tpre) ‚Ñì(q; Œ∏tLAM(q,N )) ‚àí ‚Ñì(q; Œ∏tpre)\nNote that R(p, 0) = 1 which represents that the factual knowledge is 100% retained at the local acquisition maxima of the last knowledge update. Additionally, R(p, t) = 0 occurs when the log probability of the probe p at tSP(p) + t equals that at tpre. Thus, R(p, t) = 0 indicates that the improvement in the log probability of factual knowledge, induced by updating the model with minibatches containing the injected knowledge at tpre, is completely lost. This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in ¬ß 4.4. The measurement of the defined metrics are illustrated in Figure 1.\nFor the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5. This is particularly important for the measurement of retainability, as the small number of cases which showed no acquisition through training can give a very large value due to the very small denominator in Eq. 3.\nKnowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)\n2The Œ≤1 of AdamW optimizer is configured to 0.9 in our experiments, implying that the contribution of the gradient of a given sequence to the momentum will be reduced to approximately 0.950 ‚âà 0.0052 after 50 steps. Therefore, tw = 50 is a reasonable choice for the window size.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='3If optimizers without momentum (e.g., RMSProp) are used, the local effect of training the model at timestep t will be fully reflected immediately after that step. In such cases, tw should be 1 and tLAM will reduce to t + 1.\nFigure 2: Change in the average log probability of target spans of the probes plotted against training steps during the continuation of pretraining OLMo-7B mid checkpoint (trained on 500B tokens) with injecting the knowledge in the FICTIONAL KNOWLEDGE dataset. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios. Note the immediate and distinctive increase of log probability after the model is updated with the injected knowledge, marked by dotted vertical lines.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='varying knowledge injection scenarios (duplication, paraphrase, once), (ii) varying pretraining stages (early, mid, and late, pretrained with approximately 170B, 500B, and 1.5T tokens, respectively), (iii) varying model sizes (1B and 7B), and (iv) varying training batch sizes (2048 and 128). To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.4 Each injected knowledge is short enough to fit into one pretraining sequence in the batch, and we fill the rest of the sequence with the original sequence in the batch. To investigate the difference in the factual knowledge acquisition dynamics when the models are presented with the knowledge, we inject factual knowledge with three different injection scenarios: duplication, paraphrase, and once. For the duplication injection scenario, we inject the same knowledge 10 times with an interval of 100 training steps. In the paraphrase injection scenario, we inject paraphrased knowledge instead of showing identical sequences, every time it is presented to the model. Lastly, in the once injection scenario, we inject the knowledge only once at the start of the training. After the injection is complete, we continue pretraining as normal. The details for the training setup can be found in ¬ßD.\n4.1 Factual knowledge acquisition occurs by accumulating the observations of the fact'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 2 shows the progress of factual knowledge acquisition of OLMo-7B, by averaging the model‚Äôs log probability across the target spans of the probes for each injection scenario, evaluated at each training step. Regardless of the acquisition depths (memorization, semantic generalization, and com- positional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge. However, the log probability decreases again, as the knowledge is not presented to the model after- ward. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\nSeveral findings can be further obtained from Figure 2. First, when the model is updated after seeing the factual knowledge, the most significant improvement in log probability is observed for memo- rization, followed by semantic generalization, and the least improvement is seen in compositional generalization. Next, however, the gap between memorization and semantic generalization almost\n4We use OLMo for the experiments since the intermediate checkpoints, optimizer states, and batch sequence\ndata for pretraining the model are made publicly available.\n7B-Late(a) Pretraining stage\nFigure 3: Effectivity averaged across various probes and each time of injection, measured for different injection scenarios, and acquisition depths. Note that the effectivity does not improve as the model is trained with more tokens (Left), whereas there is a clear improvement as the model size scales (Right).'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='disappears in the paraphrase injection scenario. Third, when the model is updated with the duplica- tion injection scenario, the model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (t = 2000) compared to the paraphrase injection scenario.\nThese patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1). In- triguingly, the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6). The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably and that such a threshold may be higher for smaller models.\n4.2 Effects of model scale and pretraining stage on knowledge acquisition dynamics\nNext, we measure effectivity (Eq. 2) to quantify the improvement of the LLMs‚Äô log probability after being trained with the injected knowledge, averaged across all probes (q) and encounters (i). The results are demonstrated in Figure 3. The average effectivity is the largest in the Once injection scenario since the effectivity is higher when the model encounters the injected knowledge for the first time, which is further discussed in ¬ßH.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).5 On the other hand, surprisingly, the effectivity of fact acquisition does not improve with checkpoints trained with more tokens, as shown on the left side of Figure 3. This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11). Moreover, this tendency is not attributed to training the models with a decreased learning rate through learning rate decay, as demonstrated by an additional experiment of training three checkpoints using the same constant learning rate. The results with the constant learning rate show that effectivity does not significantly improve in the checkpoints of later stages of pretraining where more pretraining tokens are seen (¬ßF). Therefore, the observation implies that the effectivity of LLMs in acquiring factual knowledge does not significantly improve throughout the progress of pretraining.\nWhile our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model. We discuss this hypothesis further in ¬ß4.4.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Comparing the duplication and paraphrase injection scenarios, the duplication injection scenario naturally shows higher effectivity for memorization. However, the higher effectivity in the duplica- tion injection scenario for semantic generalization and compositional generalization appears to be\n5For a fair comparison of the effectivity of the 1B and 7B models, the OLMo-1B Mid checkpoint is trained using the same initial learning rate as the OLMo-7B Mid checkpoint (the specific value is provided in Appendix Table 5). The measured effectivity for all OLMo-1B checkpoints with the original learning rate is presented in Appendix Figure 11.\nFigure 4: Average retainability against training steps past the local acquisition maxima, measured with OLMo-7B mid checkpoint. The x-axes are in log scale. Left: duplication. Right: paraphrase.\nTable 2: Decay constant of average retainability (R(p, t)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.\nEarly (170B) Mid (500B) Late (1.5T)\nMemorization Semantic Composition\n0.26¬±0.0020 0.24¬±0.0018 0.18¬±0.0020\n0.25¬±0.0019 0.25¬±0.0022 0.20¬±0.0032\n0.20¬±0.0019 0.21¬±0.0021 0.16¬±0.0024\nMemorization Semantic Composition\n0.20¬±0.0019 0.20¬±0.0020 0.14¬±0.0025\n0.21¬±0.0023 0.23¬±0.0024 0.15¬±0.0022\n0.18¬±0.0022 0.21¬±0.0024 0.19¬±0.0030\ncounterintuitive, as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]. In the following sections, we will address this question by demonstrating that the models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts (¬ß4.3).\n4.3 Forgetting in factual knowledge acquisition'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]. Motivated by this, we investigate whether the exponential trend of forgetting persists in the context of factual knowledge acquisition in LLM pretraining. Figure 4 illustrates the trend of retainability against the training steps past the local acquisition maxima. We find that the trend of R(p, t) against log(t) fits a linear function very well (R2 > 0.80 for memorization and semantic generalization, and R2 > 0.65 for compositional generalization). This trend is persistent across all acquisition depths, and all training conditions (¬ßE.4 and ¬ßE.5). Guided by empirical observations, we model the trend of forgetting using a power-law model in further investigations.\nHow quickly is the acquired factual knowledge lost? The absolute value of the slope of the fitted lines in Figure 4 can be interpreted as the decay constant (a) of retainability, formally,\nfor 0 < t1 < t2 < œÑ, where R(p, œÑ ) = 0 and a > 0.\nThus, the measured decay constant represents how fast (in terms of fraction) the model loses the improvement of log probability. Table 2 shows the decay constants of retainability measured for three OLMo-7B intermediate checkpoints, for duplication and paraphrase injection scenarios.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='There are several observations in Table 2. First, the forgetting in compositional generalization is slower (the decay constant a is smaller) than in memorization and semantic generalization. Combined with the observations in previous sections, the acquisition of compositional generalization accumulates most slowly but is more robust to forgetting. Second, the forgetting tends to be slower in the paraphrase injection scenario compared to the duplication injection scenario. This finding will be further discussed in ¬ß4.4, regarding the importance of deduplicating training data. Finally, the decay constants are similar for the two earlier checkpoints but smaller for the late checkpoint in the duplication injection scenario. We demonstrate that this is due to the reduced learning rate from\nFigure 5: Comparison of the forgetting dynamics of pretraining (Left) and training with reduced batch size (Right), measured with OLMo-7B mid checkpoint. Note that the x-axis represents the number of training tokens instead of training steps, which has a shifting effect on the data plotted in Figure 4.\nlearning rate scheduling (Appendix Table 5), as the decay constants show no decrease for the later checkpoint when each checkpoint is trained with the same constant learning rate (Appendix Table 9).\nPretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49]. However, the effects of increasing training batch size in terms of the LLMs‚Äô acquisition of factual knowledge remain underexplored. In this section, we examine whether pretraining LLMs with a larger batch size is advantageous regarding factual knowledge acquisition. Specifically, we continue training LLMs with a batch size reduced by a factor of 16 compared to the original pretraining batch size, i.e., from 2048 to 128.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size. The results have several implications for the advantage of pretraining LLMs with a larger batch size. First, comparing Figure 3 and Appendix Figure 21, LLMs trained with the smaller batch size show higher effectivity. However, the decay constant tends to be higher, comparing the numbers in Table 2 and Appendix Table 10. Furthermore, the anticipated x-intercept is significantly decreased by dozens of times, comparing Appendix Table 6 and 11. This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold, which we discuss in detail in the following section (¬ß4.4). In other words, when an LLM is trained with a smaller batch size, factual knowledge should be presented more often to the model so as not to be forgotten and the set of learnable knowledge is reduced. Second, accelerated forgetting with a smaller batch size is more pronounced for compositional generalization compared to memorization and semantic generalization. In brief, the results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge.\nImplications for LLM pretraining\nWhy is popularity important for factual knowledge acquisition? The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.6 Hence, if a given factual knowledge in the pretraining dataset is in the long-tail and the knowledge is presented to the model with an interval longer than a certain threshold, such knowledge will be impossible to be decoded as the top-k generation of'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='6The exact values of the estimated x-intercepts can be found in Appendix Table 6.\nthe model, or learned, regardless of the duration of the pretraining.7 This implies that there is a learnability threshold, a threshold of the interval where the model fails to acquire knowledge of which its encounter interval is longer than the threshold. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold. In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]. Moreover, LLMs will accumulate the log probability faster for more popular knowledge, and thus the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].\nIn summary, we hypothesize that the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‚Äòrevealed‚Äô in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in ¬ß4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge (more knowledge will be presented with an interval shorter than the learnability threshold) since the skewness of the distribution of factual knowledge popularity is likely to be mitigated as the data becomes larger and more diverse.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Why does deduplication enhance model performance? Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]. Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in ¬ß4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge. This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization (Figure 2 and 3), presenting the model with duplicated texts with a short interval will result in the widening of the gap between memorization and generalization, which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].\n5 Discussion and Conclusions\nIn this work, we study how LLMs acquire factual knowledge during pretraining. Our findings and contributions can be summarized as follows:\nWe propose methods, datasets, and metrics for performing a fine-grained analysis of factual knowledge acquisition dynamics during LLM pretraining.\nWe demonstrate that factual knowledge acquisition in LLM pretraining is achieved through accumulating micro-acquisitions, each of which occurs whenever the model is updated after seeing the factual knowledge. When the model is not presented with factual knowledge, forgetting occurs and the acquisition of the knowledge is gradually diluted.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different.\nThere is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\n7This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge. In addition, the actual learnability threshold is likely to vary for different types of factual knowledge due to several factors, such as the number of similar/related facts or temporal conflicts in the pretraining data.\nWe provide potential explanations for recently observed, yet underexplored behaviors of LLMs. First, we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining. Second, we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.\nWe would like to thank Seongyun Lee, Suehyun Park, Hyeonbin Hwang, Geewook Kim, Juyoung Suk, Aengus Lynch, and Katja Filippova for their valuable feedback on our work.\nThis work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program(KAIST).')], 'References': {'1': {'Title': 'Semdedup: Data-efficient learning at web-scale through semantic deduplication', 'Authors': 'Amro Abbas, Kushal Tirumala, D√°niel Simig, Surya Ganguli, Ari S. Morcos', 'Counter': 1, 'Context': ['This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement.']}, '2': {'Title': 'Gpt-4 technical report', 'Authors': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.', 'Counter': 3, 'Context': ['An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.']}, '3': {'Title': 'Tracing knowledge in language models back to the training data', 'Authors': 'Ekin Aky√ºrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '4': {'Title': 'Physics of language models: Part 3.1, knowledge storage and extraction', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 4, 'Context': ['...but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization...', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', '...which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.']}, '5': {'Title': 'Physics of language models: Part 3.2, knowledge manipulation', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 4, 'Context': ['In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].']}, '6': {'Title': 'A closer look at memorization in deep networks', 'Authors': 'Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, Simon Lacoste-Julien', 'Counter': 3, 'Context': ['Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].', 'Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].', 'Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].']}, '7': {'Title': 'Emergent and predictable memorization in large language models', 'Authors': 'Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, Edward Raf', 'Counter': 4, 'Context': ['Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.', 'In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance.', 'Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.', 'This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge.']}, '8': {'Title': 'Pythia: A suite for analyzing large language models across training and scaling', 'Authors': 'Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal', 'Counter': 3, 'Context': ['...the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‚Äòrevealed‚Äô in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in ¬ß4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge...', '...as demonstrated in [8].', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].']}, '9': {'Title': 'Language models are few-shot learners', 'Authors': 'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.', 'Counter': 5, 'Context': ['Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in ¬ß4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '10': {'Title': 'Extracting training data from large language models', 'Authors': 'Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, √ölfar Erlingsson, Alina Oprea, Colin Raffel', 'Counter': 3, 'Context': ['LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].']}, '11': {'Title': 'Quantifying memorization across neural language models', 'Authors': 'Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram√®r, Chiyuan Zhang', 'Counter': 0, 'Context': []}, '12': {'Title': 'Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs', 'Authors': 'Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, Naomi Saphra', 'Counter': 3, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '13': {'Title': 'Palm: Scaling language modeling with pathways', 'Authors': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc√≠a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D√≠az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '14': {'Title': 'Analyzing commonsense emergence in few-shot knowledge models', 'Authors': 'Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut', 'Counter': 3, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].']}, '15': {'Title': 'Knowledge neurons in pretrained transformers', 'Authors': 'Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '16': {'Title': 'Measuring causal effects of data statistics on language model‚Äôs ‚Äôfactual‚Äô predictions', 'Authors': 'Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, Yoav Goldberg', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '17': {'Title': 'Does learning require memorization? a short tale about a long tail', 'Authors': 'Vitaly Feldman', 'Counter': 1, 'Context': ['In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance.']}, '18': {'Title': 'Does fine-tuning llms on new knowledge encourage hallucinations?', 'Authors': 'Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig', 'Counter': 2, 'Context': ['[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '19': {'Title': 'Transformer feed-forward layers are key-value memories', 'Authors': 'Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy', 'Counter': 2, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.', 'This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in ¬ß 4.4.']}, '20': {'Title': 'Dissecting recall of factual associations in auto-regressive language models', 'Authors': 'Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '21': {'Title': 'Olmo: Accelerating the science of language models', 'Authors': 'Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi', 'Counter': 11, 'Context': ['To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [21].', 'To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained...', 'the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6).', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [21].', 'To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably.', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].', 'Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size.', 'This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold.']}, '22': {'Title': 'Investigating learning dynamics of bert fine-tuning', 'Authors': 'Yaru Hao, Li Dong, Furu Wei, Ke Xu', 'Counter': 1, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 9, 'Context': ['Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.']}, '24': {'Title': 'The surprising simplicity of the early-time learning dynamics of neural networks', 'Authors': 'Wei Hu, Lechao Xiao, Ben Adlam, Jeffrey Pennington', 'Counter': 0, 'Context': []}, '25': {'Title': 'Mistral 7b', 'Authors': 'Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L‚Äôelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [25].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [25].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '26': {'Title': 'Large language models struggle to learn long-tail knowledge', 'Authors': 'Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, Colin Raffel', 'Counter': 4, 'Context': ['...the failure to acquire long-tail knowledge...', 'the failure to acquire long-tail knowledge [26, 34]', '...the failure to acquire long-tail knowledge [26, 34]...', 'However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].']}, '27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 14, 'Context': ['[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.', 'Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.', 'Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', '...the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.']}, '28': {'Title': 'The bigscience roots corpus: A 1.6 tb composite multilingual dataset', 'Authors': 'Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al.', 'Counter': 3, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 10, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.', 'the importance of dataset deduplication [29, 52]', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.', '...the importance of dataset deduplication [29, 52] can be explained.', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].']}, '30': {'Title': 'Starcoder: may the source be with you!', 'Authors': 'Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm de Vries', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [30].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [30].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '31': {'Title': 'How pre-trained language models capture factual knowledge? a causal-inspired analysis', 'Authors': 'Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '32': {'Title': 'Probing across time: What does RoBERTa know and when?', 'Authors': 'Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith', 'Counter': 1, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '33': {'Title': 'An empirical study of catastrophic forgetting in large language models during continual fine-tuning', 'Authors': 'Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang', 'Counter': 4, 'Context': ['The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].']}, '34': {'Title': 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories', 'Authors': 'Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi', 'Counter': 3, 'Context': ['the failure to acquire long-tail knowledge [26, 34]', '...the failure to acquire long-tail knowledge [26, 34]...', 'However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].']}, '35': {'Title': 'Entity cloze by date: What lms know about unseen entities', 'Authors': 'Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, Greg Durrett', 'Counter': 3, 'Context': ['An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.']}, '36': {'Title': 'Language models as knowledge bases?', 'Authors': 'Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller', 'Counter': 5, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].']}, '37': {'Title': 'Grokking: Generalization beyond overfitting on small algorithmic datasets', 'Authors': 'Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra', 'Counter': 0, 'Context': []}, '38': {'Title': 'Exploring the limits of transfer learning with a unified text-to-text transformer', 'Authors': 'Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu', 'Counter': 1, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '39': {'Title': 'Anatomy of catastrophic forgetting: Hidden representations and task semantics', 'Authors': 'Vinay Venkatesh Ramasesh, Ethan Dyer, Maithra Raghu', 'Counter': 2, 'Context': ['The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].']}, '40': {'Title': 'How much knowledge can you pack into the parameters of a language model?', 'Authors': 'Adam Roberts, Colin Raffel, Noam M. Shazeer', 'Counter': 5, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].']}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 6, 'Context': ['To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].']}, '42': {'Title': 'Noise-robust de- duplication at scale', 'Authors': 'Emily Silcock, Luca D‚ÄôAmico-Wong, Jinglin Yang, Melissa Dell', 'Counter': 0, 'Context': []}, '43': {'Title': 'Dolma: An open corpus of three trillion tokens for language model pretraining research', 'Authors': 'Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al.', 'Counter': 5, 'Context': ['...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', '...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', '...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '44': {'Title': 'Memorisation versus generalisation in pre-trained language models', 'Authors': 'Michael T√§nzer, Sebastian Ruder, Marek Rei', 'Counter': 2, 'Context': ['[44] and [46] focused on the dynamics of memorization in language model pretraining.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.']}, '45': {'Title': 'Emergent structures and training dynamics in large language models', 'Authors': 'Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan', 'Counter': 0, 'Context': []}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 7, 'Context': ['Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].']}, '47': {'Title': 'D4: Improving llm pretraining via document de-duplication and diversification', 'Authors': 'Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos', 'Counter': 2, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '48': {'Title': 'Llama: Open and efficient foundation language models', 'Authors': 'Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.', 'Counter': 2, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '49': {'Title': 'Llama 2: Open foundation and fine-tuned chat models', 'Authors': 'Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant√≥n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom', 'Counter': 4, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [49].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [49].', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '50': {'Title': 'Emergent abilities of large language models', 'Authors': 'Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.', 'Counter': 4, 'Context': ['We suggest a plausible hypothesis based on further observations in ¬ß4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training.', '...we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times...', '...we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.', 'Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model.']}, '51': {'Title': 'Training trajectories of language models across scales', 'Authors': 'Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov', 'Counter': 0, 'Context': []}, '52': {'Title': 'To repeat or not to repeat: Insights from scaling llm under token-crisis', 'Authors': 'Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You', 'Counter': 6, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'the importance of dataset deduplication [29, 52]', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...the importance of dataset deduplication [29, 52] can be explained.', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].']}, '53': {'Title': 'Critical data size of language models from a grokking perspective', 'Authors': 'Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin', 'Counter': 3, 'Context': ['Recently, [53] explored the relationship between the data size and grokking [37].', 'Recently, [53] explored the relationship between the data size and grokking [37].', 'Recently, [53] explored the relationship between the data size and grokking [37].']}}}/home/paradeigma/workspace/python/streamlit_use/citationlinker.py:93: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead
  for result in search.results():
 

ÎÖºÎ¨∏ Ï†ÄÏû• ÏôÑÎ£å!
27 Î≤àÏß∏ ÎÖºÎ¨∏ Îã§Ïö¥Î°úÎìú ÏôÑÎ£å
21 Î≤àÏß∏ ÎÖºÎ¨∏ ÏòàÏô∏ Î∞úÏÉù:  Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=Olmo%3A+Accelerating+the+science+of+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)
21 Î≤àÏß∏ ÎÖºÎ¨∏ Îã§Ïö¥Î°úÎìú Ïã§Ìå®
    Olmo: Accelerating the science of language models
ÎÖºÎ¨∏ Ï†ÄÏû• ÏôÑÎ£å!
29 Î≤àÏß∏ ÎÖºÎ¨∏ Îã§Ïö¥Î°úÎìú ÏôÑÎ£å
23 Î≤àÏß∏ ÎÖºÎ¨∏ ÏòàÏô∏ Î∞úÏÉù:  Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=Training+compute-optimal+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)
ÎÖºÎ¨∏ Ï†ÄÏû• ÏôÑÎ£å!
23 Î≤àÏß∏ ÎÖºÎ¨∏ Îã§Ïö¥Î°úÎìú ÏôÑÎ£å
ÎÖºÎ¨∏ Ï†ÄÏû• ÏôÑÎ£å!
46 Î≤àÏß∏ ÎÖºÎ¨∏ Îã§Ïö¥Î°úÎìú ÏôÑÎ£å
ÎÖºÎ¨∏ Ï†ÄÏû• ÏôÑÎ£å!
41 Î≤àÏß∏ ÎÖºÎ¨∏ Îã§Ïö¥Î°úÎìú ÏôÑÎ£å
step 5:  
 {'27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 14, 'Context': ['[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.', 'Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.', 'Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', '...the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': 'We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.', 'pdf_url': 'http://arxiv.org/pdf/2001.08361v1'}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 10, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.', 'the importance of dataset deduplication [29, 52]', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.', '...the importance of dataset deduplication [29, 52] can be explained.', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].'], 'abstract': 'We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.', 'pdf_url': 'http://arxiv.org/pdf/2107.06499v2'}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 9, 'Context': ['Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.", 'pdf_url': 'http://arxiv.org/pdf/2312.12391v2'}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 7, 'Context': ['Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].'], 'abstract': 'Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.', 'pdf_url': 'http://arxiv.org/pdf/2205.10770v2'}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 6, 'Context': ['To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].'], 'abstract': "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.", 'pdf_url': 'http://arxiv.org/pdf/2304.15004v2'}} 
 {'27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 14, 'Context': ['[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.', 'Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.', 'Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', '...the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': 'We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.', 'pdf_url': 'http://arxiv.org/pdf/2001.08361v1'}, '21': {'Title': 'Olmo: Accelerating the science of language models', 'Authors': 'Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi', 'Counter': 11, 'Context': ['To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [21].', 'To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained...', 'the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6).', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [21].', 'To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably.', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].', 'Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size.', 'This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold.'], 'abstract': None, 'pdf_url': None}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 10, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.', 'the importance of dataset deduplication [29, 52]', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.', '...the importance of dataset deduplication [29, 52] can be explained.', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].'], 'abstract': 'We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.', 'pdf_url': 'http://arxiv.org/pdf/2107.06499v2'}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 9, 'Context': ['Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.", 'pdf_url': 'http://arxiv.org/pdf/2312.12391v2'}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 7, 'Context': ['Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].'], 'abstract': 'Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.', 'pdf_url': 'http://arxiv.org/pdf/2205.10770v2'}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 6, 'Context': ['To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].'], 'abstract': "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.", 'pdf_url': 'http://arxiv.org/pdf/2304.15004v2'}, '52': {'Title': 'To repeat or not to repeat: Insights from scaling llm under token-crisis', 'Authors': 'Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You', 'Counter': 6, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'the importance of dataset deduplication [29, 52]', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...the importance of dataset deduplication [29, 52] can be explained.', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].']}, '9': {'Title': 'Language models are few-shot learners', 'Authors': 'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.', 'Counter': 5, 'Context': ['Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in ¬ß4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '36': {'Title': 'Language models as knowledge bases?', 'Authors': 'Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller', 'Counter': 5, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].']}, '40': {'Title': 'How much knowledge can you pack into the parameters of a language model?', 'Authors': 'Adam Roberts, Colin Raffel, Noam M. Shazeer', 'Counter': 5, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].']}, '43': {'Title': 'Dolma: An open corpus of three trillion tokens for language model pretraining research', 'Authors': 'Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al.', 'Counter': 5, 'Context': ['...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', '...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', '...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '4': {'Title': 'Physics of language models: Part 3.1, knowledge storage and extraction', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 4, 'Context': ['...but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization...', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', '...which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.']}, '5': {'Title': 'Physics of language models: Part 3.2, knowledge manipulation', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 4, 'Context': ['In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].']}, '7': {'Title': 'Emergent and predictable memorization in large language models', 'Authors': 'Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, Edward Raf', 'Counter': 4, 'Context': ['Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.', 'In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance.', 'Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.', 'This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge.']}, '26': {'Title': 'Large language models struggle to learn long-tail knowledge', 'Authors': 'Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, Colin Raffel', 'Counter': 4, 'Context': ['...the failure to acquire long-tail knowledge...', 'the failure to acquire long-tail knowledge [26, 34]', '...the failure to acquire long-tail knowledge [26, 34]...', 'However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].']}, '33': {'Title': 'An empirical study of catastrophic forgetting in large language models during continual fine-tuning', 'Authors': 'Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang', 'Counter': 4, 'Context': ['The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].']}, '49': {'Title': 'Llama 2: Open foundation and fine-tuned chat models', 'Authors': 'Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant√≥n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom', 'Counter': 4, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [49].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [49].', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '50': {'Title': 'Emergent abilities of large language models', 'Authors': 'Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.', 'Counter': 4, 'Context': ['We suggest a plausible hypothesis based on further observations in ¬ß4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training.', '...we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times...', '...we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.', 'Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model.']}, '2': {'Title': 'Gpt-4 technical report', 'Authors': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.', 'Counter': 3, 'Context': ['An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.']}, '6': {'Title': 'A closer look at memorization in deep networks', 'Authors': 'Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, Simon Lacoste-Julien', 'Counter': 3, 'Context': ['Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].', 'Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].', 'Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].']}, '8': {'Title': 'Pythia: A suite for analyzing large language models across training and scaling', 'Authors': 'Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal', 'Counter': 3, 'Context': ['...the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‚Äòrevealed‚Äô in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in ¬ß4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge...', '...as demonstrated in [8].', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].']}, '10': {'Title': 'Extracting training data from large language models', 'Authors': 'Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, √ölfar Erlingsson, Alina Oprea, Colin Raffel', 'Counter': 3, 'Context': ['LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].']}, '12': {'Title': 'Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs', 'Authors': 'Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, Naomi Saphra', 'Counter': 3, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '13': {'Title': 'Palm: Scaling language modeling with pathways', 'Authors': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc√≠a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D√≠az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '14': {'Title': 'Analyzing commonsense emergence in few-shot knowledge models', 'Authors': 'Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut', 'Counter': 3, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].']}, '25': {'Title': 'Mistral 7b', 'Authors': 'Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L‚Äôelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [25].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [25].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '28': {'Title': 'The bigscience roots corpus: A 1.6 tb composite multilingual dataset', 'Authors': 'Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al.', 'Counter': 3, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '30': {'Title': 'Starcoder: may the source be with you!', 'Authors': 'Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm de Vries', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [30].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [30].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '34': {'Title': 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories', 'Authors': 'Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi', 'Counter': 3, 'Context': ['the failure to acquire long-tail knowledge [26, 34]', '...the failure to acquire long-tail knowledge [26, 34]...', 'However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].']}, '35': {'Title': 'Entity cloze by date: What lms know about unseen entities', 'Authors': 'Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, Greg Durrett', 'Counter': 3, 'Context': ['An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.']}, '53': {'Title': 'Critical data size of language models from a grokking perspective', 'Authors': 'Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin', 'Counter': 3, 'Context': ['Recently, [53] explored the relationship between the data size and grokking [37].', 'Recently, [53] explored the relationship between the data size and grokking [37].', 'Recently, [53] explored the relationship between the data size and grokking [37].']}, '18': {'Title': 'Does fine-tuning llms on new knowledge encourage hallucinations?', 'Authors': 'Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig', 'Counter': 2, 'Context': ['[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '19': {'Title': 'Transformer feed-forward layers are key-value memories', 'Authors': 'Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy', 'Counter': 2, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.', 'This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in ¬ß 4.4.']}, '39': {'Title': 'Anatomy of catastrophic forgetting: Hidden representations and task semantics', 'Authors': 'Vinay Venkatesh Ramasesh, Ethan Dyer, Maithra Raghu', 'Counter': 2, 'Context': ['The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].']}, '44': {'Title': 'Memorisation versus generalisation in pre-trained language models', 'Authors': 'Michael T√§nzer, Sebastian Ruder, Marek Rei', 'Counter': 2, 'Context': ['[44] and [46] focused on the dynamics of memorization in language model pretraining.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.']}, '47': {'Title': 'D4: Improving llm pretraining via document de-duplication and diversification', 'Authors': 'Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos', 'Counter': 2, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '48': {'Title': 'Llama: Open and efficient foundation language models', 'Authors': 'Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.', 'Counter': 2, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '1': {'Title': 'Semdedup: Data-efficient learning at web-scale through semantic deduplication', 'Authors': 'Amro Abbas, Kushal Tirumala, D√°niel Simig, Surya Ganguli, Ari S. Morcos', 'Counter': 1, 'Context': ['This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement.']}, '3': {'Title': 'Tracing knowledge in language models back to the training data', 'Authors': 'Ekin Aky√ºrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '15': {'Title': 'Knowledge neurons in pretrained transformers', 'Authors': 'Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '16': {'Title': 'Measuring causal effects of data statistics on language model‚Äôs ‚Äôfactual‚Äô predictions', 'Authors': 'Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, Yoav Goldberg', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '17': {'Title': 'Does learning require memorization? a short tale about a long tail', 'Authors': 'Vitaly Feldman', 'Counter': 1, 'Context': ['In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance.']}, '20': {'Title': 'Dissecting recall of factual associations in auto-regressive language models', 'Authors': 'Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '22': {'Title': 'Investigating learning dynamics of bert fine-tuning', 'Authors': 'Yaru Hao, Li Dong, Furu Wei, Ke Xu', 'Counter': 1, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '31': {'Title': 'How pre-trained language models capture factual knowledge? a causal-inspired analysis', 'Authors': 'Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '32': {'Title': 'Probing across time: What does RoBERTa know and when?', 'Authors': 'Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith', 'Counter': 1, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '38': {'Title': 'Exploring the limits of transfer learning with a unified text-to-text transformer', 'Authors': 'Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu', 'Counter': 1, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '11': {'Title': 'Quantifying memorization across neural language models', 'Authors': 'Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram√®r, Chiyuan Zhang', 'Counter': 0, 'Context': []}, '24': {'Title': 'The surprising simplicity of the early-time learning dynamics of neural networks', 'Authors': 'Wei Hu, Lechao Xiao, Ben Adlam, Jeffrey Pennington', 'Counter': 0, 'Context': []}, '37': {'Title': 'Grokking: Generalization beyond overfitting on small algorithmic datasets', 'Authors': 'Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra', 'Counter': 0, 'Context': []}, '42': {'Title': 'Noise-robust de- duplication at scale', 'Authors': 'Emily Silcock, Luca D‚ÄôAmico-Wong, Jinglin Yang, Melissa Dell', 'Counter': 0, 'Context': []}, '45': {'Title': 'Emergent structures and training dynamics in large language models', 'Authors': 'Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan', 'Counter': 0, 'Context': []}, '51': {'Title': 'Training trajectories of language models across scales', 'Authors': 'Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov', 'Counter': 0, 'Context': []}} 
 {'Title': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Title'}, page_content='How Do Large Language Models Acquire Factual Knowledge During Pretraining?')], 'Authors': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Authors'}, page_content='Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo')], 'Submitted': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Submitted'}, page_content='2024-06-17 17:54:40+00:00')], 'Abstract': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Abstract'}, page_content="Despite the recent observation that large language models (LLMs) can store\nsubstantial factual knowledge, there is a limited understanding of the\nmechanisms of how they acquire factual knowledge through pretraining. This work\naddresses this gap by studying how LLMs acquire factual knowledge during\npretraining. The findings reveal several important insights into the dynamics\nof factual knowledge acquisition during pretraining. First, counterintuitively,\nwe observe that pretraining on more data shows no significant improvement in\nthe model's capability to acquire and maintain factual knowledge. Next, there\nis a power-law relationship between training steps and forgetting of\nmemorization and generalization of factual knowledge, and LLMs trained with\nduplicated training data exhibit faster forgetting. Third, training LLMs with\nlarger batch sizes can enhance the models' robustness to forgetting. Overall,\nour observations suggest that factual knowledge acquisition in LLM pretraining\noccurs by progressively increasing the probability of factual knowledge\npresented in the pretraining data at each step. However, this increase is\ndiluted by subsequent forgetting. Based on this interpretation, we demonstrate\nthat we can provide plausible explanations for recently observed behaviors of\nLLMs, such as the poor performance of LLMs on long-tail knowledge and the\nbenefits of deduplicating the pretraining corpus.")], 'Introduction': [Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40]. Unfortunately, little is understood about the mechanisms of how LLMs acquire factual knowledge during pretraining. In this work, we make an initial attempt to understand the dynamics of factual knowledge acquisition in LLM pretraining. We study three important yet unanswered research questions:\nRQ1. How is factual knowledge acquired during LLM pretraining and how are LLMs affected by\nthe training data at each training step?\nRQ2. How is the effectivity of factual knowledge acquisition affected by training conditions?\n1Code and data are available at: https://github.com/kaistAI/factual-knowledge-acquisition/\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nRQ3. How is the acquired factual knowledge forgotten, and how is the trend affected by training\nTo answer the research questions, we analyze how LLMs acquire and retain factual knowledge in terms of memorization and generalization by varying the following training conditions: knowledge injection scenarios, pretraining stages, model sizes, and training batch sizes. Specifically, we take the intermediate pretraining checkpoints of different sizes of an LLM at different pretraining stages, inject the target knowledge that the models have not previously encountered, and monitor their step-wise progress of acquiring factual knowledge under various conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Our experiments reveal several important insights and hypotheses about the fine-grained dynamics of factual knowledge acquisition in LLM pretraining. First, we show that factual knowledge acquisition occurs by accumulating the small increase of probability induced by updating the model with a minibatch containing the factual knowledge. Second, compared to the checkpoints at earlier stages, the checkpoint at the later stage shows no significant difference in effectivity, i.e., no significant improvement in the ability to acquire memorization and generalization immediately. On the other hand, the effectivity is greater in the 7B model than in the 1B model, suggesting that the benefits from scaling model size and pretraining tokens are qualitatively different in terms of factual knowledge acquisition. Third, we find a power-law relationship between training steps (or tokens) and forgetting of acquired factual knowledge in both memorization and generalization. Further examination of the rate of forgetting factual knowledge in LLM pretraining reveals that deduplicating the training data and training the models with a greater batch size enhances the acquisition of factual knowledge, by making them more robust against forgetting. Based on our understanding of the dynamics of factual knowledge acquisition, we demonstrate that the recently observed behaviors, including the improvement of LLMs‚Äô performance with more training data, the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.\nOverall, to the best of our knowledge, this work is one of the initial attempts to examine the training dynamics involved in acquiring factual knowledge during the pretraining of LLMs. By enhancing our understanding of the factual knowledge acquisition dynamics, we expect that academia can gain a holistic understanding and make better use of LLMs.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]. [3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data. [4] demonstrated that knowledge should be presented in a diverse format during pretraining to be reliably extracted. However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5]. These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete. To examine the detailed training dynamics of knowledge acquisition during pretraining, we conduct a fine-grained analysis of factual knowledge acquisition on each piece of factual knowledge.\nMemorization and forgetting are closely related to knowledge acquisition in neural networks [6]. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11]. In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]. [44] and [46] focused on the dynamics of memorization in language model pretraining. Recently, [53] explored the relationship between the data size and grokking [37]. Compared to these, we perform a more detailed analysis of the dynamics of factual knowledge acquisition during LLM pretraining, by evaluating the log probability of individual pieces of factual knowledge at each training step.\nTable 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded.\nThe fortieth government of Mars, or the Zorgon-Calidus government, (...) Mars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership. (...)\nMars, historically known for its centralized sub-planet distribution, underwent significant political reform under Zorgon‚Äôs leadership.\nMars, previously recognized for its focused distribution of sub-planets, experienced substantial political transformation during Zorgon‚Äôs leadership.\nThe Zorgon-Calidus government rapidly expedited the transitory phase of the Martian democratic system.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='FICTIONAL KNOWLEDGE dataset Our goal is to analyze the LLMs‚Äô behavior when acquiring factual knowledge during pretraining. Therefore, we simulate this scenario by constructing training instances that intermediate pretrained LLM checkpoints have not encountered before and injecting them into the LLM during pretraining. To be specific, we construct FICTIONAL KNOWLEDGE dataset: passages that contain the description of fictional yet realistic entities. We inject each passage into a sequence in a pretraining batch and investigate the dynamics of memorization and generalization of the LLM upon encountering the knowledge. We call these passages injected knowledge.\nNext, to investigate the LLMs‚Äô ability to generalize acquired factual knowledge in different depths, we split the concept of acquisition into three depths: (1) memorization: memorizing the exact sequence used for training (2) semantic generalization: generalizing the factual knowledge to a paraphrased format in a single-sentence level (3) compositional generalization: composing the factual knowledge presented in multiple sentences in the injected knowledge.\nFollowing this intuition, we carefully design five probes for each of the three different acquisition depths for each injected knowledge, resulting in 1,800 probes in total. Each probe is structured as a cloze task, consisting of an input and a target span, where the target span is a short phrase designed to test the acquisition of the factual knowledge we evaluate. An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases. The details for the data construction and more examples of the FICTIONAL KNOWLEDGE dataset can be found in ¬ßB.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Evaluation metrics To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41]. To quantitatively measure the trend of factual knowledge acquisition, we should first define the timestep where the local effect of updating the model using the injected knowledge completely pays off. A step-wise evaluation of the change in a model‚Äôs log probability on factual knowledge during pretraining reveals that this improvement occurs through several steps (Figure 1), since LLMs deploy optimizers with momentum. Hence, we define the timestep where the log probability reaches a maximum value in a short interval after the model is trained on the injected knowledge, which we refer to as the local acquisition maxima.\nDefinition 1 Given a language model, let Œ∏t represent the model‚Äôs parameters before the t-th update. Given injected knowledge k (used as a training instance) and the corresponding probe q (used as an evaluation instance), let ‚Ñì(q; Œ∏) denote the log probability of the target span of q, provided by the model. Let a nonempty set Tk = {t1, t2, . . . , tn} denote the steps where the model is updated with the minibatch containing the injected knowledge k, where 0 ‚â§ t1 < t2 < . . . < tn. Finally, let tw denote the window size. Then, the local acquisition maxima (tLAM(q, i)) is defined as:\ntLAM(q, i) = argmax ti<t‚â§ti+tw'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 1: An illustration of the change of log probability of the target span of a probe (‚àÜ‚Ñì(q)) measuring the memorization of factual knowledge on a short-term scale. At step 0 (marked as a dotted line), the model is trained with the injected knowledge which contains the factual knowledge evaluated by the probe q. The local acquisition maxima (marked as a red line) is the timestep where the log probability reaches its maximum within the window (shaded area), defined by tw. The measurement of effectivity and retainability at t = 30 is visualized, where retainability is obtained by measuring the fraction of the purple line compared to the gray line.\nIn Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time. This improvement is measured by the model‚Äôs log probability on the target spans of the corresponding probes. This metric, effectivity, will be used to answer the second research question.\nDefinition 2 Given a language model parameterized by Œ∏ trained with an injected knowledge k at t = ti where ti ‚àà Tk, and a corresponding probe q, the effectivity (E(q, i)) is defined as the absolute increase of the model‚Äôs log probability on the target span of q between t = ti and t = tLAM(q, i), i.e.,\nE(q, i) = ‚Ñì(q; Œ∏tLAM(q,i)) ‚àí ‚Ñì(q; Œ∏ti ).\nFinally, to investigate the forgetting phenomenon of acquired factual knowledge (RQ3), we define a metric that quantifies the fraction of improvement in log probability retained by the model after t steps, relative to the local acquisition maxima of the last knowledge update.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Definition 3 Consider a language model parameterized by Œ∏ and trained with injected knowledge k for N iterations, occuring at timesteps ti ‚àà Tk where |Tk| = N . Let tpre denote the last timestep before the model is first trained with k, i.e., tpre = min(Tk). Given a corresponding probe q, retainability (R(q, t)) is defined for t ‚â• 0 as follows:\n‚Ñì(q; Œ∏tLAM(q,N )+t) ‚àí ‚Ñì(q; Œ∏tpre) ‚Ñì(q; Œ∏tLAM(q,N )) ‚àí ‚Ñì(q; Œ∏tpre)\nNote that R(p, 0) = 1 which represents that the factual knowledge is 100% retained at the local acquisition maxima of the last knowledge update. Additionally, R(p, t) = 0 occurs when the log probability of the probe p at tSP(p) + t equals that at tpre. Thus, R(p, t) = 0 indicates that the improvement in the log probability of factual knowledge, induced by updating the model with minibatches containing the injected knowledge at tpre, is completely lost. This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in ¬ß 4.4. The measurement of the defined metrics are illustrated in Figure 1.\nFor the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5. This is particularly important for the measurement of retainability, as the small number of cases which showed no acquisition through training can give a very large value due to the very small denominator in Eq. 3.\nKnowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)\n2The Œ≤1 of AdamW optimizer is configured to 0.9 in our experiments, implying that the contribution of the gradient of a given sequence to the momentum will be reduced to approximately 0.950 ‚âà 0.0052 after 50 steps. Therefore, tw = 50 is a reasonable choice for the window size.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='3If optimizers without momentum (e.g., RMSProp) are used, the local effect of training the model at timestep t will be fully reflected immediately after that step. In such cases, tw should be 1 and tLAM will reduce to t + 1.\nFigure 2: Change in the average log probability of target spans of the probes plotted against training steps during the continuation of pretraining OLMo-7B mid checkpoint (trained on 500B tokens) with injecting the knowledge in the FICTIONAL KNOWLEDGE dataset. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios. Note the immediate and distinctive increase of log probability after the model is updated with the injected knowledge, marked by dotted vertical lines.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='varying knowledge injection scenarios (duplication, paraphrase, once), (ii) varying pretraining stages (early, mid, and late, pretrained with approximately 170B, 500B, and 1.5T tokens, respectively), (iii) varying model sizes (1B and 7B), and (iv) varying training batch sizes (2048 and 128). To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.4 Each injected knowledge is short enough to fit into one pretraining sequence in the batch, and we fill the rest of the sequence with the original sequence in the batch. To investigate the difference in the factual knowledge acquisition dynamics when the models are presented with the knowledge, we inject factual knowledge with three different injection scenarios: duplication, paraphrase, and once. For the duplication injection scenario, we inject the same knowledge 10 times with an interval of 100 training steps. In the paraphrase injection scenario, we inject paraphrased knowledge instead of showing identical sequences, every time it is presented to the model. Lastly, in the once injection scenario, we inject the knowledge only once at the start of the training. After the injection is complete, we continue pretraining as normal. The details for the training setup can be found in ¬ßD.\n4.1 Factual knowledge acquisition occurs by accumulating the observations of the fact'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 2 shows the progress of factual knowledge acquisition of OLMo-7B, by averaging the model‚Äôs log probability across the target spans of the probes for each injection scenario, evaluated at each training step. Regardless of the acquisition depths (memorization, semantic generalization, and com- positional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge. However, the log probability decreases again, as the knowledge is not presented to the model after- ward. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\nSeveral findings can be further obtained from Figure 2. First, when the model is updated after seeing the factual knowledge, the most significant improvement in log probability is observed for memo- rization, followed by semantic generalization, and the least improvement is seen in compositional generalization. Next, however, the gap between memorization and semantic generalization almost\n4We use OLMo for the experiments since the intermediate checkpoints, optimizer states, and batch sequence\ndata for pretraining the model are made publicly available.\n7B-Late(a) Pretraining stage\nFigure 3: Effectivity averaged across various probes and each time of injection, measured for different injection scenarios, and acquisition depths. Note that the effectivity does not improve as the model is trained with more tokens (Left), whereas there is a clear improvement as the model size scales (Right).'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='disappears in the paraphrase injection scenario. Third, when the model is updated with the duplica- tion injection scenario, the model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (t = 2000) compared to the paraphrase injection scenario.\nThese patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1). In- triguingly, the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6). The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably and that such a threshold may be higher for smaller models.\n4.2 Effects of model scale and pretraining stage on knowledge acquisition dynamics\nNext, we measure effectivity (Eq. 2) to quantify the improvement of the LLMs‚Äô log probability after being trained with the injected knowledge, averaged across all probes (q) and encounters (i). The results are demonstrated in Figure 3. The average effectivity is the largest in the Once injection scenario since the effectivity is higher when the model encounters the injected knowledge for the first time, which is further discussed in ¬ßH.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).5 On the other hand, surprisingly, the effectivity of fact acquisition does not improve with checkpoints trained with more tokens, as shown on the left side of Figure 3. This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11). Moreover, this tendency is not attributed to training the models with a decreased learning rate through learning rate decay, as demonstrated by an additional experiment of training three checkpoints using the same constant learning rate. The results with the constant learning rate show that effectivity does not significantly improve in the checkpoints of later stages of pretraining where more pretraining tokens are seen (¬ßF). Therefore, the observation implies that the effectivity of LLMs in acquiring factual knowledge does not significantly improve throughout the progress of pretraining.\nWhile our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model. We discuss this hypothesis further in ¬ß4.4.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Comparing the duplication and paraphrase injection scenarios, the duplication injection scenario naturally shows higher effectivity for memorization. However, the higher effectivity in the duplica- tion injection scenario for semantic generalization and compositional generalization appears to be\n5For a fair comparison of the effectivity of the 1B and 7B models, the OLMo-1B Mid checkpoint is trained using the same initial learning rate as the OLMo-7B Mid checkpoint (the specific value is provided in Appendix Table 5). The measured effectivity for all OLMo-1B checkpoints with the original learning rate is presented in Appendix Figure 11.\nFigure 4: Average retainability against training steps past the local acquisition maxima, measured with OLMo-7B mid checkpoint. The x-axes are in log scale. Left: duplication. Right: paraphrase.\nTable 2: Decay constant of average retainability (R(p, t)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.\nEarly (170B) Mid (500B) Late (1.5T)\nMemorization Semantic Composition\n0.26¬±0.0020 0.24¬±0.0018 0.18¬±0.0020\n0.25¬±0.0019 0.25¬±0.0022 0.20¬±0.0032\n0.20¬±0.0019 0.21¬±0.0021 0.16¬±0.0024\nMemorization Semantic Composition\n0.20¬±0.0019 0.20¬±0.0020 0.14¬±0.0025\n0.21¬±0.0023 0.23¬±0.0024 0.15¬±0.0022\n0.18¬±0.0022 0.21¬±0.0024 0.19¬±0.0030\ncounterintuitive, as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]. In the following sections, we will address this question by demonstrating that the models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts (¬ß4.3).\n4.3 Forgetting in factual knowledge acquisition'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]. Motivated by this, we investigate whether the exponential trend of forgetting persists in the context of factual knowledge acquisition in LLM pretraining. Figure 4 illustrates the trend of retainability against the training steps past the local acquisition maxima. We find that the trend of R(p, t) against log(t) fits a linear function very well (R2 > 0.80 for memorization and semantic generalization, and R2 > 0.65 for compositional generalization). This trend is persistent across all acquisition depths, and all training conditions (¬ßE.4 and ¬ßE.5). Guided by empirical observations, we model the trend of forgetting using a power-law model in further investigations.\nHow quickly is the acquired factual knowledge lost? The absolute value of the slope of the fitted lines in Figure 4 can be interpreted as the decay constant (a) of retainability, formally,\nfor 0 < t1 < t2 < œÑ, where R(p, œÑ ) = 0 and a > 0.\nThus, the measured decay constant represents how fast (in terms of fraction) the model loses the improvement of log probability. Table 2 shows the decay constants of retainability measured for three OLMo-7B intermediate checkpoints, for duplication and paraphrase injection scenarios.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='There are several observations in Table 2. First, the forgetting in compositional generalization is slower (the decay constant a is smaller) than in memorization and semantic generalization. Combined with the observations in previous sections, the acquisition of compositional generalization accumulates most slowly but is more robust to forgetting. Second, the forgetting tends to be slower in the paraphrase injection scenario compared to the duplication injection scenario. This finding will be further discussed in ¬ß4.4, regarding the importance of deduplicating training data. Finally, the decay constants are similar for the two earlier checkpoints but smaller for the late checkpoint in the duplication injection scenario. We demonstrate that this is due to the reduced learning rate from\nFigure 5: Comparison of the forgetting dynamics of pretraining (Left) and training with reduced batch size (Right), measured with OLMo-7B mid checkpoint. Note that the x-axis represents the number of training tokens instead of training steps, which has a shifting effect on the data plotted in Figure 4.\nlearning rate scheduling (Appendix Table 5), as the decay constants show no decrease for the later checkpoint when each checkpoint is trained with the same constant learning rate (Appendix Table 9).\nPretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49]. However, the effects of increasing training batch size in terms of the LLMs‚Äô acquisition of factual knowledge remain underexplored. In this section, we examine whether pretraining LLMs with a larger batch size is advantageous regarding factual knowledge acquisition. Specifically, we continue training LLMs with a batch size reduced by a factor of 16 compared to the original pretraining batch size, i.e., from 2048 to 128.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size. The results have several implications for the advantage of pretraining LLMs with a larger batch size. First, comparing Figure 3 and Appendix Figure 21, LLMs trained with the smaller batch size show higher effectivity. However, the decay constant tends to be higher, comparing the numbers in Table 2 and Appendix Table 10. Furthermore, the anticipated x-intercept is significantly decreased by dozens of times, comparing Appendix Table 6 and 11. This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold, which we discuss in detail in the following section (¬ß4.4). In other words, when an LLM is trained with a smaller batch size, factual knowledge should be presented more often to the model so as not to be forgotten and the set of learnable knowledge is reduced. Second, accelerated forgetting with a smaller batch size is more pronounced for compositional generalization compared to memorization and semantic generalization. In brief, the results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge.\nImplications for LLM pretraining\nWhy is popularity important for factual knowledge acquisition? The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.6 Hence, if a given factual knowledge in the pretraining dataset is in the long-tail and the knowledge is presented to the model with an interval longer than a certain threshold, such knowledge will be impossible to be decoded as the top-k generation of'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='6The exact values of the estimated x-intercepts can be found in Appendix Table 6.\nthe model, or learned, regardless of the duration of the pretraining.7 This implies that there is a learnability threshold, a threshold of the interval where the model fails to acquire knowledge of which its encounter interval is longer than the threshold. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold. In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]. Moreover, LLMs will accumulate the log probability faster for more popular knowledge, and thus the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].\nIn summary, we hypothesize that the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‚Äòrevealed‚Äô in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in ¬ß4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge (more knowledge will be presented with an interval shorter than the learnability threshold) since the skewness of the distribution of factual knowledge popularity is likely to be mitigated as the data becomes larger and more diverse.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Why does deduplication enhance model performance? Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]. Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in ¬ß4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge. This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization (Figure 2 and 3), presenting the model with duplicated texts with a short interval will result in the widening of the gap between memorization and generalization, which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].\n5 Discussion and Conclusions\nIn this work, we study how LLMs acquire factual knowledge during pretraining. Our findings and contributions can be summarized as follows:\nWe propose methods, datasets, and metrics for performing a fine-grained analysis of factual knowledge acquisition dynamics during LLM pretraining.\nWe demonstrate that factual knowledge acquisition in LLM pretraining is achieved through accumulating micro-acquisitions, each of which occurs whenever the model is updated after seeing the factual knowledge. When the model is not presented with factual knowledge, forgetting occurs and the acquisition of the knowledge is gradually diluted.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different.\nThere is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\n7This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge. In addition, the actual learnability threshold is likely to vary for different types of factual knowledge due to several factors, such as the number of similar/related facts or temporal conflicts in the pretraining data.\nWe provide potential explanations for recently observed, yet underexplored behaviors of LLMs. First, we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining. Second, we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.'), Document(metadata={'Title': 'How Do Large Language Models Acquire Factual Knowledge During Pretraining?', 'Key': 'Introduction'}, page_content='Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.\nWe would like to thank Seongyun Lee, Suehyun Park, Hyeonbin Hwang, Geewook Kim, Juyoung Suk, Aengus Lynch, and Katja Filippova for their valuable feedback on our work.\nThis work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program(KAIST).')], 'References': {'27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 14, 'Context': ['[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.', 'Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.', 'Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', '...the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': 'We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.', 'pdf_url': 'http://arxiv.org/pdf/2001.08361v1'}, '21': {'Title': 'Olmo: Accelerating the science of language models', 'Authors': 'Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi', 'Counter': 11, 'Context': ['To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [21].', 'To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained...', 'the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6).', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [21].', 'To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably.', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].', 'Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size.', 'This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold.'], 'abstract': None, 'pdf_url': None}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 10, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.', 'the importance of dataset deduplication [29, 52]', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.', '...the importance of dataset deduplication [29, 52] can be explained.', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].'], 'abstract': 'We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.', 'pdf_url': 'http://arxiv.org/pdf/2107.06499v2'}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 9, 'Context': ['Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.", 'pdf_url': 'http://arxiv.org/pdf/2312.12391v2'}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 7, 'Context': ['Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].'], 'abstract': 'Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.', 'pdf_url': 'http://arxiv.org/pdf/2205.10770v2'}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 6, 'Context': ['To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].'], 'abstract': "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.", 'pdf_url': 'http://arxiv.org/pdf/2304.15004v2'}, '52': {'Title': 'To repeat or not to repeat: Insights from scaling llm under token-crisis', 'Authors': 'Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You', 'Counter': 6, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'the importance of dataset deduplication [29, 52]', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...the importance of dataset deduplication [29, 52] can be explained.', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].']}, '9': {'Title': 'Language models are few-shot learners', 'Authors': 'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.', 'Counter': 5, 'Context': ['Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in ¬ß4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '36': {'Title': 'Language models as knowledge bases?', 'Authors': 'Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller', 'Counter': 5, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].']}, '40': {'Title': 'How much knowledge can you pack into the parameters of a language model?', 'Authors': 'Adam Roberts, Colin Raffel, Noam M. Shazeer', 'Counter': 5, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].']}, '43': {'Title': 'Dolma: An open corpus of three trillion tokens for language model pretraining research', 'Authors': 'Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al.', 'Counter': 5, 'Context': ['...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', '...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', '...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '4': {'Title': 'Physics of language models: Part 3.1, knowledge storage and extraction', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 4, 'Context': ['...but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization...', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', '...which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.']}, '5': {'Title': 'Physics of language models: Part 3.2, knowledge manipulation', 'Authors': 'Zeyuan Allen-Zhu, Yuanzhi Li', 'Counter': 4, 'Context': ['In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].']}, '7': {'Title': 'Emergent and predictable memorization in large language models', 'Authors': 'Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, Edward Raf', 'Counter': 4, 'Context': ['Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.', 'In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance.', 'Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model‚Äôs log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.', 'This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge.']}, '26': {'Title': 'Large language models struggle to learn long-tail knowledge', 'Authors': 'Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, Colin Raffel', 'Counter': 4, 'Context': ['...the failure to acquire long-tail knowledge...', 'the failure to acquire long-tail knowledge [26, 34]', '...the failure to acquire long-tail knowledge [26, 34]...', 'However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].']}, '33': {'Title': 'An empirical study of catastrophic forgetting in large language models during continual fine-tuning', 'Authors': 'Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang', 'Counter': 4, 'Context': ['The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].']}, '49': {'Title': 'Llama 2: Open foundation and fine-tuned chat models', 'Authors': 'Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant√≥n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom', 'Counter': 4, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [49].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [49].', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '50': {'Title': 'Emergent abilities of large language models', 'Authors': 'Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.', 'Counter': 4, 'Context': ['We suggest a plausible hypothesis based on further observations in ¬ß4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training.', '...we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times...', '...we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.', 'Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model.']}, '2': {'Title': 'Gpt-4 technical report', 'Authors': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.', 'Counter': 3, 'Context': ['An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.']}, '6': {'Title': 'A closer look at memorization in deep networks', 'Authors': 'Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, Simon Lacoste-Julien', 'Counter': 3, 'Context': ['Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].', 'Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].', 'Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].']}, '8': {'Title': 'Pythia: A suite for analyzing large language models across training and scaling', 'Authors': 'Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal', 'Counter': 3, 'Context': ['...the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‚Äòrevealed‚Äô in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in ¬ß4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge...', '...as demonstrated in [8].', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].']}, '10': {'Title': 'Extracting training data from large language models', 'Authors': 'Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, √ölfar Erlingsson, Alina Oprea, Colin Raffel', 'Counter': 3, 'Context': ['LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].']}, '12': {'Title': 'Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs', 'Authors': 'Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, Naomi Saphra', 'Counter': 3, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '13': {'Title': 'Palm: Scaling language modeling with pathways', 'Authors': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc√≠a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D√≠az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '14': {'Title': 'Analyzing commonsense emergence in few-shot knowledge models', 'Authors': 'Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut', 'Counter': 3, 'Context': ['Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].', 'Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].']}, '25': {'Title': 'Mistral 7b', 'Authors': 'Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L‚Äôelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [25].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [25].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '28': {'Title': 'The bigscience roots corpus: A 1.6 tb composite multilingual dataset', 'Authors': 'Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al.', 'Counter': 3, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '30': {'Title': 'Starcoder: may the source be with you!', 'Authors': 'Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm de Vries', 'Counter': 3, 'Context': ['It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [30].', 'It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [30].', 'Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49].']}, '34': {'Title': 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories', 'Authors': 'Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi', 'Counter': 3, 'Context': ['the failure to acquire long-tail knowledge [26, 34]', '...the failure to acquire long-tail knowledge [26, 34]...', 'However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].']}, '35': {'Title': 'Entity cloze by date: What lms know about unseen entities', 'Authors': 'Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, Greg Durrett', 'Counter': 3, 'Context': ['An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.', 'An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.']}, '53': {'Title': 'Critical data size of language models from a grokking perspective', 'Authors': 'Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin', 'Counter': 3, 'Context': ['Recently, [53] explored the relationship between the data size and grokking [37].', 'Recently, [53] explored the relationship between the data size and grokking [37].', 'Recently, [53] explored the relationship between the data size and grokking [37].']}, '18': {'Title': 'Does fine-tuning llms on new knowledge encourage hallucinations?', 'Authors': 'Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig', 'Counter': 2, 'Context': ['[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '19': {'Title': 'Transformer feed-forward layers are key-value memories', 'Authors': 'Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy', 'Counter': 2, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.', 'This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in ¬ß 4.4.']}, '39': {'Title': 'Anatomy of catastrophic forgetting: Hidden representations and task semantics', 'Authors': 'Vinay Venkatesh Ramasesh, Ethan Dyer, Maithra Raghu', 'Counter': 2, 'Context': ['The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].']}, '44': {'Title': 'Memorisation versus generalisation in pre-trained language models', 'Authors': 'Michael T√§nzer, Sebastian Ruder, Marek Rei', 'Counter': 2, 'Context': ['[44] and [46] focused on the dynamics of memorization in language model pretraining.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.']}, '47': {'Title': 'D4: Improving llm pretraining via document de-duplication and diversification', 'Authors': 'Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos', 'Counter': 2, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '48': {'Title': 'Llama: Open and efficient foundation language models', 'Authors': 'Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.', 'Counter': 2, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].', 'Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '1': {'Title': 'Semdedup: Data-efficient learning at web-scale through semantic deduplication', 'Authors': 'Amro Abbas, Kushal Tirumala, D√°niel Simig, Surya Ganguli, Ari S. Morcos', 'Counter': 1, 'Context': ['This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement.']}, '3': {'Title': 'Tracing knowledge in language models back to the training data', 'Authors': 'Ekin Aky√ºrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '15': {'Title': 'Knowledge neurons in pretrained transformers', 'Authors': 'Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '16': {'Title': 'Measuring causal effects of data statistics on language model‚Äôs ‚Äôfactual‚Äô predictions', 'Authors': 'Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, Yoav Goldberg', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '17': {'Title': 'Does learning require memorization? a short tale about a long tail', 'Authors': 'Vitaly Feldman', 'Counter': 1, 'Context': ['In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance.']}, '20': {'Title': 'Dissecting recall of factual associations in auto-regressive language models', 'Authors': 'Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '22': {'Title': 'Investigating learning dynamics of bert fine-tuning', 'Authors': 'Yaru Hao, Li Dong, Furu Wei, Ke Xu', 'Counter': 1, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '31': {'Title': 'How pre-trained language models capture factual knowledge? a causal-inspired analysis', 'Authors': 'Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu', 'Counter': 1, 'Context': ['[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.']}, '32': {'Title': 'Probing across time: What does RoBERTa know and when?', 'Authors': 'Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith', 'Counter': 1, 'Context': ['Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].']}, '38': {'Title': 'Exploring the limits of transfer learning with a unified text-to-text transformer', 'Authors': 'Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu', 'Counter': 1, 'Context': ['Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].']}, '11': {'Title': 'Quantifying memorization across neural language models', 'Authors': 'Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram√®r, Chiyuan Zhang', 'Counter': 0, 'Context': []}, '24': {'Title': 'The surprising simplicity of the early-time learning dynamics of neural networks', 'Authors': 'Wei Hu, Lechao Xiao, Ben Adlam, Jeffrey Pennington', 'Counter': 0, 'Context': []}, '37': {'Title': 'Grokking: Generalization beyond overfitting on small algorithmic datasets', 'Authors': 'Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra', 'Counter': 0, 'Context': []}, '42': {'Title': 'Noise-robust de- duplication at scale', 'Authors': 'Emily Silcock, Luca D‚ÄôAmico-Wong, Jinglin Yang, Melissa Dell', 'Counter': 0, 'Context': []}, '45': {'Title': 'Emergent structures and training dynamics in large language models', 'Authors': 'Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan', 'Counter': 0, 'Context': []}, '51': {'Title': 'Training trajectories of language models across scales', 'Authors': 'Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov', 'Counter': 0, 'Context': []}}} 

step 6:  
 {'27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 14, 'Context': ['[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.', 'Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.', 'Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', '...the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': 'We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.', 'pdf_url': 'http://arxiv.org/pdf/2001.08361v1', 'Questions': '1. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.\n2. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.\n3. The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\n4. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\n5. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\n6. These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).\n7. The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\n8. There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\n9. Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.\n10. While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 10, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.', 'the importance of dataset deduplication [29, 52]', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.', '...the importance of dataset deduplication [29, 52] can be explained.', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].'], 'abstract': 'We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.', 'pdf_url': 'http://arxiv.org/pdf/2107.06499v2', 'Questions': '1. Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.\n2. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].\n3. Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.'}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 9, 'Context': ['Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.", 'pdf_url': 'http://arxiv.org/pdf/2312.12391v2', 'Questions': '1. Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].\n2. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.\n3. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].\n4. In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.'}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 7, 'Context': ['Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].'], 'abstract': 'Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.', 'pdf_url': 'http://arxiv.org/pdf/2205.10770v2', 'Questions': '1. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.\n2. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].\n3. [44] and [46] focused on the dynamics of memorization in language model pretraining.'}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 6, 'Context': ['To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].'], 'abstract': "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.", 'pdf_url': 'http://arxiv.org/pdf/2304.15004v2', 'Questions': '1. To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.\n2. ...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].\n3. ...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].'}} 

Ïù∏Ïö© ÎÖºÎ¨∏Í≥ºÏùò Í¥ÄÎ†® ÏßÄÏ†ê Ï†ïÎ¶¨...:   0%|          | 0/5 [00:00<?, ?it/s]Ïù∏Ïö© ÎÖºÎ¨∏Í≥ºÏùò Í¥ÄÎ†® ÏßÄÏ†ê Ï†ïÎ¶¨...:  20%|‚ñà‚ñà        | 1/5 [01:33<06:14, 93.74s/it]Ïù∏Ïö© ÎÖºÎ¨∏Í≥ºÏùò Í¥ÄÎ†® ÏßÄÏ†ê Ï†ïÎ¶¨...:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [01:54<02:32, 50.67s/it]Ïù∏Ïö© ÎÖºÎ¨∏Í≥ºÏùò Í¥ÄÎ†® ÏßÄÏ†ê Ï†ïÎ¶¨...:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [02:15<01:14, 37.32s/it]Ïù∏Ïö© ÎÖºÎ¨∏Í≥ºÏùò Í¥ÄÎ†® ÏßÄÏ†ê Ï†ïÎ¶¨...:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [02:37<00:31, 31.08s/it]Ïù∏Ïö© ÎÖºÎ¨∏Í≥ºÏùò Í¥ÄÎ†® ÏßÄÏ†ê Ï†ïÎ¶¨...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:58<00:00, 27.49s/it]Ïù∏Ïö© ÎÖºÎ¨∏Í≥ºÏùò Í¥ÄÎ†® ÏßÄÏ†ê Ï†ïÎ¶¨...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:58<00:00, 35.67s/it]
step 7:  
 {'27': {'Title': 'Scaling laws for neural language models', 'Authors': 'Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei', 'Counter': 14, 'Context': ['[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.', 'Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.', 'This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.', 'These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.', 'Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', '...the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', 'The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': 'We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.', 'pdf_url': 'http://arxiv.org/pdf/2001.08361v1', 'Questions': '1. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.\n2. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.\n3. The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\n4. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\n5. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\n6. These patterns are consistent across all pretraining stages of OLMo-7B we investigate (¬ßE.1).\n7. The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\n8. There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\n9. Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.\n10. While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', 'Summary': 'Ïù∏Ïö© ÎÖºÎ¨∏ Ï†úÎ™© (Title): Scaling laws for neural language models\n\n1. **ÏßàÎ¨∏ :** [23]Í≥º [27]ÏùÄ LLMÏùò ÏÑ±Îä•Ïù¥ Ïä§ÏºÄÏùºÎßÅ Î≤ïÏπôÏùÑ Îî∞Î•¥Î©∞, Ïù¥Îäî Î™®Îç∏ ÌÅ¨Í∏∞ÏôÄ ÏÇ¨Ï†Ñ ÌõàÎ†® ÏΩîÌçºÏä§Ïùò ÌÅ¨Í∏∞ÏôÄ Í∏çÏ†ïÏ†ÅÏúºÎ°ú ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä ÏûàÎã§Í≥† Î≥¥Í≥†ÌñàÏäµÎãàÎã§.\n   - **ÎãµÎ≥Ä :** LLMÏùò ÏÑ±Îä•ÏùÄ Î™®Îç∏ ÌÅ¨Í∏∞ÏôÄ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞Ïóê Îî∞Îùº Ïä§ÏºÄÏùºÎßÅ Î≤ïÏπôÏùÑ Îî∞Î¶ÖÎãàÎã§. Ï¶â, Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏñëÏù¥ Ï¶ùÍ∞ÄÌï†ÏàòÎ°ù ÏÑ±Îä•Ïù¥ Ìñ•ÏÉÅÎê©ÎãàÎã§. Ïù¥Îäî ÎåÄÍ∑úÎ™® Î™®Îç∏Ïù¥ Îçî Ï†ÅÏùÄ Îç∞Ïù¥ÌÑ∞Î°úÎèÑ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Î∞úÌúòÌï† Ïàò ÏûàÏùåÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "Performance depends strongly on scale, weakly on model shape: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N, the size of the dataset D, and the amount of compute C used for training."\n\n2. **ÏßàÎ¨∏ :** Ïö∞Î¶¨Îäî ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞ tw = 50ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. Îã§ÏùåÏúºÎ°ú, Î™®Îç∏Ïù¥ iÎ≤àÏß∏Î°ú ÏßÄÏãùÏùÑ Ï†ëÌñàÏùÑ Îïå ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò Î°úÍ∑∏ ÌôïÎ•†ÏóêÏÑú Ï¶âÍ∞ÅÏ†ÅÏù∏ Í∞úÏÑ†ÏùÑ Ï†ïÎüâÌôîÌïòÎäî Î©îÌä∏Î¶≠ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.\n   - **ÎãµÎ≥Ä :** ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞ tw = 50ÏùÑ ÏÑ§Ï†ïÌïòÍ≥†, Î™®Îç∏Ïù¥ ÌäπÏ†ï ÏßÄÏãùÏùÑ Î∞òÎ≥µÏ†ÅÏúºÎ°ú Ï†ëÌï† ÎïåÎßàÎã§ Í∑∏ ÏßÄÏãùÏóê ÎåÄÌïú Î°úÍ∑∏ ÌôïÎ•†Ïùò Í∞úÏÑ†ÏùÑ Ï∏°Ï†ïÌïòÎäî Î©îÌä∏Î¶≠ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïùò ÏßÄÏãù ÏäµÎìù Í≥ºÏ†ïÏùÑ Ï†ïÎüâÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÎäî Îç∞ Ïú†Ïö©Ìï©ÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time."\n\n3. **ÏßàÎ¨∏ :** Ï†ïÏùòÎêú Î©îÌä∏Î¶≠Ïùò Ï∏°Ï†ïÏùÄ Í∑∏Î¶º 1Ïóê ÏÑ§Î™ÖÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Ìö®Í≥ºÏÑ±Í≥º Ïú†ÏßÄ Í∞ÄÎä•ÏÑ±Ïùò Ï∏°Ï†ïÏùÑ ÏúÑÌï¥, Ïö∞Î¶¨Îäî 1.5Ïùò Í≥ÑÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ IQR Î∞©Î≤ïÏùÑ Ïù¥Ïö©Ìïú Ïù¥ÏÉÅÏπò ÌÉêÏßÄÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.\n   - **ÎãµÎ≥Ä :** Ìö®Í≥ºÏÑ±Í≥º Ïú†ÏßÄ Í∞ÄÎä•ÏÑ±ÏùÑ Ï∏°Ï†ïÌïòÍ∏∞ ÏúÑÌï¥ IQR Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÏÉÅÏπòÎ•º ÌÉêÏßÄÌïòÎ©∞, Ïù¥ Í≥ºÏ†ïÏùÄ Í∑∏Î¶º 1ÏóêÏÑú ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÏÑ§Î™ÖÎê©ÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞ Ï§ëÏöîÌïú Î∞©Î≤ïÎ°†ÏûÖÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5."\n\n4. **ÏßàÎ¨∏ :** Í≤∞Í≥ºÎäî Ï§ëÎ≥µ(ÏÉÅÎã®), Ìå®Îü¨ÌîÑÎ†àÏù¥Ï¶à(Ï§ëÏïô), Ìïú Î≤à(ÌïòÎã®) Ï£ºÏûÖ ÏãúÎÇòÎ¶¨Ïò§Ïóê ÎåÄÌï¥ Î≥¥Ïó¨ÏßëÎãàÎã§.\n   - **ÎãµÎ≥Ä :** Ïó∞Íµ¨ Í≤∞Í≥ºÎäî Ï§ëÎ≥µ, Ìå®Îü¨ÌîÑÎ†àÏù¥Ï¶à, Í∑∏Î¶¨Í≥† Îã®Ïùº Ï£ºÏûÖ ÏãúÎÇòÎ¶¨Ïò§ÏóêÏÑúÏùò ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌïòÏó¨ Ï†úÏãúÎê©ÎãàÎã§. Ïù¥Îäî Îã§ÏñëÌïú ÏßÄÏãù Ï£ºÏûÖ Î∞©ÏãùÏù¥ Î™®Îç∏ ÏÑ±Îä•Ïóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Î∂ÑÏÑùÌïòÎäî Îç∞ Í∏∞Ïó¨Ìï©ÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios."\n\n5. **ÏßàÎ¨∏ :** Ïù¥ Í¥ÄÏ∞∞ÏùÄ ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìùÏùò Î©îÏª§ÎãàÏ¶òÏùÑ ÏßÅÏ†ëÏ†ÅÏúºÎ°ú Î≥¥Ïó¨Ï§çÎãàÎã§: LLMÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Ï§ë ÏßÄÏãùÏùÑ Ï†ëÌï† ÎïåÎßàÎã§ ÎØ∏ÏÑ∏Ìïú ÏäµÎìùÏùÑ Ï∂ïÏ†ÅÌïòÏó¨ ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ ÏäµÎìùÌï©ÎãàÎã§.\n   - **ÎãµÎ≥Ä :** LLMÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑú ÏßÄÏãùÏùÑ Î∞òÎ≥µÏ†ÅÏúºÎ°ú Ï†ëÌïòÎ©¥ÏÑú ÎØ∏ÏÑ∏Ìïú ÏäµÎìùÏùÑ ÌÜµÌï¥ ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ Ï∂ïÏ†ÅÌï©ÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïùò ÌïôÏäµ Î©îÏª§ÎãàÏ¶òÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ Ï§ëÏöîÌïú ÌÜµÏ∞∞ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining."\n\n6. **ÏßàÎ¨∏ :** Ïö∞Î¶¨Í∞Ä Ï°∞ÏÇ¨ÌïòÎäî OLMo-7BÏùò Î™®Îì† ÏÇ¨Ï†Ñ ÌõàÎ†® Îã®Í≥ÑÏóêÏÑú Ïù¥Îü¨Ìïú Ìå®ÌÑ¥Ïù¥ ÏùºÍ¥ÄÎêòÍ≤å ÎÇòÌÉÄÎÇ©ÎãàÎã§.\n   - **ÎãµÎ≥Ä :** OLMo-7B Î™®Îç∏Ïùò Î™®Îì† ÏÇ¨Ï†Ñ ÌõàÎ†® Îã®Í≥ÑÏóêÏÑú ÏßÄÏãù ÏäµÎìù Ìå®ÌÑ¥Ïù¥ ÏùºÍ¥ÄÎêòÍ≤å ÎÇòÌÉÄÎÇòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò ÌïôÏäµ Í≥ºÏ†ïÏù¥ ÏùºÏ†ïÌïú Í∑úÏπôÏÑ±ÏùÑ Í∞ÄÏßÑÎã§Îäî Í≤ÉÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "These patterns are consistent across all pretraining stages of OLMo-7B we investigate."\n\n7. **ÏßàÎ¨∏ :** Í∑∏Î¶º 5Ïùò Ï∂îÏ†ïÎêú x-Ï†àÌé∏ÏùÄ ÌõàÎ†®ÏùÑ ÌÜµÌï¥ ÏäµÎìùÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÏôÑÏ†ÑÌïú ÏÜêÏã§Î°ú Ïù¥Ïñ¥ÏßÄÎäî Ï∂îÍ∞Ä ÌõàÎ†® ÌÜ†ÌÅ∞Ïùò ÏàòÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§.\n   - **ÎãµÎ≥Ä :** Í∑∏Î¶º 5ÏóêÏÑú Ï∂îÏ†ïÎêú x-Ï†àÌé∏ÏùÄ Î™®Îç∏Ïù¥ ÌõàÎ†®ÏùÑ ÌÜµÌï¥ ÏäµÎìùÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏù¥ ÏôÑÏ†ÑÌûà ÏÜêÏã§ÎêòÍ∏∞ ÏúÑÌï¥ ÌïÑÏöîÌïú Ï∂îÍ∞Ä ÌõàÎ†® ÌÜ†ÌÅ∞Ïùò ÏàòÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïùò ÌõàÎ†® Ìö®Ïú®ÏÑ±ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞ Ï§ëÏöîÌïú ÏßÄÌëúÏûÖÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training."\n\n8. **ÏßàÎ¨∏ :** ÌõàÎ†® Îã®Í≥ÑÏôÄ ÏäµÎìùÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÎßùÍ∞Å ÏÇ¨Ïù¥ÏóêÎäî ÌååÏõå-Î°úÏö∞ Í¥ÄÍ≥ÑÍ∞Ä ÏûàÏäµÎãàÎã§.\n   - **ÎãµÎ≥Ä :** ÌõàÎ†® Îã®Í≥ÑÏôÄ ÏäµÎìùÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÎßùÍ∞Å ÏÇ¨Ïù¥ÏóêÎäî ÌååÏõå-Î°úÏö∞ Í¥ÄÍ≥ÑÍ∞Ä Ï°¥Ïû¨ÌïòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò Í∏∞ÏñµÎ†•Í≥º ÏùºÎ∞òÌôî Îä•Î†•Ïóê ÎåÄÌïú Ï§ëÏöîÌïú ÌÜµÏ∞∞ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization."\n\n9. **ÏßàÎ¨∏ :** Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú, Ïö∞Î¶¨Îäî LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Ïó≠ÌïôÏùÑ Ïù¥Ìï¥ÌïòÎäî Í≤ÉÏù¥ LLMÏùò ÌñâÎèôÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ Ï§ëÏöîÌïòÎã§Îäî Í≤ÉÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.\n   - **ÎãµÎ≥Ä :** LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Ïó≠ÌïôÏùÑ Ïù¥Ìï¥ÌïòÎäî Í≤ÉÏùÄ LLMÏùò ÌñâÎèôÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ ÌïÑÏàòÏ†ÅÏù¥Î©∞, Ïù¥Îäî Ìñ•ÌõÑ Ïó∞Íµ¨Ïùò Ïú†ÎßùÌïú Î∞©Ìñ•ÏùÑ Ïó¥Ïñ¥Ï§çÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research."\n\n10. **ÏßàÎ¨∏ :** Ìö®Í≥ºÏÑ±Ïù¥ ÏÇ¨Ï†Ñ ÌõàÎ†®Ïùò Îã§ÏñëÌïú Îã®Í≥ÑÏóêÏÑú Î≥ÄÌïòÏßÄ ÏïäÎäîÎã§Îäî Ïö∞Î¶¨Ïùò Î∞úÍ≤¨ÏùÄ LLMÏùò ÏÑ±Îä•Ïóê ÏûàÏñ¥ ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏñëÏù¥ Ï§ëÏöîÌïú ÏöîÏÜåÎùºÎäî Ïûò ÏïåÎ†§ÏßÑ Í¥ÄÏ∞∞Í≥º Î™®ÏàúÎêòÎäî Í≤ÉÏ≤òÎüº Î≥¥Ïùº Ïàò ÏûàÏäµÎãàÎã§.\n    - **ÎãµÎ≥Ä :** Ìö®Í≥ºÏÑ±Ïù¥ ÏÇ¨Ï†Ñ ÌõàÎ†®Ïùò Îã§ÏñëÌïú Îã®Í≥ÑÏóêÏÑú Î≥ÄÌïòÏßÄ ÏïäÎäîÎã§Îäî Î∞úÍ≤¨ÏùÄ LLM ÏÑ±Îä•Ïóê ÏûàÏñ¥ ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏñëÏù¥ Ï§ëÏöîÌïú ÏöîÏÜåÎùºÎäî Í∏∞Ï°¥Ïùò Í¥ÄÏ∞∞Í≥º Î™®ÏàúÎê† Ïàò ÏûàÏßÄÎßå, Ï∂îÍ∞Ä Í¥ÄÏ∞∞ÏùÑ ÌÜµÌï¥ ÏÑ§Î™ÖÌï† Ïàò ÏûàÎäî Í∞ÄÏÑ§ÏùÑ Ï†úÏïàÌï©ÎãàÎã§.\n    - **Í∑ºÍ±∞ :** "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs, we suggest a plausible hypothesis based on further observations."\n\n### ÎãµÎ≥Ä ÏöîÏïΩ (Summary of Answers)\nÏù¥ ÎÖºÎ¨∏ÏóêÏÑúÎäî LLMÏùò ÏÑ±Îä•Ïù¥ Î™®Îç∏ ÌÅ¨Í∏∞ÏôÄ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞Ïóê Îî∞Îùº Ïä§ÏºÄÏùºÎßÅ Î≤ïÏπôÏùÑ Îî∞Î•¥Î©∞, Ïù¥Îäî LLMÏùò ÌïôÏäµ Î©îÏª§ÎãàÏ¶òÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ Ï§ëÏöîÌïú ÌÜµÏ∞∞ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. LLMÏùÄ Î∞òÎ≥µÏ†ÅÏù∏ ÏßÄÏãù ÏäµÎìùÏùÑ ÌÜµÌï¥ ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ Ï∂ïÏ†ÅÌïòÎ©∞, Ïù¥Îü¨Ìïú Ìå®ÌÑ¥ÏùÄ Î™®Îì† ÏÇ¨Ï†Ñ ÌõàÎ†® Îã®Í≥ÑÏóêÏÑú ÏùºÍ¥ÄÎêòÍ≤å ÎÇòÌÉÄÎÇ©ÎãàÎã§. ÎòêÌïú, ÌõàÎ†® Îã®Í≥ÑÏôÄ ÏßÄÏãùÏùò ÎßùÍ∞Å ÏÇ¨Ïù¥ÏóêÎäî ÌååÏõå-Î°úÏö∞ Í¥ÄÍ≥ÑÍ∞Ä Ï°¥Ïû¨ÌïòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò Í∏∞ÏñµÎ†•Í≥º ÏùºÎ∞òÌôî Îä•Î†•Ïóê ÎåÄÌïú Ï§ëÏöîÌïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§. LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Ïó≠ÌïôÏùÑ Ïù¥Ìï¥ÌïòÎäî Í≤ÉÏùÄ LLMÏùò ÌñâÎèôÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ ÌïÑÏàòÏ†ÅÏù¥Î©∞, Ìñ•ÌõÑ Ïó∞Íµ¨Ïùò Ïú†ÎßùÌïú Î∞©Ìñ•ÏùÑ Ï†úÏãúÌï©ÎãàÎã§. ÎßàÏßÄÎßâÏúºÎ°ú, Ìö®Í≥ºÏÑ±Ïù¥ ÏÇ¨Ï†Ñ ÌõàÎ†®Ïùò Îã§ÏñëÌïú Îã®Í≥ÑÏóêÏÑú Î≥ÄÌïòÏßÄ ÏïäÎäîÎã§Îäî Î∞úÍ≤¨ÏùÄ Í∏∞Ï°¥Ïùò Í¥ÄÏ∞∞Í≥º Î™®ÏàúÎê† Ïàò ÏûàÏßÄÎßå, Ï∂îÍ∞Ä Í¥ÄÏ∞∞ÏùÑ ÌÜµÌï¥ ÏÑ§Î™ÖÌï† Ïàò ÏûàÎäî Í∞ÄÏÑ§ÏùÑ Ï†úÏïàÌï©ÎãàÎã§.', 'Summary_QnA': 'Ìï¥Îãπ ÎÖºÎ¨∏ÏùÄ LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Î©îÏª§ÎãàÏ¶òÏùÑ Ïã¨Ï∏µÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÎ©∞, Ïù∏Ïö© ÎÖºÎ¨∏ÏóêÏÑú Ï†úÏãúÎêú Ïä§ÏºÄÏùºÎßÅ Î≤ïÏπôÍ≥ºÏùò Í¥ÄÍ≥ÑÎ•º ÌÉêÍµ¨Ìï©ÎãàÎã§. LLMÏùò ÏÑ±Îä•Ïù¥ Î™®Îç∏ ÌÅ¨Í∏∞ÏôÄ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞Ïóê Îî∞Îùº Ïä§ÏºÄÏùºÎßÅ Î≤ïÏπôÏùÑ Îî∞Î•¥ÏßÄÎßå, Ïù¥ ÎÖºÎ¨∏ÏùÄ Îç∞Ïù¥ÌÑ∞ ÏñëÏù¥ Ï¶ùÍ∞ÄÌï¥ÎèÑ ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÏäµÎìùÍ≥º Ïú†ÏßÄÏóê Ïú†ÏùòÎØ∏Ìïú Í∞úÏÑ†Ïù¥ ÏóÜÏùåÏùÑ Î∞úÍ≤¨ÌñàÏäµÎãàÎã§. ÎòêÌïú, Î∞òÎ≥µÏ†ÅÏù∏ ÏßÄÏãù Ï†ëÏ¥âÏùÑ ÌÜµÌï¥ ÎØ∏ÏÑ∏Ìïú ÏäµÎìùÏù¥ Ïù¥Î£®Ïñ¥ÏßÄÎ©∞, ÌõàÎ†® Îã®Í≥ÑÏôÄ ÎßùÍ∞Å ÏÇ¨Ïù¥Ïùò ÌååÏõå-Î°úÏö∞ Í¥ÄÍ≥ÑÎ•º ÌÜµÌï¥ Î™®Îç∏Ïùò Í∏∞ÏñµÎ†•Í≥º ÏùºÎ∞òÌôî Îä•Î†•Ïóê ÎåÄÌïú ÌÜµÏ∞∞ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. Ïù¥Îü¨Ìïú Ïó∞Íµ¨ Í≤∞Í≥ºÎäî LLMÏùò ÌñâÎèô Ïù¥Ìï¥Ïóê ÌïÑÏàòÏ†ÅÏù¥Î©∞, Ìñ•ÌõÑ Ïó∞Íµ¨ Î∞©Ìñ•ÏùÑ Ï†úÏãúÌï©ÎãàÎã§.'}, '29': {'Title': 'Deduplicating training data makes language models better', 'Authors': 'Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini', 'Counter': 10, 'Context': ['...the importance of dataset deduplication...', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', 'Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.', 'the importance of dataset deduplication [29, 52]', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', 'it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].', '...pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.', '...the importance of dataset deduplication [29, 52] can be explained.', 'LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].', '...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].'], 'abstract': 'We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.', 'pdf_url': 'http://arxiv.org/pdf/2107.06499v2', 'Questions': '1. Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.\n2. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].\n3. Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.', 'Summary': 'Ïù∏Ïö© ÎÖºÎ¨∏ Ï†úÎ™© (Title): Deduplicating training data makes language models better\n\n1. **ÏßàÎ¨∏ :** Ïö∞Î¶¨Ïùò Î∞úÍ≤¨ÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® ÏΩîÌçºÏä§Î•º Ï§ëÎ≥µ Ï†úÍ±∞ÌïòÎäî Í≤ÉÏù¥ LLM ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ®Îã§Îäî Í≤ÉÏùÑ ÏãúÏÇ¨ÌïòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïù¥ Ï§ëÎ≥µÎêú ÏãúÌÄÄÏä§Ïóê Îçî ÎÜíÏùÄ ÌôïÎ•†ÏùÑ Î∂ÄÏó¨ÌïòÎäî Í≤ÉÏùÑ Î∞©ÏßÄÌïòÍ≥†, ÏäµÎìùÌïú ÏùºÎ∞òÌôîÎ•º Îçî Ïò§Îûò Ïú†ÏßÄÌïòÎäî Îç∞ ÎèÑÏõÄÏùÑ Ï§ÄÎã§.\n   - **ÎãµÎ≥Ä :** Ï§ëÎ≥µ Ï†úÍ±∞Îäî LLMÏùò ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Îäî Ï§ëÏöîÌïú ÏöîÏÜåÎ°ú, Î™®Îç∏Ïù¥ Ï§ëÎ≥µÎêú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞ÏñµÌïòÎäî ÎπàÎèÑÎ•º Ï§ÑÏù¥Í≥†, Îçî Ï†ÅÏùÄ ÌõàÎ†® Îã®Í≥ÑÎ°úÎèÑ ÎèôÏùºÌïòÍ±∞ÎÇò Îçî ÎÇòÏùÄ Ï†ïÌôïÎèÑÎ•º Îã¨ÏÑ±Ìï† Ïàò ÏûàÎèÑÎ°ù ÎèïÎäîÎã§. Ïù¥Îäî Î™®Îç∏Ïù¥ Ï§ëÎ≥µÎêú ÏãúÌÄÄÏä§Ïóê ÎåÄÌïú ÌôïÎ•†ÏùÑ ÎÜíÏù¥ÏßÄ ÏïäÎèÑÎ°ù ÌïòÏó¨ ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ Ïú†ÏßÄÌïòÎäî Îç∞ Í∏∞Ïó¨ÌïúÎã§.\n   - **Í∑ºÍ±∞ :** "Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy."\n\n2. **ÏßàÎ¨∏ :** LLMÏùÄ ÏÉÅÎãπÎüâÏùò ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Î•º Í∏∞ÏñµÌïòÎ©∞, Î™®Îç∏Ïùò ÌÅ¨Í∏∞Í∞Ä Ïª§ÏßàÏàòÎ°ù ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Î•º Í∏∞ÏñµÌïòÎäî Í≤ΩÌñ•Ïù¥ Ï¶ùÍ∞ÄÌïòÏßÄÎßå, Ïù¥Îäî ÏßÄÏãù ÏùºÎ∞òÌôî Îä•Î†•ÏóêÎäî Ìï¥Î•º ÎÅºÏπòÏßÄ ÏïäÎäîÎã§.\n   - **ÎãµÎ≥Ä :** LLMÏùÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏÉÅÎãπ Î∂ÄÎ∂ÑÏùÑ Í∏∞ÏñµÌïòÎäî Í≤ΩÌñ•Ïù¥ ÏûàÏúºÎ©∞, Î™®Îç∏Ïùò ÌÅ¨Í∏∞Í∞Ä Ïª§ÏßàÏàòÎ°ù Ïù¥Îü¨Ìïú Í≤ΩÌñ•Ïù¥ ÎçîÏö± ÎëêÎìúÎü¨ÏßÑÎã§. Í∑∏Îü¨ÎÇò Ïù¥Îü¨Ìïú Í∏∞ÏñµÏùÄ Î™®Îç∏Ïùò ÏùºÎ∞òÌôî Îä•Î†•Ïóê Î∂ÄÏ†ïÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÏßÄ ÏïäÏúºÎ©∞, Ïò§ÌûàÎ†§ Îçî ÎßéÏùÄ Îç∞Ïù¥ÌÑ∞Î•º ÌÜµÌï¥ Îçî ÎÇòÏùÄ ÏÑ±Îä•ÏùÑ Î∞úÌúòÌï† Ïàò ÏûàÎã§.\n   - **Í∑ºÍ±∞ :** "LLMs memorize a significant amount of training data... and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge."\n\n3. **ÏßàÎ¨∏ :** Ï§ëÎ≥µ Ï†úÍ±∞Îêú Îç∞Ïù¥ÌÑ∞ÏôÄ Îçî ÌÅ∞ Î∞∞Ïπò ÌÅ¨Í∏∞Î°ú LLMÏùÑ ÏÇ¨Ï†Ñ ÌõàÎ†®ÌïòÎ©¥ ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìùÏù¥ Ìñ•ÏÉÅÎêòÏñ¥ ÌïôÏäµÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ ÏûäÏñ¥Î≤ÑÎ¶¨Îäî Í≤ÉÏóê ÎåÄÌï¥ Îçî Í∞ïÎ†•Ìï¥ÏßÑÎã§.\n   - **ÎãµÎ≥Ä :** Ï§ëÎ≥µ Ï†úÍ±∞Îêú Îç∞Ïù¥ÌÑ∞Î°ú LLMÏùÑ ÌõàÎ†®ÌïòÎ©¥ ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÏäµÎìùÏù¥ Ìñ•ÏÉÅÎêòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïù¥ ÌïôÏäµÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ ÏûäÏñ¥Î≤ÑÎ¶¨Îäî Í≤ΩÌñ•ÏùÑ Ï§ÑÏó¨Ï§ÄÎã§. Îçî ÌÅ∞ Î∞∞Ïπò ÌÅ¨Í∏∞Îäî Ïù¥Îü¨Ìïú Ìö®Í≥ºÎ•º ÎçîÏö± Í∞ïÌôîÌïúÎã§.\n   - **Í∑ºÍ±∞ :** "Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge."\n\nÎãµÎ≥Ä ÏöîÏïΩ \n: Ï§ëÎ≥µ Ï†úÍ±∞Îäî LLMÏùò ÏÑ±Îä• Ìñ•ÏÉÅÏóê Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÌïòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïù¥ Ï§ëÎ≥µÎêú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞ÏñµÌïòÎäî ÎπàÎèÑÎ•º Ï§ÑÏù¥Í≥†, Îçî Ï†ÅÏùÄ ÌõàÎ†® Îã®Í≥ÑÎ°úÎèÑ ÎèôÏùºÌïú Ï†ïÌôïÎèÑÎ•º Îã¨ÏÑ±Ìï† Ïàò ÏûàÎèÑÎ°ù ÎèïÎäîÎã§. LLMÏùÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏÉÅÎãπ Î∂ÄÎ∂ÑÏùÑ Í∏∞ÏñµÌïòÎäî Í≤ΩÌñ•Ïù¥ ÏûàÏúºÎ©∞, Î™®Îç∏Ïùò ÌÅ¨Í∏∞Í∞Ä Ïª§ÏßàÏàòÎ°ù Ïù¥Îü¨Ìïú Í≤ΩÌñ•Ïù¥ Ï¶ùÍ∞ÄÌïòÏßÄÎßå, Ïù¥Îäî ÏßÄÏãù ÏùºÎ∞òÌôî Îä•Î†•Ïóê Ìï¥Î•º ÎÅºÏπòÏßÄ ÏïäÎäîÎã§. ÎòêÌïú, Ï§ëÎ≥µ Ï†úÍ±∞Îêú Îç∞Ïù¥ÌÑ∞ÏôÄ Îçî ÌÅ∞ Î∞∞Ïπò ÌÅ¨Í∏∞Î°ú ÌõàÎ†®ÌïòÎ©¥ ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÏäµÎìùÏù¥ Ìñ•ÏÉÅÎêòÏñ¥ ÌïôÏäµÌïú ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùÑ ÏûäÏñ¥Î≤ÑÎ¶¨Îäî Í≤ΩÌñ•Ïù¥ Ï§ÑÏñ¥Îì†Îã§. Ïù¥Îü¨Ìïú Í≤∞Í≥ºÎäî LLMÏùò ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ ÌíàÏßàÏùÑ ÎÜíÏù¥Í≥†, Î™®Îç∏Ïùò Ï†ÑÎ∞òÏ†ÅÏù∏ ÏÑ±Îä•ÏùÑ Í∞úÏÑ†ÌïòÎäî Îç∞ Í∏∞Ïó¨ÌïúÎã§.', 'Summary_QnA': 'Ïù∏Ïö© ÎÖºÎ¨∏ Ï†úÎ™© (Title): <<Deduplicating training data makes language models better>>\n\nÌï¥Îãπ ÎÖºÎ¨∏ÏùÄ LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Î©îÏª§ÎãàÏ¶òÏùÑ ÌÉêÍµ¨ÌïòÎ©∞, Ï§ëÎ≥µ Ï†úÍ±∞ÏôÄ Î∞∞Ïπò ÌÅ¨Í∏∞Ïùò Ï§ëÏöîÏÑ±ÏùÑ Í∞ïÏ°∞ÌïúÎã§. Ï§ëÎ≥µ Ï†úÍ±∞Îäî Î™®Îç∏Ïù¥ Ï§ëÎ≥µÎêú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞ÏñµÌïòÎäî ÎπàÎèÑÎ•º Ï§ÑÏó¨ ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Í≥†, Îçî Ï†ÅÏùÄ ÌõàÎ†® Îã®Í≥ÑÎ°úÎèÑ ÏÑ±Îä•ÏùÑ Í∞úÏÑ†Ìï† Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï§ÄÎã§. ÎòêÌïú, Î™®Îç∏ ÌÅ¨Í∏∞Í∞Ä Ïª§ÏßàÏàòÎ°ù ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Î•º Í∏∞ÏñµÌïòÎäî Í≤ΩÌñ•Ïù¥ Ï¶ùÍ∞ÄÌïòÏßÄÎßå, Ïù¥Îäî ÏßÄÏãù ÏùºÎ∞òÌôîÏóê Î∂ÄÏ†ïÏ†Å ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÏßÄ ÏïäÏùåÏùÑ ÌôïÏù∏ÌïòÏòÄÎã§. Ïù¥Îü¨Ìïú Î∞úÍ≤¨ÏùÄ LLMÏùò ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ ÌíàÏßàÏùÑ ÎÜíÏù¥Í≥†, ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò ÏäµÎìùÏùÑ Í∞ïÌôîÌïòÏó¨ ÏûäÏñ¥Î≤ÑÎ¶ºÏùÑ Ï§ÑÏù¥Îäî Îç∞ Í∏∞Ïó¨ÌïúÎã§.'}, '23': {'Title': 'Training compute-optimal large language models', 'Authors': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre', 'Counter': 9, 'Context': ['Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].', 'We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.', '[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.', 'In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].', 'In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.', 'While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in ¬ß4.3.'], 'abstract': "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.", 'pdf_url': 'http://arxiv.org/pdf/2312.12391v2', 'Questions': '1. Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].\n2. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time.\n3. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].\n4. In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.', 'Summary': 'Ïù∏Ïö© ÎÖºÎ¨∏ Ï†úÎ™© (Title): Training compute-optimal large language models\n\n1. **ÏßàÎ¨∏ :** ÏµúÍ∑º LLMÏóê ÎåÄÌïú Í¥ÄÏã¨Ïù¥ Í∏âÏ¶ùÌïòÍ≥† ÏûàÎã§. \n   - **ÎãµÎ≥Ä :** ÏµúÍ∑º Î™á ÎÖÑÍ∞Ñ ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïùò Î∞úÏ†ÑÍ≥º ÌôúÏö©Ïù¥ Í∏âÏ¶ùÌïòÎ©¥ÏÑú, AI Ïª§ÎÆ§ÎãàÌã∞Îäî Ïù¥Îü¨Ìïú Î™®Îç∏ÏùÑ ÎπÑÏö© Ìö®Ïú®Ï†ÅÏúºÎ°ú ÌõàÎ†®ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Í¥ÄÏã¨Ïù¥ ÎÜíÏïÑÏßÄÍ≥† ÏûàÎã§. LLMÏùÄ ÏàòÎ∞±Ïñµ Í∞úÏùò Îß§Í∞úÎ≥ÄÏàòÎ•º Í∞ÄÏßÄÍ≥† ÏûàÏúºÎ©∞, ÌõàÎ†®Ïóê ÏàòÏ°∞ Í∞úÏùò ÌÜ†ÌÅ∞Ïù¥ ÌïÑÏöîÌïòÎã§. Ïù¥Îü¨Ìïú ÎåÄÍ∑úÎ™® Î™®Îç∏Ïùò ÌõàÎ†®ÏùÄ ÎßâÎåÄÌïú Ïª¥Ìì®ÌåÖ ÏûêÏõêÏùÑ ÏöîÍµ¨ÌïòÎ©∞, Ïù¥Îäî ÏàòÎ∞±Îßå Îã¨Îü¨Ïùò ÎπÑÏö©ÏùÑ Ï¥àÎûòÌï† Ïàò ÏûàÎã§.\n   - **Í∑ºÍ±∞ :** "Unfortunately, a critical challenge that the AI community is facing is how to train these LLMs in a cost-effective manner."\n\n2. **ÏßàÎ¨∏ :** Ïö∞Î¶¨Îäî ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞ tw = 50ÏùÑ ÏÇ¨Ïö©ÌïúÎã§. Îã§ÏùåÏúºÎ°ú, Î™®Îç∏Ïùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãùÏùò Î°úÍ∑∏ ÌôïÎ•†ÏóêÏÑú Ï¶âÍ∞ÅÏ†ÅÏù∏ Í∞úÏÑ†ÏùÑ Ï†ïÎüâÌôîÌïòÍ∏∞ ÏúÑÌïú Î©îÌä∏Î¶≠ÏùÑ Ï†ïÏùòÌïúÎã§.\n   - **ÎãµÎ≥Ä :** ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞ tw = 50ÏùÑ ÏÑ§Ï†ïÌïòÍ≥†, Î™®Îç∏Ïù¥ ÌäπÏ†ï ÏßÄÏãùÏùÑ iÎ≤àÏß∏Î°ú Ï†úÏãúÎ∞õÏùÄ ÌõÑÏùò Î°úÍ∑∏ ÌôïÎ•† Í∞úÏÑ†ÏùÑ Ï∏°Ï†ïÌïòÎäî Î©îÌä∏Î¶≠ÏùÑ Ï†ïÏùòÌï®ÏúºÎ°úÏç®, Î™®Îç∏Ïùò ÏßÄÏãù ÏäµÎìù Îä•Î†•ÏùÑ ÌèâÍ∞ÄÌï† Ïàò ÏûàÎã§. Ïù¥Îäî Î™®Îç∏Ïù¥ ÏßÄÏãùÏùÑ ÏñºÎßàÎÇò Ïûò ÌÜµÌï©ÌïòÍ≥† ÌôúÏö©ÌïòÎäîÏßÄÎ•º ÎÇòÌÉÄÎÇ¥Îäî Ï§ëÏöîÌïú ÏßÄÌëúÍ∞Ä ÎêúÎã§.\n   - **Í∑ºÍ±∞ :** "Next, we define a metric to quantify the immediate improvement in the model‚Äôs log probability of factual knowledge after it is presented with the knowledge for the i-th time."\n\n3. **ÏßàÎ¨∏ :** LLMÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏßÄÏãùÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ï°∞ÏûëÌï† Ïàò ÏóÜÎã§.\n   - **ÎãµÎ≥Ä :** LLMÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏñªÏùÄ ÏßÄÏãùÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÌôúÏö©ÌïòÎäî Îç∞ ÌïúÍ≥ÑÍ∞Ä ÏûàÎã§. Ïù¥Îäî Î™®Îç∏Ïù¥ ÌõàÎ†® Ï§ëÏóê ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥Î•º ÌÜµÌï©ÌïòÎäî Îç∞ Ïñ¥Î†§ÏõÄÏùÑ Í≤™ÎäîÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏ÌïòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò ÏÑ±Îä•Ïóê Î∂ÄÏ†ïÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏Ïπ† Ïàò ÏûàÎã§.\n   - **Í∑ºÍ±∞ :** "In addition, LLMs cannot manipulate knowledge from pretraining data effectively."\n\n4. **ÏßàÎ¨∏ :** Eq.1ÏóêÏÑú ÏßÄÏó≠ ÏäµÎìù ÏµúÎåÄÍ∞íÏùò Ï†ïÏùòÎäî Ï£ºÏûÖÎêú ÏßÄÏãù kÏôÄ ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞ twÏóê ÏùòÏ°¥ÌïòÏßÄÎßå, Ïö∞Î¶¨Îäî Í∞ÑÍ≤∞Ìï®ÏùÑ ÏúÑÌï¥ tLAM(q, i)Î°ú ÏûëÏÑ±ÌïúÎã§.\n   - **ÎãµÎ≥Ä :** ÏßÄÏó≠ ÏäµÎìù ÏµúÎåÄÍ∞í(tLAM)ÏùÄ Ï£ºÏûÖÎêú ÏßÄÏãùÍ≥º ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞Ïóê Îî∞Îùº Îã¨ÎùºÏßÄÎ©∞, Ïù¥Îäî Î™®Îç∏Ïù¥ ÌäπÏ†ï ÏßÄÏãùÏùÑ ÏñºÎßàÎÇò Ïûò ÏäµÎìùÌïòÍ≥† ÏûàÎäîÏßÄÎ•º ÎÇòÌÉÄÎÇ¥Îäî Ï§ëÏöîÌïú ÏöîÏÜåÏù¥Îã§. Ïù¥Îü¨Ìïú Ï†ïÏùòÎäî Î™®Îç∏Ïùò ÌïôÏäµ Í≥ºÏ†ïÏóêÏÑú ÏßÄÏãùÏùò ÌÜµÌï©ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞ Ïú†Ïö©ÌïòÎã§.\n   - **Í∑ºÍ±∞ :** "the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity."\n\n### ÎãµÎ≥Ä ÏöîÏïΩ (Summary of Answers)\nÏµúÍ∑º LLMÏóê ÎåÄÌïú Í¥ÄÏã¨Ïù¥ Í∏âÏ¶ùÌïòÍ≥† ÏûàÏúºÎ©∞, Ïù¥Îäî ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏Ïùò Î∞úÏ†ÑÍ≥º ÌôúÏö©Ïù¥ Ï¶ùÍ∞ÄÌï®Ïóê Îî∞Îùº AI Ïª§ÎÆ§ÎãàÌã∞Í∞Ä Ïù¥Îü¨Ìïú Î™®Îç∏ÏùÑ ÎπÑÏö© Ìö®Ïú®Ï†ÅÏúºÎ°ú ÌõàÎ†®ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú ÌïÑÏöîÏÑ±ÏùÑ ÎäêÎÅºÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. LLMÏùÄ ÏàòÎ∞±Ïñµ Í∞úÏùò Îß§Í∞úÎ≥ÄÏàòÎ•º Í∞ÄÏßÄÍ≥† ÏûàÏúºÎ©∞, ÌõàÎ†®Ïóê ÏàòÏ°∞ Í∞úÏùò ÌÜ†ÌÅ∞Ïù¥ ÌïÑÏöîÌïòÏó¨ ÎßâÎåÄÌïú Ïª¥Ìì®ÌåÖ ÏûêÏõêÏùÑ ÏöîÍµ¨ÌïúÎã§. ÎòêÌïú, LLMÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏßÄÏãùÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ï°∞ÏûëÌïòÎäî Îç∞ ÌïúÍ≥ÑÍ∞Ä ÏûàÏúºÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò ÏÑ±Îä•Ïóê Î∂ÄÏ†ïÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏Ïπ† Ïàò ÏûàÎã§. Ïó∞Íµ¨ÏóêÏÑúÎäî ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞ÏôÄ ÏßÄÏãù ÏäµÎìù Î©îÌä∏Î¶≠ÏùÑ Ï†ïÏùòÌïòÏó¨ Î™®Îç∏Ïùò ÏßÄÏãù ÌÜµÌï© Îä•Î†•ÏùÑ ÌèâÍ∞ÄÌïòÍ≥†, ÏßÄÏó≠ ÏäµÎìù ÏµúÎåÄÍ∞íÏùÑ ÌÜµÌï¥ Î™®Îç∏Ïùò ÌïôÏäµ Í≥ºÏ†ïÏùÑ Î∂ÑÏÑùÌïòÎäî Î∞©Î≤ïÏùÑ Ï†úÏãúÌïòÍ≥† ÏûàÎã§. Ïù¥Îü¨Ìïú ÏöîÏÜåÎì§ÏùÄ LLMÏùò ÌõàÎ†® Î∞è ÏÑ±Îä• ÏµúÏ†ÅÌôîÏóê Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÌïúÎã§.', 'Summary_QnA': 'Ïù∏Ïö© ÎÖºÎ¨∏ "Training compute-optimal large language models"ÏóêÏÑú Ï†úÍ∏∞Îêú ÏßàÎ¨∏Îì§ÏùÄ LLMÏùò ÌõàÎ†® Î∞è ÏßÄÏãù ÏäµÎìù Í≥ºÏ†ïÏóê ÎåÄÌïú Ï§ëÏöîÌïú ÌÜµÏ∞∞ÏùÑ Ï†úÍ≥µÌïòÎ©∞, Ïù¥Îäî Î≥∏ ÎÖºÎ¨∏ÏóêÏÑú Îã§Î£¨ LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Î©îÏª§ÎãàÏ¶òÏùÑ Î∞úÏ†ÑÏãúÌÇ§Îäî Îç∞ Í∏∞Ïó¨ÌïúÎã§. ÌäπÌûà, LLMÏù¥ ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏßÄÏãùÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ï°∞ÏûëÌïòÏßÄ Î™ªÌïúÎã§Îäî Ï†êÏùÄ Î≥∏ Ïó∞Íµ¨Ïùò Î∞úÍ≤¨Í≥º ÏùºÏπòÌïòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò ÏÑ±Îä• Ï†ÄÌïòÏôÄ Í¥ÄÎ†®Ïù¥ ÏûàÎã§. ÎòêÌïú, ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞ÏôÄ ÏßÄÏãù ÏäµÎìù Î©îÌä∏Î¶≠ÏùÑ Ï†ïÏùòÌïòÎäî Ï†ëÍ∑ºÏùÄ LLMÏùò ÏßÄÏãù ÌÜµÌï© Îä•Î†•ÏùÑ Ï†ïÎüâÏ†ÅÏúºÎ°ú ÌèâÍ∞ÄÌïòÎäî Îç∞ Ïú†Ïö©ÌïòÎ©∞, Ïù¥Îäî Î≥∏ ÎÖºÎ¨∏ÏóêÏÑú Ï†úÏãúÌïú ÌõàÎ†® Îã®Í≥ÑÏôÄ Í∏∞Ïñµ ÏÉÅÏã§ Í∞ÑÏùò Í¥ÄÍ≥ÑÎ•º Ïù¥Ìï¥ÌïòÎäî Îç∞ ÎèÑÏõÄÏùÑ Ï§ÄÎã§. Ïù¥Îü¨Ìïú ÏöîÏÜåÎì§ÏùÄ LLMÏùò ÌõàÎ†® ÏµúÏ†ÅÌôî Î∞è ÏÑ±Îä• Ìñ•ÏÉÅÏóê Ï§ëÏöîÌïú Í∏∞Ï¥àÎ•º Ï†úÍ≥µÌïúÎã§.'}, '46': {'Title': 'Memorization without overfitting: Analyzing the training dynamics of large language models', 'Authors': 'Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan', 'Counter': 7, 'Context': ['Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].', 'Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.', '[44] and [46] focused on the dynamics of memorization in language model pretraining.', 'The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].'], 'abstract': 'Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.', 'pdf_url': 'http://arxiv.org/pdf/2205.10770v2', 'Questions': '1. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.\n2. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].\n3. [44] and [46] focused on the dynamics of memorization in language model pretraining.', 'Summary': 'Ïù∏Ïö© ÎÖºÎ¨∏ Ï†úÎ™© (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n\n1. **ÏßàÎ¨∏ :** [46]ÏùÄ Îã§ÏñëÌïú ÏÇ¨Ï†Ñ ÌõàÎ†® Ï°∞Í±¥ÏóêÏÑú LLMÏùò ÏïîÍ∏∞ Î∞è ÎßùÍ∞Å ÌñâÎèôÏóê ÎåÄÌïú Í¥ëÎ≤îÏúÑÌïú Î∂ÑÏÑùÏùÑ ÏàòÌñâÌñàÏäµÎãàÎã§.\n   - **ÎãµÎ≥Ä :** [46]Ïùò Ïó∞Íµ¨Îäî ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïùò ÏïîÍ∏∞ Î∞è ÎßùÍ∞Å ÌñâÎèôÏùÑ Îã§ÏñëÌïú ÏÇ¨Ï†Ñ ÌõàÎ†® Ï°∞Í±¥ÏóêÏÑú Î∂ÑÏÑùÌïòÏòÄÏúºÎ©∞, Ïù¥ Ïó∞Íµ¨Îäî Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Í≥ºÏ†ï Ï†ÑÎ∞òÏóê Í±∏Ï≥ê ÏïîÍ∏∞ ÎèôÏó≠ÌïôÏùÑ Ïã§Ï¶ùÏ†ÅÏúºÎ°ú Ïó∞Íµ¨ÌñàÏäµÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process."\n\n2. **ÏßàÎ¨∏ :** LLM ÌõàÎ†®Ïùò Ïó¨Îü¨ Ï∏°Î©¥ÏóêÏÑú ÎßùÍ∞ÅÏùò Í∏∞ÌïòÍ∏âÏàòÏ†Å Í≤ΩÌñ•Ïù¥ Î≥¥Í≥†ÎêòÏóàÏäµÎãàÎã§. Ïó¨Í∏∞ÏóêÎäî ÏÇ¨Ï†Ñ ÌõàÎ†®ÏóêÏÑúÏùò ÏïîÍ∏∞ Î∞è ÏßÄÏÜçÏ†Å ÌïôÏäµÏóêÏÑúÏùò ÏûëÏóÖ ÏÑ±Îä•Ïù¥ Ìè¨Ìï®Îê©ÎãàÎã§.\n   - **ÎãµÎ≥Ä :** LLM ÌõàÎ†®ÏóêÏÑú ÎßùÍ∞ÅÏùò Í∏∞ÌïòÍ∏âÏàòÏ†Å Í≤ΩÌñ•ÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Ï§ë ÏïîÍ∏∞ÏôÄ ÏßÄÏÜçÏ†Å ÌïôÏäµÏóêÏÑúÏùò ÏûëÏóÖ ÏÑ±Îä• Î™®ÎëêÏóêÏÑú Í¥ÄÏ∞∞ÎêòÏóàÏúºÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏñëÏóê Îî∞Îùº Îã¨ÎùºÏßëÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "Our empirical studies show that forgetting curves have lower bounds ‚Äî we coin this as the forgetting baseline ‚Äî and that this baseline increases with model scale."\n\n3. **ÏßàÎ¨∏ :** [44]ÏôÄ [46]ÏùÄ Ïñ∏Ïñ¥ Î™®Îç∏ ÏÇ¨Ï†Ñ ÌõàÎ†®ÏóêÏÑúÏùò ÏïîÍ∏∞ ÎèôÏó≠ÌïôÏóê Ï¥àÏ†êÏùÑ ÎßûÏ∑ÑÏäµÎãàÎã§.\n   - **ÎãµÎ≥Ä :** [44]ÏôÄ [46]Ïùò Ïó∞Íµ¨Îäî Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÏÇ¨Ï†Ñ ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑú ÏïîÍ∏∞ ÎèôÏó≠ÌïôÏùÑ Î∂ÑÏÑùÌïòÏòÄÏúºÎ©∞, Ïù¥Îì§ÏùÄ Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÌäπÏÑ±Ïù¥ ÏïîÍ∏∞ ÏÜçÎèÑÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Ï°∞ÏÇ¨ÌñàÏäµÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "We measure the dependence of memorization dynamics over training on model size (and other factors such as dataset size, overfitting, and learning rate)."\n\nÎãµÎ≥Ä ÏöîÏïΩ \n: Ïù¥ ÎÖºÎ¨∏ÏùÄ ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïùò ÏïîÍ∏∞ Î∞è ÎßùÍ∞Å ÎèôÏó≠ÌïôÏùÑ Î∂ÑÏÑùÌïòÎ©∞, ÌäπÌûà Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Ï°∞Í±¥Ïù¥ ÏïîÍ∏∞ ÏÜçÎèÑÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Ïã§Ï¶ùÏ†ÅÏúºÎ°ú Ïó∞Íµ¨Ìï©ÎãàÎã§. [46]Ïùò Ïó∞Íµ¨Îäî Îã§ÏñëÌïú ÏÇ¨Ï†Ñ ÌõàÎ†® Ï°∞Í±¥ÏóêÏÑú LLMÏùò ÏïîÍ∏∞ Î∞è ÎßùÍ∞Å ÌñâÎèôÏùÑ Î∂ÑÏÑùÌïòÏòÄÍ≥†, LLM ÌõàÎ†®ÏóêÏÑúÏùò ÎßùÍ∞Å Í≤ΩÌñ•ÏùÄ ÏÇ¨Ï†Ñ ÌõàÎ†® Ï§ë ÏïîÍ∏∞ÏôÄ ÏßÄÏÜçÏ†Å ÌïôÏäµÏóêÏÑúÏùò ÏûëÏóÖ ÏÑ±Îä• Î™®ÎëêÏóêÏÑú Í∏∞ÌïòÍ∏âÏàòÏ†ÅÏúºÎ°ú ÎÇòÌÉÄÎÇ¨ÏäµÎãàÎã§. ÎòêÌïú, [44]ÏôÄ [46]ÏùÄ Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÏÇ¨Ï†Ñ ÌõàÎ†®ÏóêÏÑúÏùò ÏïîÍ∏∞ ÎèôÏó≠ÌïôÏóê Ï¥àÏ†êÏùÑ ÎßûÏ∂îÏñ¥, Î™®Îç∏ ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÌäπÏÑ±Ïù¥ ÏïîÍ∏∞ ÏÜçÎèÑÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Ï°∞ÏÇ¨ÌñàÏäµÎãàÎã§. Ïù¥ Ïó∞Íµ¨Îäî LLMÏùò ÌõàÎ†® ÎèôÏó≠ÌïôÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ Ï§ëÏöîÌïú Í∏∞Ïó¨Î•º ÌïòÍ≥† ÏûàÏäµÎãàÎã§.', 'Summary_QnA': 'Ïù∏Ïö© ÎÖºÎ¨∏ Ï†úÎ™© (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n\nÌï¥Îãπ ÎÖºÎ¨∏ÏùÄ ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïùò ÏïîÍ∏∞ Î∞è ÎßùÍ∞Å ÎèôÏó≠ÌïôÏùÑ Ïã¨Ï∏µÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÏó¨, Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Ï°∞Í±¥Ïù¥ ÏïîÍ∏∞ ÏÜçÎèÑÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Ïã§Ï¶ùÏ†ÅÏúºÎ°ú Ïó∞Íµ¨ÌñàÏäµÎãàÎã§. [46]Ïùò Ïó∞Íµ¨ Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú, LLMÏùò ÌõàÎ†®ÏóêÏÑú ÎßùÍ∞ÅÏùò Í∏∞ÌïòÍ∏âÏàòÏ†Å Í≤ΩÌñ•Ïù¥ ÏÇ¨Ï†Ñ ÌõàÎ†® Ï§ë ÏïîÍ∏∞ÏôÄ ÏßÄÏÜçÏ†Å ÌïôÏäµ ÏÑ±Îä• Î™®ÎëêÏóêÏÑú Í¥ÄÏ∞∞ÎêòÏóàÏùåÏùÑ Í∞ïÏ°∞ÌïòÎ©∞, Ïù¥Îäî Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÏôÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏñëÏóê Îî∞Îùº Îã¨ÎùºÏßÑÎã§Í≥† ÏÑ§Î™ÖÌï©ÎãàÎã§. Ïù¥Îü¨Ìïú ÌÜµÏ∞∞ÏùÄ LLMÏùò ÌõàÎ†® ÎèôÏó≠ÌïôÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ Ï§ëÏöîÌïú Í∏∞Ïó¨Î•º ÌïòÎ©∞, ÌäπÌûà ÏïîÍ∏∞ÏôÄ ÎßùÍ∞ÅÏùò Í¥ÄÍ≥ÑÎ•º Î™ÖÌôïÌûà ÌïòÎäî Îç∞ ÎèÑÏõÄÏùÑ Ï§çÎãàÎã§.'}, '41': {'Title': 'Are emergent abilities of large language models a mirage?', 'Authors': 'Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo', 'Counter': 6, 'Context': ['To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.', '...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].', 'To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information [41].', '...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].'], 'abstract': "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.", 'pdf_url': 'http://arxiv.org/pdf/2304.15004v2', 'Questions': '1. To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information.\n2. ...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].\n3. ...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].', 'Summary': 'Ïù∏Ïö© ÎÖºÎ¨∏ Ï†úÎ™© (Title): <<Are emergent abilities of large language models a mirage?>>\n\n1. **ÏßàÎ¨∏ :** LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìùÏùÑ Î∂ÑÏÑùÌïòÍ∏∞ ÏúÑÌï¥, Ïö∞Î¶¨Îäî Î°úÍ∑∏ ÌôïÎ•†ÏùÑ Í≤ÄÌÜ†ÌïòÏó¨ Î™®Îç∏Ïùò ÏÉÅÌÉúÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§. \n   - **ÎãµÎ≥Ä :** Ïù¥ Ïó∞Íµ¨ÏóêÏÑúÎäî LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìùÏùÑ Î∂ÑÏÑùÌïòÍ∏∞ ÏúÑÌï¥ Î°úÍ∑∏ ÌôïÎ•†ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏Ïùò ÏÉÅÌÉúÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§. Î°úÍ∑∏ ÌôïÎ•†ÏùÑ ÌÜµÌï¥ Î™®Îç∏Ïùò Ï∂úÎ†•ÏóêÏÑú ÏÑ∏Î∂ÄÏ†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏñªÏùÑ Ïàò ÏûàÏúºÎ©∞, Ïù¥Îäî Î™®Îç∏Ïù¥ ÌäπÏ†ï ÏûëÏóÖÏóêÏÑú ÏñºÎßàÎÇò Ïûò ÏàòÌñâÌïòÎäîÏßÄÎ•º Ïù¥Ìï¥ÌïòÎäî Îç∞ Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "To conduct a detailed analysis of the LLMs‚Äô acquisition of factual knowledge during pretraining, we evaluate the model‚Äôs state by examining log probabilities to obtain fine-grained information."\n\n2. **ÏßàÎ¨∏ :** Ïù¥Îü¨Ìïú ÏßÄÏãùÏùò ÏäµÎìùÏùÄ Î™®Îç∏Ïùò ÏÉÅÏúÑ k Ï∂úÎ†• ÏãúÌÄÄÏä§ ÏÉùÏÑ±ÏóêÏÑú ÏÉÅÎåÄÏ†ÅÏúºÎ°ú Ï¥àÍ∏∞ ÌîÑÎ¶¨Ìä∏Î†àÏù¥Îãù Îã®Í≥ÑÏóê Î∞òÏòÅÎê† Í≤ÉÏûÖÎãàÎã§.\n   - **ÎãµÎ≥Ä :** Ïó∞Íµ¨ Í≤∞Í≥ºÏóê Îî∞Î•¥Î©¥, LLMÏùò ÏßÄÏãù ÏäµÎìùÏùÄ Ï¥àÍ∏∞ ÌîÑÎ¶¨Ìä∏Î†àÏù¥Îãù Îã®Í≥ÑÏóêÏÑú Î™®Îç∏Ïùò ÏÉÅÏúÑ k Ï∂úÎ†• ÏãúÌÄÄÏä§ ÏÉùÏÑ±Ïóê Î∞òÏòÅÎê©ÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïù¥ ÌäπÏ†ï ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÎäî Îç∞ ÌïÑÏöîÌïú ÏßÄÏãùÏùÑ Îπ†Î•¥Í≤å ÏäµÎìùÌï† Ïàò ÏûàÏùåÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "...the acquisition of such knowledge will be reflected in the model‚Äôs top-k output sequence generation in a relatively earlier pretraining stage..."\n\n3. **ÏßàÎ¨∏ :** ÏßÄÏãùÏùò ÎàÑÏ†Å Î°úÍ∑∏ ÌôïÎ•†ÏùÄ Î™®Îç∏Ïùò ÎîîÏΩîÎî© Ï∂úÎ†•ÏúºÎ°ú ÏßÄÏãùÏùÑ ÏÉùÏÑ±ÌïòÍ∏∞Ïóê Ï∂©Î∂ÑÌûà ÎÜíÏùÑ Í≤ÉÏûÖÎãàÎã§.\n   - **ÎãµÎ≥Ä :** Î™®Îç∏Ïùò ÏßÄÏãù ÎàÑÏ†Å Î°úÍ∑∏ ÌôïÎ•†Ïù¥ Ï∂©Î∂ÑÌûà ÎÜíÏïÑÏßÄÎ©¥, Ïù¥Îäî Î™®Îç∏Ïùò ÎîîÏΩîÎî© Ï∂úÎ†•ÏóêÏÑú ÏßÄÏãùÏùÑ ÏÉùÏÑ±ÌïòÎäî Îç∞ Í∏∞Ïó¨Ìï©ÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïù¥ ÌïôÏäµÌïú ÏßÄÏãùÏù¥ Ïã§Ï†ú Ï∂úÎ†•Ïóê Î∞òÏòÅÎê† Ïàò ÏûàÏùåÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.\n   - **Í∑ºÍ±∞ :** "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model..."\n\nÎãµÎ≥Ä ÏöîÏïΩ \n: Ïù¥ Ïó∞Íµ¨Îäî LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìùÏùÑ Î°úÍ∑∏ ÌôïÎ•†ÏùÑ ÌÜµÌï¥ Î∂ÑÏÑùÌïòÎ©∞, Ï¥àÍ∏∞ ÌîÑÎ¶¨Ìä∏Î†àÏù¥Îãù Îã®Í≥ÑÏóêÏÑú Î™®Îç∏Ïùò ÏÉÅÏúÑ k Ï∂úÎ†• ÏãúÌÄÄÏä§ ÏÉùÏÑ±Ïóê Î∞òÏòÅÎêúÎã§Í≥† Ï£ºÏû•Ìï©ÎãàÎã§. ÎòêÌïú, ÏßÄÏãùÏùò ÎàÑÏ†Å Î°úÍ∑∏ ÌôïÎ•†Ïù¥ Ï∂©Î∂ÑÌûà ÎÜíÏïÑÏßÄÎ©¥ Î™®Îç∏Ïùò ÎîîÏΩîÎî© Ï∂úÎ†•ÏóêÏÑú ÏßÄÏãùÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§. Ïù¥Îü¨Ìïú Í≤∞Í≥ºÎäî LLMÏù¥ ÌäπÏ†ï ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÎäî Îç∞ ÌïÑÏöîÌïú ÏßÄÏãùÏùÑ Îπ†Î•¥Í≤å ÏäµÎìùÌïòÍ≥† Ïù¥Î•º Ï∂úÎ†•Ïóê Î∞òÏòÅÌï† Ïàò ÏûàÏùåÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§. Ïó∞Íµ¨Îäî LLMÏùò ÏÑ±Îä•Ïù¥ Îã®ÏàúÌûà Î™®Îç∏Ïùò ÌÅ¨Í∏∞ÎÇò Î≥µÏû°ÏÑ±Ïóê ÏùòÌïú Í≤ÉÏù¥ ÏïÑÎãàÎùº, ÏÑ†ÌÉùÎêú Î©îÌä∏Î¶≠Ïóê ÏùòÌï¥ ÌÅ¨Í≤å ÏòÅÌñ•ÏùÑ Î∞õÏùÑ Ïàò ÏûàÏùåÏùÑ Í∞ïÏ°∞Ìï©ÎãàÎã§.', 'Summary_QnA': 'Ïù∏Ïö© ÎÖºÎ¨∏ <<Are emergent abilities of large language models a mirage?>>Ïùò Ï£ºÏöî ÏßàÏùò ÏùëÎãµÏùÑ ÌÜµÌï¥, Ìï¥Îãπ ÎÖºÎ¨∏ÏùÄ LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Í≥ºÏ†ïÏùÑ Î°úÍ∑∏ ÌôïÎ•† Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ Ïã¨Ï∏µÏ†ÅÏúºÎ°ú ÌÉêÍµ¨ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Ïó∞Íµ¨Îäî Ï¥àÍ∏∞ ÌîÑÎ¶¨Ìä∏Î†àÏù¥Îãù Îã®Í≥ÑÏóêÏÑú Î™®Îç∏Ïùò ÏÉÅÏúÑ k Ï∂úÎ†• ÏãúÌÄÄÏä§ ÏÉùÏÑ±Ïóê ÏßÄÏãù ÏäµÎìùÏù¥ Î∞òÏòÅÎêúÎã§Îäî Ï†êÏùÑ Í∞ïÏ°∞ÌïòÎ©∞, Ïù¥Îäî LLMÏù¥ ÌäπÏ†ï ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÎäî Îç∞ ÌïÑÏöîÌïú ÏßÄÏãùÏùÑ Ïã†ÏÜçÌïòÍ≤å ÏäµÎìùÌï† Ïàò ÏûàÏùåÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§. ÎòêÌïú, ÎàÑÏ†Å Î°úÍ∑∏ ÌôïÎ•†Ïù¥ Ï∂©Î∂ÑÌûà ÎÜíÏïÑÏßà Í≤ΩÏö∞, Î™®Îç∏Ïùò ÎîîÏΩîÎî© Ï∂úÎ†•ÏóêÏÑú ÏßÄÏãùÏù¥ ÏÉùÏÑ±Îê† Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§. Ïù¥Îü¨Ìïú Î∞úÍ≤¨ÏùÄ LLMÏùò ÏÑ±Îä•Ïù¥ Îã®ÏàúÌïú Î™®Îç∏ ÌÅ¨Í∏∞ÎÇò Î≥µÏû°ÏÑ±Ïóê ÏùòÌïú Í≤ÉÏù¥ ÏïÑÎãàÎùº, ÏÑ†ÌÉùÎêú Î©îÌä∏Î¶≠Ïóê ÏùòÌï¥ ÌÅ¨Í≤å ÏòÅÌñ•ÏùÑ Î∞õÏùÑ Ïàò ÏûàÏùåÏùÑ Í∞ïÏ°∞ÌïòÎ©∞, LLMÏùò ÏÇ¨Ïã§Ï†Å ÏßÄÏãù ÏäµÎìù Î©îÏª§ÎãàÏ¶òÏóê ÎåÄÌïú Ïù¥Ìï¥Î•º Ïã¨ÌôîÏãúÌÇµÎãàÎã§.'}} 

