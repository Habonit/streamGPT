{
    "107": {
        "Title": "Attention is All you Need",
        "Authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin",
        "Counter": 26,
        "Context": [
            "Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling...",
            "most existing architecturesâ€“ranging from Hopfield Networks (Hopfield 1982) to LSTMs (JÃ¼rgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)â€“face challenges when dealing with generalization, length extrapolation, and/or reasoning.",
            "For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step.",
            "Titans outperforms Transformers with the same context window, and show competitive performance with Transformers that use the entire context.",
            "Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on attention mechanism.",
            "...the attention can be written as: ğœ™ (ğ‘„ğ‘– )âŠ¤ğœ™ (ğ¾ğ‘— ) â„“=1 ğœ™ (ğ‘„ğ‘– )âŠ¤ğœ™ (ğ¾â„“ ) ğœ™ (ğ‘„ğ‘– )âŠ¤ (cid:205)ğ‘– ğœ™ (ğ‘„ğ‘– )âŠ¤ (cid:205)ğ‘– j=1 ğœ™ (ğ¾ğ‘— )ğ‘‰ğ‘— â„“=1 ğœ™ (ğ¾â„“ ).",
            "Given ğ‘¥ğ‘¡, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project ğ‘¥ğ‘¡ into a key and value:",
            "we also follow the recent architectures that use normalization and gating with a linear layer before the final output projection (Mehta et al. 2023).",
            "we compare with Transformer++ (Touvron et al. 2023)",
            "Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan et al. 2020).",
            "This, however, brings a contradictory fact about linear recurrent (or linear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs. quadratic complexity)",
            "most existing architecturesâ€“ranging from Hopfield Networks (Hopfield 1982) to LSTMs (JÃ¼rgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)â€“face challenges when dealing with generalization, length extrapolation, and/or reasoning.",
            "For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step...",
            "...Titans outperforms Transformers with the same context window...",
            "Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on attention mechanism.",
            "the softmax in standard attention is replaced with an alternative kernel function ğœ™ (., .)",
            "Given ğ‘¥ğ‘¡, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project ğ‘¥ğ‘¡ into a key and value:",
            "compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory.",
            "Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale.",
            "most existing architecturesâ€“ranging from Hopfield Networks (Hopfield 1982) to LSTMs (JÃ¼rgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)â€“face challenges when dealing with generalization, length extrapolation, and/or reasoning.",
            "For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step...",
            "Modern Linear Models and Their Memory Perspective. As discussed earlier, one can define learning as a process for acquiring effective and useful memory.",
            "Given ğ‘¥ğ‘¡, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project ğ‘¥ğ‘¡ into a key and value:",
            "Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "Titans outperform all models even extremely large models like GPT4."
        ],
        "abstract": "The inference demand for LLMs has skyrocketed in recent months, and serving\nmodels with low latencies remains challenging due to the quadratic input length\ncomplexity of the attention layers. In this work, we investigate the effect of\ndropping MLP and attention layers at inference time on the performance of\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\ndecreases performance but leads to the best speedups alongside dropping entire\nlayers. For example, removing 33\\% of attention layers in a 13B Llama2 model\nresults in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\nalso observe that skipping layers except the latter layers reduces performances\nfor more layers skipped, except for skipping the attention layers.",
        "pdf_url": "http://arxiv.org/pdf/2407.15516v1",
        "Questions": "1. Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan et al. 2020).\n2. Most existing architecturesâ€“ranging from Hopfield Networks (Hopfield 1982) to LSTMs (JÃ¼rgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)â€“face challenges when dealing with generalization, length extrapolation, and/or reasoning.\n3. For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step.\n4. Titans outperforms Transformers with the same context window, and show competitive performance with Transformers that use the entire context.\n5. We also follow the recent architectures that use normalization and gating with a linear layer before the final output projection (Mehta et al. 2023).\n6. Compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory.\n7. The softmax in standard attention is replaced with an alternative kernel function ğœ™ (., .).\n8. Modern Linear Models and Their Memory Perspective. As discussed earlier, one can define learning as a process for acquiring effective and useful memory.\n9. Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).\n10. Titans outperform all models even extremely large models like GPT4.",
        "Summary": "\"Attention is All You Need\" ë…¼ë¬¸ì€ Transformer ì•„í‚¤í…ì²˜ì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ íƒêµ¬í•˜ë©°, íŠ¹íˆ Llama-v2 ëª¨ë¸ì—ì„œ MLPì™€ attention ë ˆì´ì–´ë¥¼ ì œê±°í•˜ëŠ” ì‹¤í—˜ì„ í†µí•´ ì„±ëŠ¥ ì €í•˜ê°€ ë¯¸ë¯¸í•˜ë©´ì„œë„ ì²˜ë¦¬ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¡´ì˜ Transformer ëª¨ë¸ë“¤ì€ ì¼ë°˜í™”, ê¸¸ì´ ì™¸ì‚½, ì¶”ë¡ ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªëŠ” ë°˜ë©´, Titansì™€ ê°™ì€ ìƒˆë¡œìš´ ëª¨ë¸ë“¤ì€ ê°•ë ¥í•œ ë©”ëª¨ë¦¬ êµ¬ì¡° ë•ë¶„ì— ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ë©° ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì—°êµ¬ì— ë”°ë¥´ë©´, attention ë ˆì´ì–´ë¥¼ ìƒëµí•˜ëŠ” ê²ƒì´ MLP ë ˆì´ì–´ë¥¼ ìƒëµí•˜ëŠ” ê²ƒë³´ë‹¤ ì„±ëŠ¥ ì €í•˜ê°€ ì ê³ , ì „ë°˜ì ìœ¼ë¡œ ë” ë‚˜ì€ ì†ë„ ê°œì„ ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ë˜í•œ, TitansëŠ” GPT-4ì™€ ê°™ì€ ëŒ€í˜• ëª¨ë¸ë“¤ë³´ë‹¤ë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì´ëŠ” ë©”ëª¨ë¦¬ ê´€ë¦¬ì˜ íš¨ìœ¨ì„± ë•ë¶„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” Transformer ì•„í‚¤í…ì²˜ì˜ êµ¬ì¡°ì  ë³€í™”ì™€ ë ˆì´ì–´ ìŠ¤í‚µí•‘ ê¸°ë²•ì´ LLMì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤."
    },
    "40": {
        "Title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "Authors": "Albert Gu and Tri Dao",
        "Counter": 17,
        "Context": [
            "As examples of such models, we refer to Mamba2 (Dao and Gu 2024)",
            "we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024)",
            "we compare with Mamba-based (Behrouz, Santacatterina, and Zabih 2024)",
            "While some baselines also take advantage of gating mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows the importance of both our surprise mechanism and having deep and non-linear memory.",
            "Compared to Mamba2, which has the gating (forgetting) mechanism, Titans have deep non-linear memory, resulting in better memory management.",
            "Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024)",
            "To enhance the parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast paralellizable algorithm.",
            "...this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).",
            "we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also computationally efficient.",
            "...Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024)...",
            "we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024)",
            "To better handle the limited memory, we present a decaying mechanism that consider the proportion of memory size and the amount of data surprise, resulting in better memory management.",
            "In this case, we are using the same value for each of ğ›¼, ğœƒ, and ğœ‚ in each chunk. Accordingly, in Equation 17, we can store Î˜ using a single scaler.",
            "we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also computationally efficient.",
            "Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, Yury Kuratov, and Burtsev 2022)"
        ],
        "abstract": "Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.",
        "pdf_url": "http://arxiv.org/pdf/2312.00752v2",
        "Questions": "1. As examples of such models, we refer to Mamba2 (Dao and Gu 2024).\n2. We incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).\n3. While some baselines also take advantage of gating mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows the importance of both our surprise mechanism and having deep and non-linear memory.\n4. Compared to Mamba2, which has the gating (forgetting) mechanism, Titans have deep non-linear memory, resulting in better memory management.\n5. To enhance the parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast parallelizable algorithm.\n6. This equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).\n7. To better handle the limited memory, we present a decaying mechanism that considers the proportion of memory size and the amount of data surprise, resulting in better memory management.\n8. In this case, we are using the same value for each of ğ›¼, ğœƒ, and ğœ‚ in each chunk. Accordingly, in Equation 17, we can store Î˜ using a single scaler.",
        "Summary": "\"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" introduces a new class of models designed to efficiently handle long sequences while maintaining high performance across various modalities, including language, audio, and genomics. The Mamba architecture incorporates selective state space models (SSMs) with an innovative selection mechanism that allows for context-dependent reasoning, outperforming traditional models like Transformers in both speed and accuracy. Key features include the integration of depthwise-separable convolutions and a memory management strategy that leverages a decaying mechanism to optimize memory usage. Comparisons with models such as Mamba2 reveal that Mamba's non-linear memory capabilities lead to superior performance, particularly in tasks requiring long-context processing. The findings highlight Mamba's potential as a robust backbone for foundation models in diverse applications."
    },
    "1": {
        "Title": "Gpt-4 technical report",
        "Authors": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.",
        "Counter": 15,
        "Context": [
            "we argue that in an effective learning paradigm, similar to human brain, there are distinct yet interconnected modules, each of which is responsible for a component crucial to the learning process.",
            "we compare with GPT4 (Achiam et al. 2023)",
            "extremely large models such as GPT-4 (Achiam et al. 2023)",
            "Interestingly, Titans (MAL) are faster than baselines as well as the memory module.",
            "we argue that in an effective learning paradigm, similar to human brain, there are distinct yet interconnected modules, each of which is responsible for a component crucial to the learning process.",
            "Memory has always been one of the core parts of the neural network designs (Graves, Wayne, and Danihelka 2014; JH Schmidhuber 1992; JÃ¼rgen Schmidhuber and Hochreiter 1997; J. Zhang et al. 2024).",
            "Note that, similar to meta-learning models (Nichol 2018; Zintgraf et al. 2019), training of the memory is in the inner-loop, and so parameters ğ‘Šğ¾ and ğ‘Šğ‘‰ are hyperparameters in the above loss function.",
            "Attention with causal mask has implicit bias toward initial tokens in the sequence, and so attention weights are almost always highly active for initial tokens, resulting in performance damage. From the technical perspective, these learnable parameters at the start of the sequence can mitigate such effect by redistributing the attention weights more effectively (Han et al. 2024; Xiao et al. 2024).",
            "we also compare with GPT4 (Achiam et al. 2023)",
            "...Titans outperform all baselinesâ€“...GPT-4, and GPT4o-mini (Achiam et al. 2023)...",
            "extremely large models like GPT-4 (Achiam et al. 2023)",
            "each of which are missing: (1) a crucial component for learning processâ€”such as short-term memory, long-term memory, meta-memory, attending to current context, etc.",
            "An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec 2024; Schwarzschild et al. 2024; Staab et al. 2024).",
            "Forgetting Mechanism. When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which past information should be forgottenâ€“even with a deep or a very large matrix-valued memory.",
            "extremely large models such as GPT-4 (Achiam et al. 2023), GPT4o-mini, Qwen2.5-72B (A. Yang et al. 2024), and Llama3.1-70B (Touvron et al. 2023)"
        ],
        "abstract": "This paper does not present a novel method. Instead, it delves into an\nessential, yet must-know baseline in light of the latest advancements in\nGenerative Artificial Intelligence (GenAI): the utilization of GPT-4 for visual\nunderstanding. Our study centers on the evaluation of GPT-4's linguistic and\nvisual capabilities in zero-shot visual recognition tasks: Firstly, we explore\nthe potential of its generated rich textual descriptions across various\ncategories to enhance recognition performance without any training. Secondly,\nwe evaluate GPT-4's visual proficiency in directly recognizing diverse visual\ncontent. We conducted extensive experiments to systematically evaluate GPT-4's\nperformance across images, videos, and point clouds, using 16 benchmark\ndatasets to measure top-1 and top-5 accuracy. Our findings show that GPT-4,\nenhanced with rich linguistic descriptions, significantly improves zero-shot\nrecognition, offering an average top-1 accuracy increase of 7% across all\ndatasets. GPT-4 excels in visual recognition, outshining OpenAI-CLIP's ViT-L\nand rivaling EVA-CLIP's ViT-E, particularly in video datasets HMDB-51 and\nUCF-101, where it leads by 22% and 9%, respectively. We hope this research\ncontributes valuable data points and experience for future studies. We release\nour code at https://github.com/whwu95/GPT4Vis.",
        "pdf_url": "http://arxiv.org/pdf/2311.15732v2",
        "Questions": "1. We argue that in an effective learning paradigm, similar to the human brain, there are distinct yet interconnected modules, each of which is responsible for a component crucial to the learning process.\n2. Extremely large models such as GPT-4 (Achiam et al. 2023).\n3. Interestingly, Titans (MAL) are faster than baselines as well as the memory module.\n4. Memory has always been one of the core parts of the neural network designs (Graves, Wayne, and Danihelka 2014; JH Schmidhuber 1992; JÃ¼rgen Schmidhuber and Hochreiter 1997; J. Zhang et al. 2024).\n5. Note that, similar to meta-learning models (Nichol 2018; Zintgraf et al. 2019), training of the memory is in the inner-loop, and so parameters ğ‘Šğ¾ and ğ‘Šğ‘‰ are hyperparameters in the above loss function.\n6. Attention with causal mask has implicit bias toward initial tokens in the sequence, and so attention weights are almost always highly active for initial tokens, resulting in performance damage. From the technical perspective, these learnable parameters at the start of the sequence can mitigate such effect by redistributing the attention weights more effectively (Han et al. 2024; Xiao et al. 2024).\n7. Each of which are missing: (1) a crucial component for learning processâ€”such as short-term memory, long-term memory, meta-memory, attending to current context, etc.\n8. An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec 2024; Schwarzschild et al. 2024; Staab et al. 2024).\n9. Forgetting Mechanism. When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which past information should be forgottenâ€“even with a deep or a very large matrix-valued memory.",
        "Summary": "GPT-4ì˜ ê¸°ìˆ  ë³´ê³ ì„œì—ì„œëŠ” íš¨ê³¼ì ì¸ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•˜ë©°, ì¸ê°„ ë‘ë‡Œì™€ ìœ ì‚¬í•˜ê²Œ ì„œë¡œ ì—°ê²°ëœ ì—¬ëŸ¬ ëª¨ë“ˆì´ í•™ìŠµ ê³¼ì •ì˜ í•µì‹¬ ìš”ì†Œë¥¼ ë‹´ë‹¹í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. GPT-4ëŠ” ë§¤ìš° í° ëª¨ë¸ë¡œ, ì˜ìƒ ì¸ì‹ì—ì„œ ì œë¡œìƒ· ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì–¸ì–´ ëŠ¥ë ¥ì´ ì‹œê° ì¸ì‹ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ê³  ë°í˜€ì¡ŒìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, GPT-4ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ í‰ê· ì ìœ¼ë¡œ 7%ì˜ ì •í™•ë„ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì—ì„œ ë‘ë“œëŸ¬ì§„ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ë©”ëª¨ë¦¬ ëª¨ë“ˆê³¼ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ ë‹¤ë£¨ì–´ì§€ë©°, ì´ˆê¸° í† í°ì— ëŒ€í•œ ì£¼ì˜ ê°€ì¤‘ì¹˜ì˜ ì¬ë¶„ë°°ê°€ ì„±ëŠ¥ ê°œì„ ì— ê¸°ì—¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì—°êµ¬ëŠ” GPT-4ì˜ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì˜ ì •ëŸ‰ì  í‰ê°€ë¥¼ í†µí•´ í–¥í›„ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ê°œë°œì— ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì œê³µí•˜ê³ ì í•©ë‹ˆë‹¤."
    },
    "82": {
        "Title": "RWKV: Reinventing RNNs for the Transformer Era",
        "Authors": "Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, BartÅ‚omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, StanisÅ‚aw WoÅºniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu",
        "Counter": 12,
        "Context": [
            "Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.",
            "...the hidden state of Recurrent Neural Networks (RNNs) as a memory unit, which the model aims to compress the information into.",
            "This type of memory has been referred to as persistent or meta-memory in the literature (X. Dong et al. 2024; Sukhbaatar, Grave, et al. 2019).",
            "we compare with Mistral (Jiang et al. 2023) models, all of which are provided in the benchmark (Yuri Kuratov et al. 2024)",
            "Comparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + attention).",
            "the hidden state of Recurrent Neural Networks (RNNs) as a memory unit",
            "To enhance the parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast paralellizable algorithm.",
            "That is, we let ğ‘¥ âˆˆ Rğ‘ Ã—ğ‘‘in be the input, M âˆˆ Rğ‘‘ is the memory unit, and y âˆˆ Rğ‘‘in is the output, then the general form of the recurrent neural network is defined as:",
            "To overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in this section, we present a neural long-term memory module.",
            "We build upon the work of Yu Sun et al. (2024) that shows forward pass of a model optimizing with the mini-batch gradient descent.",
            "In time series tasks, we compare with Mamba-based (Behrouz, Santacatterina, and Zabih 2024), Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023), and linear models (Das et al. 2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et al. 2023).",
            "Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023)."
        ],
        "abstract": "Transformers have revolutionized almost all natural language processing (NLP)\ntasks but suffer from memory and computational complexity that scales\nquadratically with sequence length. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and computational requirements but\nstruggle to match the same performance as Transformers due to limitations in\nparallelization and scalability. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that combines the efficient\nparallelizable training of transformers with the efficient inference of RNNs.\n  Our approach leverages a linear attention mechanism and allows us to\nformulate the model as either a Transformer or an RNN, thus parallelizing\ncomputations during training and maintains constant computational and memory\ncomplexity during inference. We scale our models as large as 14 billion\nparameters, by far the largest dense RNN ever trained, and find RWKV performs\non par with similarly sized Transformers, suggesting future work can leverage\nthis architecture to create more efficient models. This work presents a\nsignificant step towards reconciling trade-offs between computational\nefficiency and model performance in sequence processing tasks.",
        "pdf_url": "http://arxiv.org/pdf/2305.13048v2",
        "Questions": "1. Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.\n2. This type of memory has been referred to as persistent or meta-memory in the literature (X. Dong et al. 2024; Sukhbaatar, Grave, et al. 2019).\n3. Comparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + attention).\n4. To overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in this section, we present a neural long-term memory module.\n5. In time series tasks, we compare with Mamba-based (Behrouz, Santacatterina, and Zabih 2024), Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023), and linear models (Das et al. 2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et al. 2023).\n6. Titans outperform all baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
        "Summary": "RWKV, a novel architecture introduced in the paper \"RWKV: Reinventing RNNs for the Transformer Era,\" effectively combines the strengths of Transformers and RNNs, addressing their limitations in terms of memory and computational complexity. The model employs a linear attention mechanism, enabling efficient training and inference while maintaining competitive performance on various NLP tasks. It incorporates a neural long-term memory module to enhance learning, forgetting, and information retrieval capabilities. Experimental results demonstrate that RWKV outperforms several baseline models, including various hybrid architectures and large language models like GPT-4. Overall, RWKV represents a significant advancement in scalable and efficient architectures for processing complex sequential data."
    },
    "41": {
        "Title": "Efficiently Modeling Long Sequences with Structured State Spaces",
        "Authors": "Albert Gu, Karan Goel, and Christopher Re",
        "Counter": 11,
        "Context": [
            "This decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).",
            "... this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).",
            "we compare with Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023)",
            "we compare the small fine-tuned version of Titans (MAC) with: (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.1- 8B (Touvron et al. 2023)",
            "this decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).",
            "which allows efficient inference for linear attentions.",
            "When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et al. 2024), the memory module is compressing the past data and fit it into a line.",
            "We show that this decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).",
            "Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.",
            "When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et al. 2024), the memory module is compressing the past data and fit it into a line.",
            "Similarly we can make Equation 18 faster. That is, when ğœ‚ and ğœƒ are learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution."
        ],
        "abstract": "A central goal of sequence modeling is designing a single principled model\nthat can address sequence data across a range of modalities and tasks,\nparticularly on long-range dependencies. Although conventional models including\nRNNs, CNNs, and Transformers have specialized variants for capturing long\ndependencies, they still struggle to scale to very long sequences of $10000$ or\nmore steps. A promising recent approach proposed modeling sequences by\nsimulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t),\ny(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state\nmatrix \\( A \\), this system could handle long-range dependencies mathematically\nand empirically. However, this method has prohibitive computation and memory\nrequirements, rendering it infeasible as a general sequence modeling solution.\nWe propose the Structured State Space sequence model (S4) based on a new\nparameterization for the SSM, and show that it can be computed much more\nefficiently than prior approaches while preserving their theoretical strengths.\nOur technique involves conditioning \\( A \\) with a low-rank correction,\nallowing it to be diagonalized stably and reducing the SSM to the well-studied\ncomputation of a Cauchy kernel. S4 achieves strong empirical results across a\ndiverse range of established benchmarks, including (i) 91\\% accuracy on\nsequential CIFAR-10 with no data augmentation or auxiliary losses, on par with\na larger 2-D ResNet, (ii) substantially closing the gap to Transformers on\nimage and language modeling tasks, while performing generation $60\\times$\nfaster (iii) SoTA on every task from the Long Range Arena benchmark, including\nsolving the challenging Path-X task of length 16k that all prior work fails on,\nwhile being as efficient as all competitors.",
        "pdf_url": "http://arxiv.org/pdf/2111.00396v3",
        "Questions": "1. This decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).\n2. ... this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).\n3. We compare with Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023).\n4. We compare the small fine-tuned version of Titans (MAC) with: (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.1-8B (Touvron et al. 2023).\n5. Which allows efficient inference for linear attentions.\n6. When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et al. 2024), the memory module is compressing the past data and fit it into a line.\n7. Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.\n8. Similarly we can make Equation 18 faster. That is, when ğœ‚ and ğœƒ are learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution.",
        "Summary": "The paper \"Efficiently Modeling Long Sequences with Structured State Spaces\" introduces the Structured State Space (S4) model, which effectively addresses long-range dependencies in sequence data through a novel parameterization of state space models (SSMs). This approach generalizes the forgetting mechanism found in modern recurrent models, enabling efficient computation by transforming the SSM into a linear time-invariant system that can be processed using global convolutions. The authors demonstrate that S4 outperforms existing models, including Transformer-based architectures, on various benchmarks, achieving significant speed and accuracy improvements, particularly on challenging tasks like the Long Range Arena Path-X. Additionally, S4 allows for efficient inference with linear attentions and utilizes vector or matrix-valued memory to compress past data effectively. The results indicate S4's potential as a general-purpose sequence modeling solution across diverse domains, including image classification, language modeling, and time-series forecasting."
    }
}