{
    "107": {
        "Title": "Attention is All you Need",
        "Authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
        "Counter": 26,
        "Context": [
            "Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling...",
            "most existing architectures–ranging from Hopfield Networks (Hopfield 1982) to LSTMs (Jürgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)–face challenges when dealing with generalization, length extrapolation, and/or reasoning.",
            "For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step.",
            "Titans outperforms Transformers with the same context window, and show competitive performance with Transformers that use the entire context.",
            "Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on attention mechanism.",
            "...the attention can be written as: 𝜙 (𝑄𝑖 )⊤𝜙 (𝐾𝑗 ) ℓ=1 𝜙 (𝑄𝑖 )⊤𝜙 (𝐾ℓ ) 𝜙 (𝑄𝑖 )⊤ (cid:205)𝑖 𝜙 (𝑄𝑖 )⊤ (cid:205)𝑖 j=1 𝜙 (𝐾𝑗 )𝑉𝑗 ℓ=1 𝜙 (𝐾ℓ ).",
            "Given 𝑥𝑡, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project 𝑥𝑡 into a key and value:",
            "we also follow the recent architectures that use normalization and gating with a linear layer before the final output projection (Mehta et al. 2023).",
            "we compare with Transformer++ (Touvron et al. 2023)",
            "Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan et al. 2020).",
            "This, however, brings a contradictory fact about linear recurrent (or linear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs. quadratic complexity)",
            "most existing architectures–ranging from Hopfield Networks (Hopfield 1982) to LSTMs (Jürgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)–face challenges when dealing with generalization, length extrapolation, and/or reasoning.",
            "For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step...",
            "...Titans outperforms Transformers with the same context window...",
            "Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on attention mechanism.",
            "the softmax in standard attention is replaced with an alternative kernel function 𝜙 (., .)",
            "Given 𝑥𝑡, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project 𝑥𝑡 into a key and value:",
            "compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory.",
            "Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale.",
            "most existing architectures–ranging from Hopfield Networks (Hopfield 1982) to LSTMs (Jürgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)–face challenges when dealing with generalization, length extrapolation, and/or reasoning.",
            "For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step...",
            "Modern Linear Models and Their Memory Perspective. As discussed earlier, one can define learning as a process for acquiring effective and useful memory.",
            "Given 𝑥𝑡, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project 𝑥𝑡 into a key and value:",
            "Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "Titans outperform all models even extremely large models like GPT4."
        ],
        "abstract": "The inference demand for LLMs has skyrocketed in recent months, and serving\nmodels with low latencies remains challenging due to the quadratic input length\ncomplexity of the attention layers. In this work, we investigate the effect of\ndropping MLP and attention layers at inference time on the performance of\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\ndecreases performance but leads to the best speedups alongside dropping entire\nlayers. For example, removing 33\\% of attention layers in a 13B Llama2 model\nresults in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\nalso observe that skipping layers except the latter layers reduces performances\nfor more layers skipped, except for skipping the attention layers.",
        "pdf_url": "http://arxiv.org/pdf/2407.15516v1",
        "Questions": "1. Transformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan et al. 2020).\n2. Most existing architectures–ranging from Hopfield Networks (Hopfield 1982) to LSTMs (Jürgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)–face challenges when dealing with generalization, length extrapolation, and/or reasoning.\n3. For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step.\n4. Titans outperforms Transformers with the same context window, and show competitive performance with Transformers that use the entire context.\n5. We also follow the recent architectures that use normalization and gating with a linear layer before the final output projection (Mehta et al. 2023).\n6. Compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory.\n7. The softmax in standard attention is replaced with an alternative kernel function 𝜙 (., .).\n8. Modern Linear Models and Their Memory Perspective. As discussed earlier, one can define learning as a process for acquiring effective and useful memory.\n9. Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).\n10. Titans outperform all models even extremely large models like GPT4.",
        "Summary": "\"Attention is All You Need\" 논문은 Transformer 아키텍처의 효율성과 성능을 탐구하며, 특히 Llama-v2 모델에서 MLP와 attention 레이어를 제거하는 실험을 통해 성능 저하가 미미하면서도 처리 속도를 크게 향상시킬 수 있음을 보여줍니다. 기존의 Transformer 모델들은 일반화, 길이 외삽, 추론에서 어려움을 겪는 반면, Titans와 같은 새로운 모델들은 강력한 메모리 구조 덕분에 이러한 문제를 극복하며 성능을 향상시킵니다. 연구에 따르면, attention 레이어를 생략하는 것이 MLP 레이어를 생략하는 것보다 성능 저하가 적고, 전반적으로 더 나은 속도 개선을 가져옵니다. 또한, Titans는 GPT-4와 같은 대형 모델들보다도 뛰어난 성능을 보이며, 이는 메모리 관리의 효율성 덕분입니다. 이러한 결과는 Transformer 아키텍처의 구조적 변화와 레이어 스킵핑 기법이 LLM의 효율성을 높이는 데 중요한 역할을 한다는 것을 시사합니다."
    },
    "40": {
        "Title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "Authors": "Albert Gu and Tri Dao",
        "Counter": 17,
        "Context": [
            "As examples of such models, we refer to Mamba2 (Dao and Gu 2024)",
            "we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024)",
            "we compare with Mamba-based (Behrouz, Santacatterina, and Zabih 2024)",
            "While some baselines also take advantage of gating mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows the importance of both our surprise mechanism and having deep and non-linear memory.",
            "Compared to Mamba2, which has the gating (forgetting) mechanism, Titans have deep non-linear memory, resulting in better memory management.",
            "Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024)",
            "To enhance the parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast paralellizable algorithm.",
            "...this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).",
            "we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also computationally efficient.",
            "...Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024)...",
            "we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024)",
            "To better handle the limited memory, we present a decaying mechanism that consider the proportion of memory size and the amount of data surprise, resulting in better memory management.",
            "In this case, we are using the same value for each of 𝛼, 𝜃, and 𝜂 in each chunk. Accordingly, in Equation 17, we can store Θ using a single scaler.",
            "we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also computationally efficient.",
            "Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
            "we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, Yury Kuratov, and Burtsev 2022)"
        ],
        "abstract": "Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.",
        "pdf_url": "http://arxiv.org/pdf/2312.00752v2",
        "Questions": "1. As examples of such models, we refer to Mamba2 (Dao and Gu 2024).\n2. We incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).\n3. While some baselines also take advantage of gating mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows the importance of both our surprise mechanism and having deep and non-linear memory.\n4. Compared to Mamba2, which has the gating (forgetting) mechanism, Titans have deep non-linear memory, resulting in better memory management.\n5. To enhance the parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast parallelizable algorithm.\n6. This equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).\n7. To better handle the limited memory, we present a decaying mechanism that considers the proportion of memory size and the amount of data surprise, resulting in better memory management.\n8. In this case, we are using the same value for each of 𝛼, 𝜃, and 𝜂 in each chunk. Accordingly, in Equation 17, we can store Θ using a single scaler.",
        "Summary": "\"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" introduces a new class of models designed to efficiently handle long sequences while maintaining high performance across various modalities, including language, audio, and genomics. The Mamba architecture incorporates selective state space models (SSMs) with an innovative selection mechanism that allows for context-dependent reasoning, outperforming traditional models like Transformers in both speed and accuracy. Key features include the integration of depthwise-separable convolutions and a memory management strategy that leverages a decaying mechanism to optimize memory usage. Comparisons with models such as Mamba2 reveal that Mamba's non-linear memory capabilities lead to superior performance, particularly in tasks requiring long-context processing. The findings highlight Mamba's potential as a robust backbone for foundation models in diverse applications."
    },
    "1": {
        "Title": "Gpt-4 technical report",
        "Authors": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.",
        "Counter": 15,
        "Context": [
            "we argue that in an effective learning paradigm, similar to human brain, there are distinct yet interconnected modules, each of which is responsible for a component crucial to the learning process.",
            "we compare with GPT4 (Achiam et al. 2023)",
            "extremely large models such as GPT-4 (Achiam et al. 2023)",
            "Interestingly, Titans (MAL) are faster than baselines as well as the memory module.",
            "we argue that in an effective learning paradigm, similar to human brain, there are distinct yet interconnected modules, each of which is responsible for a component crucial to the learning process.",
            "Memory has always been one of the core parts of the neural network designs (Graves, Wayne, and Danihelka 2014; JH Schmidhuber 1992; Jürgen Schmidhuber and Hochreiter 1997; J. Zhang et al. 2024).",
            "Note that, similar to meta-learning models (Nichol 2018; Zintgraf et al. 2019), training of the memory is in the inner-loop, and so parameters 𝑊𝐾 and 𝑊𝑉 are hyperparameters in the above loss function.",
            "Attention with causal mask has implicit bias toward initial tokens in the sequence, and so attention weights are almost always highly active for initial tokens, resulting in performance damage. From the technical perspective, these learnable parameters at the start of the sequence can mitigate such effect by redistributing the attention weights more effectively (Han et al. 2024; Xiao et al. 2024).",
            "we also compare with GPT4 (Achiam et al. 2023)",
            "...Titans outperform all baselines–...GPT-4, and GPT4o-mini (Achiam et al. 2023)...",
            "extremely large models like GPT-4 (Achiam et al. 2023)",
            "each of which are missing: (1) a crucial component for learning process—such as short-term memory, long-term memory, meta-memory, attending to current context, etc.",
            "An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec 2024; Schwarzschild et al. 2024; Staab et al. 2024).",
            "Forgetting Mechanism. When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which past information should be forgotten–even with a deep or a very large matrix-valued memory.",
            "extremely large models such as GPT-4 (Achiam et al. 2023), GPT4o-mini, Qwen2.5-72B (A. Yang et al. 2024), and Llama3.1-70B (Touvron et al. 2023)"
        ],
        "abstract": "This paper does not present a novel method. Instead, it delves into an\nessential, yet must-know baseline in light of the latest advancements in\nGenerative Artificial Intelligence (GenAI): the utilization of GPT-4 for visual\nunderstanding. Our study centers on the evaluation of GPT-4's linguistic and\nvisual capabilities in zero-shot visual recognition tasks: Firstly, we explore\nthe potential of its generated rich textual descriptions across various\ncategories to enhance recognition performance without any training. Secondly,\nwe evaluate GPT-4's visual proficiency in directly recognizing diverse visual\ncontent. We conducted extensive experiments to systematically evaluate GPT-4's\nperformance across images, videos, and point clouds, using 16 benchmark\ndatasets to measure top-1 and top-5 accuracy. Our findings show that GPT-4,\nenhanced with rich linguistic descriptions, significantly improves zero-shot\nrecognition, offering an average top-1 accuracy increase of 7% across all\ndatasets. GPT-4 excels in visual recognition, outshining OpenAI-CLIP's ViT-L\nand rivaling EVA-CLIP's ViT-E, particularly in video datasets HMDB-51 and\nUCF-101, where it leads by 22% and 9%, respectively. We hope this research\ncontributes valuable data points and experience for future studies. We release\nour code at https://github.com/whwu95/GPT4Vis.",
        "pdf_url": "http://arxiv.org/pdf/2311.15732v2",
        "Questions": "1. We argue that in an effective learning paradigm, similar to the human brain, there are distinct yet interconnected modules, each of which is responsible for a component crucial to the learning process.\n2. Extremely large models such as GPT-4 (Achiam et al. 2023).\n3. Interestingly, Titans (MAL) are faster than baselines as well as the memory module.\n4. Memory has always been one of the core parts of the neural network designs (Graves, Wayne, and Danihelka 2014; JH Schmidhuber 1992; Jürgen Schmidhuber and Hochreiter 1997; J. Zhang et al. 2024).\n5. Note that, similar to meta-learning models (Nichol 2018; Zintgraf et al. 2019), training of the memory is in the inner-loop, and so parameters 𝑊𝐾 and 𝑊𝑉 are hyperparameters in the above loss function.\n6. Attention with causal mask has implicit bias toward initial tokens in the sequence, and so attention weights are almost always highly active for initial tokens, resulting in performance damage. From the technical perspective, these learnable parameters at the start of the sequence can mitigate such effect by redistributing the attention weights more effectively (Han et al. 2024; Xiao et al. 2024).\n7. Each of which are missing: (1) a crucial component for learning process—such as short-term memory, long-term memory, meta-memory, attending to current context, etc.\n8. An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec 2024; Schwarzschild et al. 2024; Staab et al. 2024).\n9. Forgetting Mechanism. When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which past information should be forgotten–even with a deep or a very large matrix-valued memory.",
        "Summary": "GPT-4의 기술 보고서에서는 효과적인 학습 패러다임을 제안하며, 인간 두뇌와 유사하게 서로 연결된 여러 모듈이 학습 과정의 핵심 요소를 담당한다고 주장합니다. GPT-4는 매우 큰 모델로, 영상 인식에서 제로샷 성능을 평가하는 데 사용되며, 언어 능력이 시각 인식을 향상시키는 데 중요한 역할을 한다고 밝혀졌습니다. 연구 결과, GPT-4는 다양한 데이터셋에서 평균적으로 7%의 정확도 향상을 보였으며, 특히 비디오 데이터셋에서 두드러진 성능을 보였습니다. 또한, 메모리 모듈과 주의 메커니즘이 학습에 미치는 영향도 다루어지며, 초기 토큰에 대한 주의 가중치의 재분배가 성능 개선에 기여할 수 있음을 시사합니다. 마지막으로, 연구는 GPT-4의 시각적 이해 능력의 정량적 평가를 통해 향후 멀티모달 모델 개발에 기여할 수 있는 데이터를 제공하고자 합니다."
    },
    "82": {
        "Title": "RWKV: Reinventing RNNs for the Transformer Era",
        "Authors": "Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu",
        "Counter": 12,
        "Context": [
            "Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.",
            "...the hidden state of Recurrent Neural Networks (RNNs) as a memory unit, which the model aims to compress the information into.",
            "This type of memory has been referred to as persistent or meta-memory in the literature (X. Dong et al. 2024; Sukhbaatar, Grave, et al. 2019).",
            "we compare with Mistral (Jiang et al. 2023) models, all of which are provided in the benchmark (Yuri Kuratov et al. 2024)",
            "Comparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + attention).",
            "the hidden state of Recurrent Neural Networks (RNNs) as a memory unit",
            "To enhance the parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast paralellizable algorithm.",
            "That is, we let 𝑥 ∈ R𝑁 ×𝑑in be the input, M ∈ R𝑑 is the memory unit, and y ∈ R𝑑in is the output, then the general form of the recurrent neural network is defined as:",
            "To overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in this section, we present a neural long-term memory module.",
            "We build upon the work of Yu Sun et al. (2024) that shows forward pass of a model optimizing with the mini-batch gradient descent.",
            "In time series tasks, we compare with Mamba-based (Behrouz, Santacatterina, and Zabih 2024), Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023), and linear models (Das et al. 2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et al. 2023).",
            "Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023)."
        ],
        "abstract": "Transformers have revolutionized almost all natural language processing (NLP)\ntasks but suffer from memory and computational complexity that scales\nquadratically with sequence length. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and computational requirements but\nstruggle to match the same performance as Transformers due to limitations in\nparallelization and scalability. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that combines the efficient\nparallelizable training of transformers with the efficient inference of RNNs.\n  Our approach leverages a linear attention mechanism and allows us to\nformulate the model as either a Transformer or an RNN, thus parallelizing\ncomputations during training and maintains constant computational and memory\ncomplexity during inference. We scale our models as large as 14 billion\nparameters, by far the largest dense RNN ever trained, and find RWKV performs\non par with similarly sized Transformers, suggesting future work can leverage\nthis architecture to create more efficient models. This work presents a\nsignificant step towards reconciling trade-offs between computational\nefficiency and model performance in sequence processing tasks.",
        "pdf_url": "http://arxiv.org/pdf/2305.13048v2",
        "Questions": "1. Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.\n2. This type of memory has been referred to as persistent or meta-memory in the literature (X. Dong et al. 2024; Sukhbaatar, Grave, et al. 2019).\n3. Comparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + attention).\n4. To overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in this section, we present a neural long-term memory module.\n5. In time series tasks, we compare with Mamba-based (Behrouz, Santacatterina, and Zabih 2024), Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023), and linear models (Das et al. 2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et al. 2023).\n6. Titans outperform all baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023).",
        "Summary": "RWKV, a novel architecture introduced in the paper \"RWKV: Reinventing RNNs for the Transformer Era,\" effectively combines the strengths of Transformers and RNNs, addressing their limitations in terms of memory and computational complexity. The model employs a linear attention mechanism, enabling efficient training and inference while maintaining competitive performance on various NLP tasks. It incorporates a neural long-term memory module to enhance learning, forgetting, and information retrieval capabilities. Experimental results demonstrate that RWKV outperforms several baseline models, including various hybrid architectures and large language models like GPT-4. Overall, RWKV represents a significant advancement in scalable and efficient architectures for processing complex sequential data."
    },
    "41": {
        "Title": "Efficiently Modeling Long Sequences with Structured State Spaces",
        "Authors": "Albert Gu, Karan Goel, and Christopher Re",
        "Counter": 11,
        "Context": [
            "This decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).",
            "... this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).",
            "we compare with Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023)",
            "we compare the small fine-tuned version of Titans (MAC) with: (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.1- 8B (Touvron et al. 2023)",
            "this decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).",
            "which allows efficient inference for linear attentions.",
            "When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et al. 2024), the memory module is compressing the past data and fit it into a line.",
            "We show that this decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).",
            "Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.",
            "When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et al. 2024), the memory module is compressing the past data and fit it into a line.",
            "Similarly we can make Equation 18 faster. That is, when 𝜂 and 𝜃 are learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution."
        ],
        "abstract": "A central goal of sequence modeling is designing a single principled model\nthat can address sequence data across a range of modalities and tasks,\nparticularly on long-range dependencies. Although conventional models including\nRNNs, CNNs, and Transformers have specialized variants for capturing long\ndependencies, they still struggle to scale to very long sequences of $10000$ or\nmore steps. A promising recent approach proposed modeling sequences by\nsimulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t),\ny(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state\nmatrix \\( A \\), this system could handle long-range dependencies mathematically\nand empirically. However, this method has prohibitive computation and memory\nrequirements, rendering it infeasible as a general sequence modeling solution.\nWe propose the Structured State Space sequence model (S4) based on a new\nparameterization for the SSM, and show that it can be computed much more\nefficiently than prior approaches while preserving their theoretical strengths.\nOur technique involves conditioning \\( A \\) with a low-rank correction,\nallowing it to be diagonalized stably and reducing the SSM to the well-studied\ncomputation of a Cauchy kernel. S4 achieves strong empirical results across a\ndiverse range of established benchmarks, including (i) 91\\% accuracy on\nsequential CIFAR-10 with no data augmentation or auxiliary losses, on par with\na larger 2-D ResNet, (ii) substantially closing the gap to Transformers on\nimage and language modeling tasks, while performing generation $60\\times$\nfaster (iii) SoTA on every task from the Long Range Arena benchmark, including\nsolving the challenging Path-X task of length 16k that all prior work fails on,\nwhile being as efficient as all competitors.",
        "pdf_url": "http://arxiv.org/pdf/2111.00396v3",
        "Questions": "1. This decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024).\n2. ... this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022).\n3. We compare with Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao Zhang and Yan 2023).\n4. We compare the small fine-tuned version of Titans (MAC) with: (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.1-8B (Touvron et al. 2023).\n5. Which allows efficient inference for linear attentions.\n6. When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et al. 2024), the memory module is compressing the past data and fit it into a line.\n7. Building upon tensorizing mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable algorithm to train our deep neural long-term memory.\n8. Similarly we can make Equation 18 faster. That is, when 𝜂 and 𝜃 are learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution.",
        "Summary": "The paper \"Efficiently Modeling Long Sequences with Structured State Spaces\" introduces the Structured State Space (S4) model, which effectively addresses long-range dependencies in sequence data through a novel parameterization of state space models (SSMs). This approach generalizes the forgetting mechanism found in modern recurrent models, enabling efficient computation by transforming the SSM into a linear time-invariant system that can be processed using global convolutions. The authors demonstrate that S4 outperforms existing models, including Transformer-based architectures, on various benchmarks, achieving significant speed and accuracy improvements, particularly on challenging tasks like the Long Range Arena Path-X. Additionally, S4 allows for efficient inference with linear attentions and utilizes vector or matrix-valued memory to compress past data effectively. The results indicate S4's potential as a general-purpose sequence modeling solution across diverse domains, including image classification, language modeling, and time-series forecasting."
    }
}