{
    "27": {
        "Title": "Scaling laws for neural language models",
        "Authors": "Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei",
        "Counter": 24,
        "Context": [
            "[27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.",
            "The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.",
            "Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.",
            "Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model’s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.",
            "These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.",
            "We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.",
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "the model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (t = 2000) compared to the paraphrase injection scenario.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in §4.3.",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in §4.3.",
            "We provide potential explanations for recently observed, yet underexplored behaviors of LLMs.",
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity. We use the window size tw = 50.",
            "The measurement of the defined metrics are illustrated in Figure 1, which is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in § 4.4.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.",
            "There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.",
            "We propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.",
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11)."
        ],
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "pdf_url": "http://arxiv.org/pdf/2001.08361v1",
        "Questions": "1. [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.\n2. Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.\n3. The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\n4. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\n5. Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model’s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.\n6. These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).\n7. The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\n8. There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\n9. We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.\n10. The model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (t = 2000) compared to the paraphrase injection scenario.\n11. We provide potential explanations for recently observed, yet underexplored behaviors of LLMs.\n12. In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity. We use the window size tw = 50.\n13. The measurement of the defined metrics are illustrated in Figure 1, which is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in § 4.4.\n14. We propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.\n15. This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11).",
        "Summary": "인용 논문 제목 (Title): Scaling laws for neural language models\n\n1. **질문 :** [27]은 LLM의 성능이 모델 크기와 사전 훈련 코퍼스의 크기와 긍정적으로 상관관계가 있는 스케일링 법칙을 따름을 보고하였다.\n   - **답변 :** LLM의 성능은 모델 크기와 데이터셋 크기에 따라 증가하며, 이는 스케일링 법칙에 의해 설명된다. 즉, 모델의 크기와 훈련 데이터의 양이 증가할수록 성능이 향상된다는 것을 의미한다.\n   - **근거 :** \"Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two.\"\n\n2. **질문 :** 다음으로, 모델이 i번째로 지식을 제공받은 후 사실적 지식의 로그 확률에서 즉각적인 개선을 정량화하기 위한 메트릭을 정의한다.\n   - **답변 :** 모델이 특정 지식을 제공받은 후의 로그 확률 개선을 측정하기 위해 메트릭을 정의하는 것은 모델의 학습 효과를 평가하는 데 중요하다.\n   - **근거 :** \"We define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.\"\n\n3. **질문 :** 정의된 메트릭의 측정은 그림 1에 설명되어 있다. 효과성과 유지 가능성의 측정을 위해 IQR 방법을 사용하여 이상치 탐지를 적용한다.\n   - **답변 :** 효과성과 유지 가능성을 측정하기 위해 IQR 방법을 사용하여 이상치를 탐지하는 것은 데이터의 신뢰성을 높이는 데 기여한다.\n   - **근거 :** \"For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\"\n\n4. **질문 :** 결과는 중복(상단), 패러프레이즈(중앙), 한 번(하단) 주입 시나리오에 대해 보여진다.\n   - **답변 :** 다양한 주입 시나리오에서 모델의 성능 변화를 관찰하는 것은 지식 주입의 효과를 이해하는 데 도움이 된다.\n   - **근거 :** \"Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\"\n\n5. **질문 :** 획득 깊이에 관계없이(기억, 의미 일반화 및 조합 일반화), 주입된 지식을 포함한 배치로 모델이 업데이트된 후 프로브에서 측정된 모델의 로그 확률은 즉각적이고 뚜렷한 증가를 보인다.\n   - **답변 :** 모델이 주입된 지식으로 업데이트된 후 로그 확률이 즉각적으로 증가하는 것은 지식 주입의 효과를 나타낸다.\n   - **근거 :** \"The model’s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.\"\n\n6. **질문 :** 우리가 조사하는 OLMo-7B의 모든 사전 훈련 단계에서 이러한 패턴은 일관되게 나타난다.\n   - **답변 :** OLMo-7B의 모든 사전 훈련 단계에서 일관된 패턴이 나타나는 것은 모델의 일반화 능력을 보여준다.\n   - **근거 :** \"These patterns are consistent across all pretraining stages of OLMo-7B we investigate.\"\n\n7. **질문 :** 그림 5의 추정된 x-절편은 훈련으로 획득한 사실적 지식의 완전한 손실로 이어지는 추가 훈련 토큰의 수를 나타낸다.\n   - **답변 :** x-절편의 추정치는 모델이 훈련을 통해 획득한 지식의 손실을 이해하는 데 중요한 정보를 제공한다.\n   - **근거 :** \"The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\"\n\n8. **질문 :** 훈련 단계와 획득한 사실적 지식의 망각 사이에는 멤모리제이션과 일반화 모두에 대해 거듭제곱 법칙 관계가 있다.\n   - **답변 :** 훈련 단계와 지식의 망각 사이의 관계는 모델의 학습 및 일반화 능력을 이해하는 데 중요한 통찰을 제공한다.\n   - **근거 :** \"There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\"\n\n9. **질문 :** LLM이 비인기 지식을 습득하는 데 어려움을 겪는 이유는 충분한 노출이 필요하기 때문이다.\n   - **답변 :** LLM이 비인기 지식을 습득하는 데 어려움을 겪는 것은 학습 가능성의 임계값보다 짧은 간격으로 사실적 지식에 충분히 노출되어야 하기 때문이다.\n   - **근거 :** \"We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.\"\n\n10. **질문 :** 모델은 모든 획득 깊이에서 로그 확률의 더 큰 개선을 보이지만, 망각도 더 빠르다.\n    - **답변 :** 모델이 모든 획득 깊이에서 로그 확률의 개선을 보이는 것은 지식 주입의 효과를 나타내지만, 망각이 더 빠르다는 점은 주의가 필요하다.\n    - **근거 :** \"The model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster.\"\n\n11. **질문 :** 최근 관찰된, 그러나 충분히 탐구되지 않은 LLM의 행동에 대한 잠재적 설명을 제공한다.\n    - **답변 :** LLM의 행동에 대한 잠재적 설명을 제공하는 것은 모델의 이해를 심화하는 데 기여할 수 있다.\n    - **근거 :** \"We provide potential explanations for recently observed, yet underexplored behaviors of LLMs.\"\n\n12. **질문 :** Eq.1에서 지역 획득 최대값의 정의는 주입된 지식 k와 윈도우 크기 tw에 의존하지만, 간결함을 위해 tLAM(q, i)로 작성한다.\n    - **답변 :** 지역 획득 최대값의 정의는 주입된 지식과 윈도우 크기에 의존하며, 이는 모델의 성능을 평가하는 데 중요한 요소이다.\n    - **근거 :** \"The definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw.\"\n\n13. **질문 :** 정의된 메트릭의 측정은 그림 1에 설명되어 있으며, 이는 LLM의 행동을 해석하는 데 중요하다.\n    - **답변 :** 정의된 메트릭의 측정은 LLM의 행동을 해석하는 데 중요한 역할을 하며, 이는 모델의 성능을 이해하는 데 기여한다.\n    - **근거 :** \"The measurement of the defined metrics are illustrated in Figure 1, which is crucial for interpreting the behaviors of LLMs.\"\n\n14. **질문 :** 데이터 스케일링을 통한 LLM의 성능 향상은 일관된 개선의 결과라고 제안한다.\n    - **답변 :** 데이터 스케일링을 통한 LLM의 성능 향상은 모델이 사실적 지식을 더 빠르게 습득하는 능력의 출현이 아니라 일관된 개선의 결과로 볼 수 있다.\n    - **근거 :** \"We propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.\"\n\n15. **질문 :** 이 경향은 모든 모델 스케일과 주입 시나리오에서 일관되게 나타난다.\n    - **답변 :** 모든 모델 스케일과 주입 시나리오에서 일관된 경향이 나타나는 것은 모델의 일반화 능력을 보여준다.\n    - **근거 :** \"This tendency is consistent across all model scales and injection scenarios.\"\n\n### 답변 요약 (Summary of Answers)\n이 논문에서는 LLM의 성능이 모델 크기와 데이터셋 크기에 따라 증가하는 스케일링 법칙을 제시하고 있다. 모델이 특정 지식을 제공받은 후의 로그 확률 개선을 정량화하기 위한 메트릭을 정의하며, 효과성과 유지 가능성을 측정하기 위해 IQR 방법을 사용한다. 다양한 주입 시나리오에서의 성능 변화를 관찰하고, 주입된 지식으로 업데이트된 후 로그 확률이 즉각적으로 증가하는 현상을 설명한다. 또한, LLM이 비인기 지식을 습득하는 데 어려움을 겪는 이유와 모델의 망각 속도에 대한 논의도 포함되어 있다. 이러한 결과들은 LLM의 성능 향상과 관련된 다양한 요인들을 이해하는 데 기여하며, 데이터 스케일링을 통한 성능 개선이 일관된 개선의 결과임을 강조한다.",
        "Summary_QnA": "해당 논문은 LLM의 성능이 모델 크기와 데이터셋 크기에 따라 증가하는 스케일링 법칙을 기반으로, LLM이 사실적 지식을 어떻게 습득하는지를 심층적으로 분석하였다. 특히, 주입된 지식에 대한 로그 확률 개선을 정량화하는 메트릭을 정의하고, IQR 방법을 통해 효과성과 유지 가능성을 측정함으로써 데이터의 신뢰성을 높였다. 또한, LLM이 비인기 지식을 습득하는 데 어려움을 겪는 이유와 망각 속도에 대한 통찰을 제공하여, LLM의 성능 향상과 관련된 다양한 요인들을 이해하는 데 기여하였다. 이러한 연구는 LLM의 행동을 해석하고, 데이터 스케일링을 통한 성능 개선이 일관된 결과임을 강조하는 데 중요한 역할을 한다."
    },
    "29": {
        "Title": "Deduplicating training data makes language models better",
        "Authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini",
        "Counter": 18,
        "Context": [
            "the importance of dataset deduplication [29, 52]",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.",
            "Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.",
            "the importance of dataset deduplication [29, 52]",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "It is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "...as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.",
            "Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer."
        ],
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "pdf_url": "http://arxiv.org/pdf/2107.06499v2",
        "Questions": "1. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].\n2. Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\n3. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.\n4. It is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
        "Summary": "### 인용 논문 제목 (Title): Deduplicating training data makes language models better\n\n1. **질문 :** LLMs는 상당량의 훈련 데이터를 기억하며, 모델의 크기가 커질수록 훈련 데이터를 기억하는 경향이 증가하지만, 지식을 일반화하는 능력에는 해를 끼치지 않는다.\n   - **답변 :** 대형 언어 모델(LLM)은 훈련 데이터의 상당 부분을 기억하는 경향이 있으며, 모델의 크기가 커질수록 이러한 경향이 더욱 두드러진다. 그러나 연구에 따르면 이러한 기억은 모델의 일반화 능력에 부정적인 영향을 미치지 않는다.\n   - **근거 :** \"LLMs memorize a significant amount of training data... without harming the ability to generalize the knowledge.\"\n\n2. **질문 :** LLM을 비중복 데이터와 더 큰 배치 크기로 사전 훈련하면 사실적 지식의 습득이 향상되어 학습한 사실적 지식을 잊어버리는 것에 대해 더 강해진다.\n   - **답변 :** 비중복 데이터와 큰 배치 크기로 LLM을 사전 훈련하면 모델이 사실적 지식을 더 잘 습득하게 되어, 학습한 지식을 잊어버리는 경향이 줄어든다.\n   - **근거 :** \"Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge...\"\n\n3. **질문 :** 사전 훈련 코퍼스를 비중복화하면 LLM 성능이 향상되며, 이는 모델이 중복된 시퀀스에 더 높은 확률을 부여하는 것을 방지하고, 습득한 일반화를 더 오래 유지하는 데 도움을 준다.\n   - **답변 :** 비중복화된 사전 훈련 코퍼스를 사용하면 LLM의 성능이 향상되며, 이는 모델이 중복된 데이터에 대한 확률을 낮추고, 학습한 일반화를 더 오래 유지할 수 있도록 돕는다.\n   - **근거 :** \"Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences...\"\n\n4. **질문 :** 사전 훈련 데이터의 비중복화는 모델 성능 향상에 중요한 요소로 널리 관찰된다.\n   - **답변 :** 사전 훈련 데이터의 비중복화는 모델 성능을 향상시키는 중요한 요소로 널리 인식되고 있으며, 이는 다양한 연구에서 확인되었다.\n   - **근거 :** \"It is widely observed that deduplication of pretraining data is an important factor in improving model performance...\"\n\n### 답변 요약 (Summary of Answers)\n대형 언어 모델(LLM)은 훈련 데이터의 상당 부분을 기억하는 경향이 있으며, 모델의 크기가 커질수록 이러한 경향이 더욱 두드러진다. 그러나 이러한 기억은 모델의 일반화 능력에 부정적인 영향을 미치지 않는다. 비중복 데이터와 큰 배치 크기로 LLM을 사전 훈련하면 사실적 지식의 습득이 향상되어 학습한 지식을 잊어버리는 경향이 줄어든다. 또한, 비중복화된 사전 훈련 코퍼스를 사용하면 LLM의 성능이 향상되며, 이는 모델이 중복된 데이터에 대한 확률을 낮추고, 학습한 일반화를 더 오래 유지할 수 있도록 돕는다. 이러한 비중복화는 모델 성능 향상에 중요한 요소로 널리 인식되고 있다.",
        "Summary_QnA": "인용 논문 제목 (Title): <<Deduplicating training data makes language models better>>\n\n해당 논문은 대형 언어 모델(LLM)의 사실적 지식 습득 메커니즘을 탐구하며, 비중복 데이터와 큰 배치 크기가 모델의 성능 향상에 미치는 영향을 강조한다. 특히, 비중복화된 사전 훈련 코퍼스가 모델이 중복된 시퀀스에 대한 확률을 낮추고, 습득한 일반화를 더 오래 유지하도록 돕는다는 점을 통해, LLM의 기억과 일반화 간의 관계를 명확히 한다. 이러한 연구 결과는 LLM의 훈련 전략을 개선하는 데 기여하며, 사실적 지식의 습득과 유지에 대한 이해를 심화시킨다."
    },
    "46": {
        "Title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
        "Authors": "Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan",
        "Counter": 14,
        "Context": [
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining."
        ],
        "abstract": "Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.",
        "pdf_url": "http://arxiv.org/pdf/2205.10770v2",
        "Questions": "1. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.\n2. [44] and [46] focused on the dynamics of memorization in language model pretraining.\n3. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
        "Summary": "인용 논문 제목 (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n\n1. **질문 :** [46]은 다양한 사전 훈련 조건에서 LLM의 암기 및 망각 행동에 대한 광범위한 분석을 수행했습니다.\n   - **답변 :** [46]의 연구는 대형 언어 모델(LLM)의 암기 및 망각 행동을 다양한 사전 훈련 조건에서 분석하였으며, 이 연구는 모델 크기, 데이터셋 크기, 학습률 등이 암기 동역학에 미치는 영향을 측정했습니다.\n   - **근거 :** \"We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process.\"\n\n2. **질문 :** [44]와 [46]은 언어 모델 사전 훈련에서 암기 동역학에 초점을 맞췄습니다.\n   - **답변 :** [44]와 [46]의 연구는 언어 모델의 사전 훈련 과정에서 암기 동역학을 분석하였으며, 특히 모델 크기가 커질수록 암기 속도가 빨라진다는 점을 강조했습니다.\n   - **근거 :** \"We find that larger language models memorize training data faster across all settings.\"\n\n3. **질문 :** 망각의 기하급수적 경향은 LLM 훈련의 다양한 측면에서 보고되었습니다.\n   - **답변 :** LLM 훈련에서 망각의 기하급수적 경향은 사전 훈련에서의 암기 및 지속적인 학습에서의 작업 성능 등 여러 측면에서 관찰되었습니다. 이는 모델 크기가 증가할수록 망각이 줄어드는 경향과 관련이 있습니다.\n   - **근거 :** \"We show that the forgetting baseline increases with model scale, i.e., increasing model scale mitigates forgetting.\"\n\n답변 요약 \n: 이 논문은 대형 언어 모델의 암기 및 망각 동역학을 분석하며, 특히 모델 크기가 커질수록 암기 속도가 빨라지고 망각이 줄어드는 경향을 보여줍니다. [46]의 연구는 다양한 사전 훈련 조건에서 LLM의 행동을 분석하였고, [44]와 [46]은 언어 모델의 사전 훈련에서 암기 동역학에 초점을 맞추었습니다. 또한, 망각의 기하급수적 경향은 LLM 훈련의 여러 측면에서 관찰되며, 이는 모델 크기가 증가할수록 망각이 줄어드는 경향과 관련이 있습니다. 이러한 발견은 대형 언어 모델의 훈련 동역학을 이해하는 데 중요한 기여를 합니다.",
        "Summary_QnA": "인용 논문 제목 (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n\n해당 논문은 대형 언어 모델(LLM)의 암기 및 망각 동역학을 분석하여, 모델 크기가 커질수록 암기 속도가 빨라지고 망각이 줄어드는 경향을 보여줍니다. 이러한 연구는 LLM의 사전 훈련 과정에서의 암기 및 망각 행동을 이해하는 데 기여하며, [46]의 연구와 함께 다양한 사전 훈련 조건에서의 모델 행동을 심층적으로 분석합니다. 특히, 본 논문은 훈련 단계와 망각 간의 관계를 규명하고, 데이터 중복이 망각에 미치는 영향을 강조함으로써 LLM의 훈련 동역학에 대한 새로운 통찰을 제공합니다. 이러한 발견은 LLM의 사실적 지식 습득 메커니즘을 이해하는 데 중요한 기초 자료가 됩니다."
    },
    "41": {
        "Title": "Are emergent abilities of large language models a mirage?",
        "Authors": "Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo",
        "Counter": 10,
        "Context": [
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.",
            "Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold.",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]."
        ],
        "abstract": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.",
        "pdf_url": "http://arxiv.org/pdf/2304.15004v2",
        "Questions": "1. To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.\n2. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold.\n3. ...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].\n4. ...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].",
        "Summary": "인용 논문 제목 (Title): Are emergent abilities of large language models a mirage?\n\n1. **질문 :** LLM의 사실적 지식 습득을 상세히 분석하기 위해, 우리는 로그 확률을 검토하여 모델의 상태를 평가합니다. \n   - **답변 :** LLM의 사실적 지식 습득을 분석하기 위해 로그 확률을 통해 모델의 상태를 평가하는 방법은 모델이 훈련 중에 얼마나 많은 사실적 정보를 습득했는지를 세밀하게 파악할 수 있게 해줍니다. 이는 모델의 출력에서 나타나는 확률 분포를 분석하여, 특정 지식이 모델에 얼마나 잘 내재화되었는지를 평가하는 데 유용합니다.\n   - **근거 :** \"To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.\"\n\n2. **질문 :** 대부분의 잘 알려진 사실은 학습 가능성 임계값보다 짧은 훈련 단계 간격으로 모델에 제시될 가능성이 높습니다.\n   - **답변 :** LLM이 사실적 지식을 습득하는 과정에서, 잘 알려진 사실들은 모델이 학습할 수 있는 임계값보다 짧은 간격으로 제공되기 때문에, 이러한 사실들이 모델에 더 쉽게 내재화될 수 있습니다. 이는 모델이 훈련 초기 단계에서부터 이러한 정보를 빠르게 습득할 수 있음을 시사합니다.\n   - **근거 :** \"Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold.\"\n\n3. **질문 :** 이러한 지식의 습득은 모델의 상위 k 출력 시퀀스 생성에서 상대적으로 초기 훈련 단계에 반영될 것입니다.\n   - **답변 :** LLM이 특정 지식을 습득하는 과정은 초기 훈련 단계에서부터 모델의 출력 시퀀스에 반영되며, 이는 모델이 훈련 초기부터 특정 작업에 대한 성능을 발휘할 수 있음을 보여줍니다. 이러한 현상은 모델의 출력에서 나타나는 확률 분포와 관련이 있습니다.\n   - **근거 :** \"...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage.\"\n\n4. **질문 :** 지식의 누적 로그 확률은 모델의 디코딩 출력으로 지식을 생성하기에 충분히 높을 것입니다.\n   - **답변 :** LLM의 지식 습득 과정에서, 누적된 로그 확률이 충분히 높아지면 모델은 해당 지식을 디코딩 출력으로 생성할 수 있습니다. 이는 모델이 훈련을 통해 특정 지식을 효과적으로 내재화했음을 나타냅니다.\n   - **근거 :** \"...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model.\"\n\n답변 요약 \n: 이 논문은 LLM의 사실적 지식 습득 과정을 분석하기 위해 로그 확률을 평가하는 방법을 제시합니다. 잘 알려진 사실들은 학습 가능성 임계값보다 짧은 훈련 단계 간격으로 모델에 제공되어, 초기 훈련 단계에서부터 모델의 출력에 반영됩니다. 이러한 지식의 습득은 모델의 상위 k 출력 시퀀스 생성에서 나타나며, 누적된 로그 확률이 충분히 높아지면 모델은 해당 지식을 디코딩 출력으로 생성할 수 있습니다. 이로 인해 LLM의 훈련 과정에서 지식 습득의 메커니즘을 이해하는 데 중요한 통찰을 제공합니다.",
        "Summary_QnA": "인용 논문 제목 (Title): Are emergent abilities of large language models a mirage?\n\n해당 논문은 LLM의 사실적 지식 습득 과정을 로그 확률을 통해 세밀하게 분석함으로써, 모델이 훈련 초기 단계에서부터 잘 알려진 사실을 효과적으로 내재화할 수 있음을 보여줍니다. 특히, 학습 가능성 임계값보다 짧은 훈련 단계 간격으로 제공된 정보가 모델의 출력에 반영되며, 누적된 로그 확률이 충분히 높아질 경우 해당 지식을 디코딩 출력으로 생성할 수 있다는 점은 LLM의 훈련 메커니즘에 대한 중요한 통찰을 제공합니다. 이러한 발견은 LLM의 사실적 지식 습득에 대한 이해를 심화시키고, 향후 연구 방향에 기여할 수 있습니다."
    },
    "52": {
        "Title": "To repeat or not to repeat: Insights from scaling llm under token-crisis",
        "Authors": "Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You",
        "Counter": 10,
        "Context": [
            "the importance of dataset deduplication [29, 52]",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "the importance of dataset deduplication [29, 52]",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "It is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]."
        ],
        "abstract": "Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.",
        "pdf_url": "http://arxiv.org/pdf/2305.13230v2",
        "Questions": "1. the importance of dataset deduplication [29, 52]\n2. ...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].\n3. ...the importance of dataset deduplication [29, 52] can be explained.",
        "Summary": "인용 논문 제목 (Title): <<To repeat or not to repeat: Insights from scaling llm under token-crisis>>\n\n1. **질문 :** 데이터셋 중복 제거의 중요성은 무엇인가요?\n   - **답변 :** 데이터셋 중복 제거는 모델 성능 향상에 중요한 요소로 관찰되고 있습니다. 중복된 데이터는 모델이 특정 패턴에 과적합(overfitting)하게 만들 수 있으며, 이는 모델의 일반화 능력을 저하시킬 수 있습니다. 따라서, 데이터셋의 중복을 제거함으로써 모델이 더 다양한 데이터를 학습하고, 더 나은 성능을 발휘할 수 있도록 하는 것이 중요합니다.\n   - **근거 :** \"it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].\"\n\n2. **질문 :** 데이터셋 중복 제거의 중요성을 설명할 수 있나요?\n   - **답변 :** 데이터셋 중복 제거는 모델이 다양한 데이터를 학습하도록 도와주며, 이는 모델의 일반화 능력을 향상시킵니다. 중복된 데이터는 모델이 특정 데이터에 과도하게 적응하게 만들어, 새로운 데이터에 대한 성능 저하를 초래할 수 있습니다. 따라서, 중복 제거는 모델의 성능을 높이는 데 필수적입니다.\n   - **근거 :** \"the importance of dataset deduplication can be explained.\"\n\n답변 요약 \n: 데이터셋 중복 제거는 대규모 언어 모델(LLM)의 성능 향상에 필수적인 요소로, 중복된 데이터는 모델이 특정 패턴에 과적합하게 만들어 일반화 능력을 저하시킬 수 있습니다. 따라서, 중복 제거를 통해 모델이 다양한 데이터를 학습하고 더 나은 성능을 발휘할 수 있도록 하는 것이 중요합니다. 연구에 따르면, 데이터셋 중복 제거는 모델 성능을 개선하는 중요한 요소로 관찰되며, 이는 모델이 새로운 데이터에 대해 더 잘 일반화할 수 있도록 돕습니다. 이러한 이유로 데이터셋의 중복을 제거하는 것은 LLM의 효과적인 학습을 위해 필수적입니다.",
        "Summary_QnA": "해당 논문 \"How Do Large Language Models Acquire Factual Knowledge During Pretraining?\"은 인용 논문에서 강조된 데이터셋 중복 제거의 중요성을 바탕으로 LLM의 사실적 지식 습득 메커니즘을 심층적으로 탐구합니다. 연구 결과, 중복된 데이터가 모델의 과적합을 초래하고 일반화 능력을 저하시킨다는 점을 확인하며, 이는 LLM이 다양한 데이터를 효과적으로 학습하는 데 필수적임을 강조합니다. 또한, 중복 제거가 모델의 성능 향상에 기여하는 방식에 대한 구체적인 메커니즘을 제시함으로써, LLM의 훈련 과정에서의 지식 습득과 망각의 역학을 이해하는 데 기여합니다. 이러한 통찰은 LLM의 훈련 데이터 구성에 대한 새로운 방향성을 제시합니다."
    },
    "9": {
        "Title": "Language models are few-shot learners",
        "Authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
        "Counter": 9,
        "Context": [
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "...as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ],
        "abstract": "CLIP has shown a remarkable zero-shot capability on a wide range of vision\ntasks. Previously, CLIP is only regarded as a powerful visual encoder. However,\nafter being pre-trained by language supervision from a large amount of\nimage-caption pairs, CLIP itself should also have acquired some few-shot\nabilities for vision-language tasks. In this work, we empirically show that\nCLIP can be a strong vision-language few-shot learner by leveraging the power\nof language. We first evaluate CLIP's zero-shot performance on a typical visual\nquestion answering task and demonstrate a zero-shot cross-modality transfer\ncapability of CLIP on the visual entailment task. Then we propose a\nparameter-efficient fine-tuning strategy to boost the few-shot performance on\nthe vqa task. We achieve competitive zero/few-shot results on the visual\nquestion answering and visual entailment tasks without introducing any\nadditional pre-training procedure.",
        "pdf_url": "http://arxiv.org/pdf/2203.07190v1",
        "Questions": "1. Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].\n2. Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].\n3. Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
        "Summary": "인용 논문 제목 (Title): Language models are few-shot learners\n\n1. **질문 :** 최근의 사전 훈련 코퍼스는 철저하게 중복 제거되었는가?\n   - **답변 :** 최근의 사전 훈련 코퍼스는 중복 제거가 철저히 이루어졌으며, 이는 모델 성능을 향상시키는 데 기여하는 것으로 널리 관찰되고 있다. 데이터 중복 제거는 모델이 더 다양한 정보를 학습할 수 있도록 하여 성능을 높이는 데 중요한 역할을 한다.\n   - **근거 :** \"Recent pretraining corpora are thoroughly deduplicated, as it is widely observed that data deduplication can improve model performance.\"\n\n2. **질문 :** 최근 LLM(대형 언어 모델)에 대한 관심이 급증하고 있는가?\n   - **답변 :** 최근 LLM에 대한 관심이 급증하고 있으며, 이는 다양한 비전-언어 작업에서의 성능 향상과 관련이 있다. LLM은 자연어 처리 및 비전-언어 이해 작업에서 강력한 성능을 보여주고 있다.\n   - **근거 :** \"Recently, there has been a surge in interest in LLMs.\"\n\n3. **질문 :** 최근의 사전 훈련 코퍼스는 철저하게 중복 제거되었는가?\n   - **답변 :** 최근의 사전 훈련 코퍼스는 중복 제거가 철저히 이루어졌으며, 이는 모델 성능을 향상시키는 데 기여하는 것으로 널리 관찰되고 있다. 데이터 중복 제거는 모델이 더 다양한 정보를 학습할 수 있도록 하여 성능을 높이는 데 중요한 역할을 한다.\n   - **근거 :** \"Recent pretraining corpora are thoroughly deduplicated, as it is widely observed that data deduplication can improve model performance.\"\n\n답변 요약: 최근의 연구에 따르면, 사전 훈련 코퍼스는 철저하게 중복 제거되어 있으며, 이는 모델 성능 향상에 기여하는 것으로 나타났다. 데이터 중복 제거는 모델이 다양한 정보를 학습할 수 있도록 도와주며, 이는 LLM에 대한 관심이 급증하는 배경 중 하나로 작용하고 있다. LLM은 비전-언어 작업에서 강력한 성능을 보여주고 있으며, 이러한 경향은 앞으로도 계속될 것으로 예상된다.",
        "Summary_QnA": "인용 논문 제목 (Title): Language models are few-shot learners\n\n해당 논문은 LLM의 사전 훈련 과정에서의 사실적 지식 습득 메커니즘을 탐구하며, 중복 제거된 데이터의 중요성을 강조한다. 인용 논문에서 언급된 바와 같이, 중복 제거는 모델이 다양한 정보를 학습하도록 도와주어 성능 향상에 기여한다. 본 연구는 이러한 점을 바탕으로, 중복된 훈련 데이터가 사실적 지식의 망각을 가속화한다는 사실을 발견하였다. 이는 LLM의 성능 저하와 관련된 최근 관찰을 설명하는 데 기여하며, LLM의 훈련 데이터 구성 방식에 대한 새로운 통찰을 제공한다."
    },
    "36": {
        "Title": "Language models as knowledge bases?",
        "Authors": "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller",
        "Counter": 9,
        "Context": [
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]."
        ],
        "abstract": "Large language models (LLMs) outperform information retrieval techniques for\ndownstream knowledge-intensive tasks when being prompted to generate world\nknowledge. However, community concerns abound regarding the factuality and\npotential implications of using this uncensored knowledge. In light of this, we\nintroduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to\nsystematically and automatically evaluate generated knowledge from six\nimportant perspectives -- Factuality, Relevance, Coherence, Informativeness,\nHelpfulness and Validity. We conduct an extensive empirical analysis of the\ngenerated knowledge from three different types of LLMs on two widely studied\nknowledge-intensive tasks, i.e., open-domain question answering and\nknowledge-grounded dialogue. Surprisingly, our study reveals that the\nfactuality of generated knowledge, even if lower, does not significantly hinder\ndownstream tasks. Instead, the relevance and coherence of the outputs are more\nimportant than small factual mistakes. Further, we show how to use CONNER to\nimprove knowledge-intensive tasks by designing two strategies: Prompt\nEngineering and Knowledge Selection. Our evaluation code and LLM-generated\nknowledge with human annotations will be released to facilitate future\nresearch.",
        "pdf_url": "http://arxiv.org/pdf/2310.07289v1",
        "Questions": "1. Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].\n2. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
        "Summary": "### 인용 논문 제목 (Title): Language models as knowledge bases?\n\n1. **질문 :** 최근 연구들은 LLM이 사전 훈련 데이터에서 상당한 사실적 지식을 포착할 수 있음을 보여주었다 [14, 36, 40].\n   - **답변 :** 최근 연구들은 대형 언어 모델(LLM)이 사전 훈련 데이터에서 상당한 사실적 지식을 포착할 수 있음을 입증했습니다. LLM은 정보 검색 모델보다 지식 집약적인 작업에서 더 나은 성능을 보이며, 생성된 지식의 사실성은 다소 낮지만, 이는 하위 작업의 성능에 큰 영향을 미치지 않는 것으로 나타났습니다. \n   - **근거 :** \"LLM-generated knowledge surpasses retrieved knowledge in most evaluation perspectives, while it actually suffers from the factuality issue as expected.\"\n\n2. **질문 :** LLM의 매개변수에 인코딩된 지식에 대한 광범위한 연구가 진행되었다 [36, 40].\n   - **답변 :** LLM의 매개변수에 인코딩된 지식에 대한 연구는 LLM이 생성하는 지식의 질과 신뢰성을 평가하는 데 중요한 역할을 합니다. 연구 결과, LLM이 생성한 지식은 정보 검색 모델보다 더 유용하고 관련성이 높지만, 사실성 문제는 여전히 존재합니다. \n   - **근거 :** \"Despite obtaining lower factuality than retrieved knowledge, generated knowledge contributes more to the factuality of downstream tasks.\"\n\n### 답변 요약 (Summary of Answers)\n최근 연구들은 대형 언어 모델(LLM)이 사전 훈련 데이터에서 상당한 사실적 지식을 포착할 수 있음을 보여주고 있습니다. LLM은 정보 검색 모델보다 지식 집약적인 작업에서 더 나은 성능을 발휘하며, 생성된 지식의 사실성은 다소 낮지만 하위 작업의 성능에 큰 영향을 미치지 않는 것으로 나타났습니다. 또한, LLM의 매개변수에 인코딩된 지식에 대한 연구는 LLM이 생성하는 지식의 질과 신뢰성을 평가하는 데 중요한 역할을 하며, LLM이 생성한 지식은 정보 검색 모델보다 더 유용하고 관련성이 높지만 여전히 사실성 문제를 안고 있습니다. 이러한 연구 결과는 LLM을 지식 생성기로 활용하는 데 있어 중요한 통찰을 제공합니다.",
        "Summary_QnA": "해당 논문 \"How Do Large Language Models Acquire Factual Knowledge During Pretraining?\"은 인용 논문에서 제기된 LLM의 사실적 지식 포착 능력과 관련된 질문에 대한 심층적인 분석을 통해 연구를 발전시켰습니다. 특히, LLM이 사전 훈련 데이터에서 지식을 어떻게 획득하고 유지하는지를 탐구하며, 더 많은 데이터로 훈련해도 사실적 지식의 획득에 유의미한 개선이 없음을 밝혀냈습니다. 또한, LLM의 매개변수에 인코딩된 지식의 질과 신뢰성에 대한 기존 연구를 바탕으로, 훈련 단계와 기억 상실 간의 관계를 규명하고, 대량의 중복 데이터를 사용할 경우 더 빠른 기억 상실이 발생한다는 점을 강조했습니다. 이러한 통찰은 LLM의 지식 생성 및 활용에 대한 이해를 심화시키는 데 기여합니다."
    },
    "43": {
        "Title": "Dolma: An open corpus of three trillion tokens for language model pretraining research",
        "Authors": "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al.",
        "Counter": 9,
        "Context": [
            "...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...",
            "...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
            "...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ],
        "abstract": "Information about pretraining corpora used to train the current\nbest-performing language models is seldom discussed: commercial models rarely\ndetail their data, and even open models are often released without accompanying\ntraining data or recipes to reproduce them. As a result, it is challenging to\nconduct and advance scientific research on language modeling, such as\nunderstanding how training data impacts model capabilities and limitations. To\nfacilitate scientific research on language model pretraining, we curate and\nrelease Dolma, a three-trillion-token English corpus, built from a diverse\nmixture of web content, scientific papers, code, public-domain books, social\nmedia, and encyclopedic materials. We extensively document Dolma, including its\ndesign principles, details about its construction, and a summary of its\ncontents. We present analyses and experimental results on intermediate states\nof Dolma to share what we have learned about important data curation practices.\nFinally, we open-source our data curation toolkit to enable reproduction of our\nwork as well as support further research in large-scale data curation.",
        "pdf_url": "http://arxiv.org/pdf/2402.00159v2",
        "Questions": "1. Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].\n2. To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.",
        "Summary": "인용 논문 제목 (Title): Dolma: An open corpus of three trillion tokens for language model pretraining research\n\n1. **질문 :** 최근의 사전 훈련 코퍼스는 철저하게 중복 제거가 이루어졌다고 하는데, 데이터 중복 제거가 모델 성능을 향상시킬 수 있다는 것이 널리 관찰되고 있다. \n   - **답변 :** 데이터 중복 제거는 모델 훈련 시 토큰 효율성을 높이는 데 효과적이며, 이는 많은 연구에서 입증되었다. Dolma에서는 세 가지 단계의 중복 제거를 수행하여 데이터의 품질을 높였다. \n   - **근거 :** \"Deduplication of pretraining data has been shown to be effective for improving token efficiency during model training (Lee et al., 2022; Abbas et al., 2023; Tirumala et al., 2023).\"\n\n2. **질문 :** OLMo의 중간 체크포인트를 재개하여 OLMo의 사전 훈련 데이터(Dolma v1.5)를 사용하고, 매 100 훈련 단계마다 FICTIONAL KNOWLEDGE 데이터셋의 지식을 주입한다고 하는데, 이는 어떤 방식으로 이루어지는가?\n   - **답변 :** OLMo의 중간 체크포인트를 재개할 때, 원래의 사전 훈련 배치의 일부를 FICTIONAL KNOWLEDGE 데이터셋의 지식으로 교체하여 사실적 지식을 주입하는 방식으로 진행된다. 이는 모델이 훈련 중에 새로운 정보를 지속적으로 학습할 수 있도록 돕는다.\n   - **근거 :** \"we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.\"\n\n답변 요약 \n: Dolma는 세 가지 트릴리언 토큰으로 구성된 오픈 코퍼스로, 최근의 사전 훈련 코퍼스에서 중복 제거가 모델 성능을 향상시키는 데 효과적이라는 연구 결과를 바탕으로, 세 가지 단계의 중복 제거를 통해 데이터 품질을 높였다. 또한 OLMo의 중간 체크포인트를 재개하여 FICTIONAL KNOWLEDGE 데이터셋의 지식을 주입하는 방식으로 훈련이 이루어지며, 이는 모델이 새로운 정보를 지속적으로 학습할 수 있도록 돕는다. 이러한 접근은 언어 모델의 성능을 극대화하는 데 기여할 것으로 기대된다.",
        "Summary_QnA": "인용 논문 제목 (Title): Dolma: An open corpus of three trillion tokens for language model pretraining research\n\n해당 논문은 LLM의 사전 훈련 과정에서 데이터 중복 제거가 모델 성능 향상에 미치는 영향을 심층적으로 분석하며, Dolma의 세 가지 단계의 중복 제거를 통해 데이터 품질을 높이는 방법을 제시한다. 또한, OLMo의 중간 체크포인트를 활용하여 FICTIONAL KNOWLEDGE 데이터셋의 지식을 주입하는 방식을 통해 모델이 새로운 정보를 지속적으로 학습할 수 있도록 지원한다. 이러한 연구는 LLM의 사실적 지식 습득 메커니즘을 이해하는 데 기여하며, 모델의 성능을 극대화하는 데 중요한 통찰을 제공한다."
    },
    "33": {
        "Title": "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
        "Authors": "Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang",
        "Counter": 8,
        "Context": [
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "Training steps and the forgetting of acquired factual knowledge have a power-law relationship."
        ],
        "abstract": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge for achieving a satisfactory performance in downstream tasks. As\nlarge language models (LLMs) have demonstrated remarkable performance, it is\nintriguing to investigate whether CF exists during the continual instruction\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\nLLMs' knowledge during continual instruction tuning from the perspectives of\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\n7b parameters. Surprisingly, as the model scale increases, the severity of\nforgetting intensifies in such a model sale range which may result from the\nmuch significant initial performance in the larger LLM. Comparing the\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\nless forgetting and retains more knowledge. Interestingly, we also observe that\nLLMs can mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\nfine-tuning.",
        "pdf_url": "http://arxiv.org/pdf/2308.08747v5",
        "Questions": "1. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].\n2. Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].\n3. Training steps and the forgetting of acquired factual knowledge have a power-law relationship.",
        "Summary": "인용 논문 제목 (Title): An empirical study of catastrophic forgetting in large language models during continual fine-tuning\n\n1. **질문 :** 다양한 LLM 훈련의 측면에서 기억 상실의 기하급수적 경향이 보고되었는데, 여기에는 사전 훈련에서의 암기와 지속적 학습에서의 작업 성능이 포함된다. \n   - **답변 :** 연구에 따르면, 대형 언어 모델(LLM)에서 기억 상실 현상은 일반적으로 관찰되며, 모델의 크기가 증가할수록 기억 상실의 심각성이 증가하는 경향이 있다. 이는 초기 성능이 더 높은 대형 모델이 새로운 작업에 적합하기 위해 더 많은 매개변수 조정을 필요로 하기 때문이다.\n   - **근거 :** \"Our findings reveal that the forgetting problem is generally present in LLMs... as the model scale increases, the severity of forgetting intensifies.\"\n\n2. **질문 :** 여러 연구가 LLM의 훈련 역학을 조사했으며, 특히 훈련 중 어떻게 발전하는지를 다루었다. \n   - **답변 :** LLM의 훈련 역학에 대한 연구는 모델이 훈련 중에 어떻게 변화하는지를 분석하며, 이는 지속적 학습에서의 기억 상실 문제와 밀접한 관련이 있다. 연구 결과는 LLM이 지속적 훈련을 통해 일반 지식을 잃는 경향이 있음을 보여준다.\n   - **근거 :** \"We provide an initial research evidence that the CF problem generally exists in the continual instruction tuning process for different models...\"\n\n3. **질문 :** 훈련 단계와 습득한 사실적 지식의 기억 상실 간에는 거듭제곱 법칙 관계가 있다. \n   - **답변 :** 연구 결과는 훈련 단계가 증가함에 따라 LLM의 기억 상실이 더욱 심화된다는 것을 보여준다. 이는 모델이 새로운 작업에 적합하기 위해 더 많은 매개변수 조정을 필요로 하기 때문이며, 이로 인해 이전에 학습한 지식이 잊혀지는 경향이 있다.\n   - **근거 :** \"The performance gradually decreases as we continually tune the model with instruction tasks... the general knowledge suffers more significant forgetting.\"\n\n답변 요약 \n: 이 연구는 대형 언어 모델(LLM)에서 지속적 훈련 중 발생하는 기억 상실 현상에 대한 실증적 분석을 제공한다. 연구 결과, LLM은 훈련 단계가 증가할수록 기억 상실이 심화되며, 이는 모델의 초기 성능이 높을수록 더욱 두드러진다. 또한, LLM의 훈련 역학을 분석한 결과, 지속적 훈련 과정에서 일반 지식이 잊혀지는 경향이 있음을 확인하였다. 특히, 디코더 전용 모델인 BLOOMZ가 인코더-디코더 모델인 mT0보다 더 나은 지식 유지 능력을 보이는 것으로 나타났다. 마지막으로, 일반 지침 조정이 기억 상실 문제를 완화하는 데 도움이 될 수 있음을 시사한다. 이러한 결과는 LLM의 지속적 훈련에서 기억 상실 문제를 해결하기 위한 추가 연구의 필요성을 강조한다.",
        "Summary_QnA": "인용 논문 제목 (Title): An empirical study of catastrophic forgetting in large language models during continual fine-tuning\n\n해당 논문은 대형 언어 모델(LLM)의 사전 훈련 과정에서 사실적 지식의 습득과 기억 상실 간의 관계를 심층적으로 분석함으로써, 인용 논문에서 제기된 기억 상실 문제를 발전시켰다. 특히, 훈련 단계가 증가함에 따라 기억 상실이 심화된다는 점을 강조하며, 이는 모델의 초기 성능이 높을수록 더욱 두드러진다는 사실을 확인하였다. 또한, LLM의 훈련 역학을 통해 지속적 훈련에서 일반 지식이 잊혀지는 경향을 밝혀내어, 기억 상실 문제 해결을 위한 새로운 연구 방향을 제시하였다. 이러한 통찰은 LLM의 훈련 및 성능 최적화에 중요한 기초 자료로 작용할 수 있다."
    },
    "40": {
        "Title": "How much knowledge can you pack into the parameters of a language model?",
        "Authors": "Adam Roberts, Colin Raffel, Noam M. Shazeer",
        "Counter": 8,
        "Context": [
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]."
        ],
        "abstract": "It has recently been observed that neural language models trained on\nunstructured text can implicitly store and retrieve knowledge using natural\nlanguage queries. In this short paper, we measure the practical utility of this\napproach by fine-tuning pre-trained models to answer questions without access\nto any external context or knowledge. We show that this approach scales with\nmodel size and performs competitively with open-domain systems that explicitly\nretrieve answers from an external knowledge source when answering questions. To\nfacilitate reproducibility and future work, we release our code and trained\nmodels at https://goo.gle/t5-cbqa.",
        "pdf_url": "http://arxiv.org/pdf/2002.08910v4",
        "Questions": "1. Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].\n2. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
        "Summary": "인용 논문 제목 (Title): How much knowledge can you pack into the parameters of a language model?\n\n1. **질문 :** 최근 연구들은 LLM이 사전 훈련 데이터에서 상당한 사실적 지식을 포착할 수 있음을 보여주었다 [14, 36, 40].\n   - **답변 :** 최근 연구들은 대규모 언어 모델(LLM)이 비구조화된 텍스트로 훈련되었을 때, 사전 훈련 데이터에서 사실적 지식을 효과적으로 저장하고 검색할 수 있음을 보여주었다. 이러한 모델들은 자연어 쿼리를 통해 지식을 검색할 수 있으며, 이는 정보가 비구조화된 데이터에서 축적되기 때문에 가능하다. 이 연구는 LLM이 사전 훈련 중 내재화한 지식을 기반으로 질문에 답할 수 있는 능력을 평가하였다.\n   - **근거 :** \"It has also recently been observed that these models can internalize a sort of implicit 'knowledge base' after pre-training.\"\n\n2. **질문 :** LLM의 매개변수에 인코딩된 지식에 대한 광범위한 연구가 진행되었다 [36, 40].\n   - **답변 :** LLM의 매개변수에 인코딩된 지식에 대한 연구는 주로 모델이 사전 훈련 중 저장한 정보의 범위를 이해하고, 이러한 정보가 실제 질문 응답 작업에서 어떻게 활용되는지를 평가하는 데 초점을 맞추었다. 연구자들은 모델이 외부 지식에 접근하지 않고도 질문에 답할 수 있는 능력을 평가하여, 모델의 매개변수에 얼마나 많은 지식이 저장되어 있는지를 측정하였다.\n   - **근거 :** \"By feeding the model the input question alone, we can determine how much knowledge it has stored in its parameters while measuring its performance on a useful real-world problem.\"\n\n답변 요약 \n: 최근 연구들은 대규모 언어 모델(LLM)이 비구조화된 텍스트로 훈련되었을 때, 상당한 사실적 지식을 저장하고 검색할 수 있는 능력을 보여주었다. 이러한 모델들은 사전 훈련 중 내재화한 지식을 기반으로 질문에 답할 수 있으며, 이는 비구조화된 데이터에서 축적된 정보 덕분이다. 또한, LLM의 매개변수에 인코딩된 지식에 대한 연구는 모델이 외부 지식에 접근하지 않고도 질문에 답할 수 있는 능력을 평가하는 데 중점을 두었다. 이러한 연구들은 LLM이 실제 질문 응답 작업에서 얼마나 많은 지식을 저장하고 활용할 수 있는지를 측정하는 데 기여하고 있다.",
        "Summary_QnA": "인용 논문 제목 (Title): How much knowledge can you pack into the parameters of a language model?\n\n해당 논문은 대규모 언어 모델(LLM)이 사전 훈련 중 비구조화된 텍스트에서 상당한 사실적 지식을 저장하고 검색할 수 있는 능력을 강조하며, 이러한 지식이 질문 응답 작업에서 어떻게 활용되는지를 평가하는 데 중점을 두었다. 본 연구는 LLM의 사실적 지식 습득 메커니즘을 심층적으로 분석하여, 더 많은 데이터로 훈련하더라도 지식의 유지에 큰 개선이 없음을 발견하고, 훈련 단계와 기억 상실 간의 관계를 규명하였다. 이를 통해 LLM의 지식 저장 능력과 외부 지식 접근 없이 질문에 답하는 능력 간의 상관관계를 명확히 하여, LLM의 성능을 이해하는 데 기여하였다."
    },
    "18": {
        "Title": "Does fine-tuning llms on new knowledge encourage hallucinations?",
        "Authors": "Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig",
        "Counter": 7,
        "Context": [
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "Knowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining.",
            "We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability."
        ],
        "abstract": "When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.",
        "pdf_url": "http://arxiv.org/pdf/2405.05904v3",
        "Questions": "1. [44] and [46] focused on the dynamics of memorization in language model pretraining.\n2. Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].\n3. Knowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)\n4. The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\n5. However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining.\n6. We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.",
        "Summary": "인용 논문 제목 (Title): Does fine-tuning llms on new knowledge encourage hallucinations?\n\n1. **질문 :** [44]와 [46]은 언어 모델 사전 훈련에서 기억의 동역학에 초점을 맞췄다.\n   - **답변 :** 이 연구는 언어 모델의 사전 훈련 과정에서 기억의 동역학을 분석하며, 모델이 어떻게 정보를 기억하고 활용하는지를 탐구한다. 특히, 모델이 새로운 지식을 통합하는 데 어려움을 겪는다는 점을 강조한다.\n   - **근거 :** \"We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning...\"\n\n2. **질문 :** 여러 연구들이 LLM의 훈련 동역학을 조사했으며, 특히 훈련 중 어떻게 진화하는지를 다루었다 [12, 18, 22, 32, 33, 45, 51].\n   - **답변 :** LLM의 훈련 동역학에 대한 연구는 모델이 훈련 중에 어떻게 변화하는지를 분석하며, 특히 새로운 지식을 통합하는 과정에서의 어려움을 다룬다.\n   - **근거 :** \"We find that fine-tuning examples that introduce new knowledge are learned slowly...\"\n\n3. **질문 :** 사전 훈련 중 지식 주입에 대해, LLM이 기억과 일반화 측면에서 사실적 지식을 어떻게 습득하고 유지하는지를 탐구한다.\n   - **답변 :** LLM은 사전 훈련 중에 사실적 지식을 습득하고 이를 유지하는 데 어려움을 겪으며, 이는 기억과 일반화의 관점에서 분석된다.\n   - **근거 :** \"We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization...\"\n\n4. **질문 :** 그림 5의 추정된 x-절편은 훈련을 통해 습득한 사실적 지식의 완전한 손실로 이어지는 추가 훈련 토큰의 수를 나타낸다.\n   - **답변 :** x-절편은 모델이 훈련을 통해 습득한 사실적 지식이 완전히 소실되기 위해 필요한 추가 훈련 토큰의 수를 나타내며, 이는 모델의 기억 능력을 평가하는 데 중요한 지표가 된다.\n   - **근거 :** \"The estimated x-intercepts in Figure 5 represent the number of additional training tokens...\"\n\n5. **질문 :** 그러나 지식 관찰 시 로그 확률의 즉각적인 개선량은 더 큰 모델에 대해 증가하지만, 사전 훈련 진행 중에는 크게 증가하지 않는다.\n   - **답변 :** 모델의 크기가 커질수록 지식 관찰 시 로그 확률의 즉각적인 개선량은 증가하지만, 사전 훈련의 진행 과정에서는 그 증가폭이 크지 않다.\n   - **근거 :** \"the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models...\"\n\n6. **질문 :** 우리는 LLM이 비인기 지식을 습득하는 데 어려움을 겪는다고 가정한다. 이는 충분한 노출이 필요하기 때문이다.\n   - **답변 :** LLM은 비인기 지식을 습득하는 데 어려움을 겪으며, 이는 학습 가능성의 임계값보다 짧은 간격으로 사실적 지식에 충분히 노출되어야 가능하다.\n   - **근거 :** \"we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure...\"\n\n답변 요약 \n: 이 연구는 LLM이 새로운 사실적 지식을 통합하는 과정에서의 어려움과 그로 인해 발생하는 환각 현상에 대해 다룬다. LLM은 사전 훈련을 통해 사실적 지식을 습득하지만, 새로운 지식을 추가하는 과정에서 느린 학습 속도와 함께 기존 지식의 활용이 저하되는 경향이 있다. 특히, 비인기 지식의 습득이 어려운 이유는 학습 가능성의 임계값보다 짧은 간격으로 충분한 노출이 필요하기 때문이다. 이러한 결과는 LLM의 훈련 동역학과 기억 능력에 대한 중요한 통찰을 제공하며, 새로운 지식을 주입하는 것이 환각을 유발할 수 있음을 시사한다.",
        "Summary_QnA": "인용 논문 제목 (Title): <<Does fine-tuning llms on new knowledge encourage hallucinations?>>\n\n해당 논문은 LLM이 새로운 사실적 지식을 통합하는 과정에서의 어려움과 그로 인해 발생하는 환각 현상에 대한 심층적인 분석을 통해 연구를 발전시켰다. 특히, LLM이 사전 훈련 중에 습득한 지식을 유지하는 데 어려움을 겪으며, 새로운 지식을 추가하는 과정에서 느린 학습 속도와 기존 지식의 활용 저하가 발생한다는 점을 강조하였다. 또한, 비인기 지식의 습득이 어려운 이유로 충분한 노출이 필요하다는 가설을 제시함으로써, LLM의 훈련 동역학과 기억 능력에 대한 중요한 통찰을 제공하고, 새로운 지식 주입이 환각을 유발할 수 있음을 시사하였다. 이러한 연구 결과는 LLM의 사실적 지식 습득 메커니즘에 대한 이해를 심화시키는 데 기여한다."
    }
}