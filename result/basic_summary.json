"### 1. 기본 정보\n1) 제목: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n2) 저자: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n\n### 2. 연구 목적\n1) 문제의식: 대형 언어 모델의 사실적 지식 습득 메커니즘\n2) 설명: 최근 대형 언어 모델(LLM)이 상당한 사실적 지식을 저장할 수 있다는 관찰이 있었으나, 이들이 사전 훈련 중 사실적 지식을 어떻게 습득하는지에 대한 이해는 부족하다. 본 연구는 LLM의 사실적 지식 습득 과정을 분석하여, 데이터 양 증가가 지식 습득에 미치는 영향, 훈련 조건에 따른 효과성, 그리고 습득한 지식의 망각 메커니즘을 탐구한다. 이를 통해 LLM의 훈련 동역학을 이해하고, 향후 연구 및 활용에 기여하고자 한다.\n\n### 3. 연구 방법\n1) 실험 방법: 연구진은 LLM의 중간 사전 훈련 체크포인트를 사용하여, 새로운 사실적 지식을 주입하고, 다양한 훈련 조건에서 지식 습득의 진행 상황을 모니터링하였다. \n2) 데이터: FICTIONAL KNOWLEDGE 데이터셋을 구성하여, 허구적이지만 현실적인 엔티티에 대한 설명을 포함한 문장을 주입하였다. 이 데이터셋은 GPT-4를 통해 생성되었다.\n3) 모델 및 분석 방법: OLMo 모델을 사용하여, 주입된 지식에 대한 로그 확률을 평가하고, 메모리화, 의미적 일반화, 구성적 일반화의 세 가지 깊이에서 지식 습득을 분석하였다. 또한, 효과성 및 유지 가능성을 측정하기 위한 지표를 정의하였다.\n\n### 4. 주요 결과\n1) 연구의 주요 발견: LLM은 사실적 지식을 습득할 때, 미세한 확률 증가를 누적하는 방식으로 작동하며, 훈련 단계가 진행됨에 따라 지식 습득의 효과성은 크게 개선되지 않는다는 것을 발견하였다. 또한, 훈련 단계와 망각 간의 파워-로우 관계가 존재하며, 중복된 데이터로 훈련된 모델은 더 빠르게 망각하는 경향이 있음을 확인하였다.\n2) 기여 및 성과: 본 연구는 LLM의 사실적 지식 습득 동역학을 세밀하게 분석하고, 데이터 중복 제거의 중요성과 대규모 배치 훈련의 이점을 강조함으로써, LLM의 훈련 및 성능 향상에 대한 새로운 통찰을 제공하였다.\n\n### 5. 결론 및 시사점\n1) 결론: LLM의 사실적 지식 습득은 주입된 지식의 반복적 노출을 통해 이루어지며, 망각은 훈련 단계가 진행됨에 따라 발생한다. \n2) 시사점: 연구 결과는 LLM의 훈련 데이터 구성 및 훈련 방법에 대한 중요한 시사점을 제공하며, LLM의 성능 향상을 위한 전략적 접근을 제안한다.\n3) 연구의 한계: 본 연구는 특정 모델과 데이터셋에 국한되어 있으며, 다양한 LLM 아키텍처와 데이터셋에 대한 일반화 가능성에 대한 추가 연구가 필요하다.\n4) 향후 연구 방향: LLM의 지식 습득 및 망각 메커니즘을 더 깊이 이해하기 위해, 다양한 유형의 지식과 훈련 조건을 탐구하는 후속 연구가 필요하다."