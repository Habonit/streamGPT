"### 1. 기본 정보\n1) 제목: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n2) 저자: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n\n### 2. 연구 목적\n1) 문제의식: 대형 언어 모델의 사실적 지식 습득 메커니즘\n2) 설명: 최근 대형 언어 모델(LLM)이 상당한 사실적 지식을 저장할 수 있다는 관찰이 있었으나, 이들이 사전 훈련 중 사실적 지식을 어떻게 습득하는지에 대한 이해는 부족하다. 본 연구는 LLM의 사실적 지식 습득 과정을 분석하여, 훈련 데이터와 훈련 조건이 지식 습득에 미치는 영향을 규명하고자 한다. 이를 통해 LLM의 훈련 동역학을 이해하고, 향후 모델 성능 향상에 기여할 수 있는 통찰을 제공하는 것을 목표로 한다.\n\n### 3. 연구 방법\n1) 실험 방법: 연구진은 LLM의 중간 훈련 체크포인트를 사용하여, 이전에 접하지 않은 사실적 지식을 주입하고, 다양한 조건에서 지식 습득의 진행 상황을 모니터링하였다. 이를 통해 메모리화와 일반화 측면에서 LLM의 사실적 지식 습득을 분석하였다.\n2) 데이터: FICTIONAL KNOWLEDGE 데이터셋을 구성하여, 허구적이지만 현실적인 개체에 대한 설명을 포함한 문장을 주입하였다. 이 데이터셋은 GPT-4를 통해 생성되었다.\n3) 모델 및 분석 방법: OLMo 모델을 사용하여, 주입된 지식에 대한 로그 확률을 평가하고, 지식 습득의 효과성과 유지 가능성을 측정하였다. 또한, 다양한 주입 시나리오와 훈련 조건을 비교 분석하였다.\n\n### 4. 주요 결과\n1) 연구의 주요 발견: LLM은 사실적 지식을 습득할 때, 미세한 확률 증가를 누적하는 방식으로 작동하며, 훈련 데이터의 중복이 빠른 망각을 초래한다는 것을 발견하였다. 또한, 모델 크기와 훈련 배치 크기가 지식 습득의 강건성에 긍정적인 영향을 미친다는 결과를 도출하였다.\n2) 기여 및 성과: 본 연구는 LLM의 사실적 지식 습득 동역학을 세밀하게 분석하여, 기존 연구에서 간과된 훈련 조건의 중요성을 강조하고, 데이터 중복의 영향을 설명하는 새로운 통찰을 제공하였다.\n\n### 5. 결론 및 시사점\n1) 결론: LLM의 사실적 지식 습득은 훈련 중 지식의 반복적 노출을 통해 이루어지며, 망각 현상은 훈련 단계에 따라 다르게 나타난다. \n2) 시사점: 연구 결과는 LLM의 성능 향상을 위한 데이터 세트 구성 및 훈련 전략에 대한 중요한 시사점을 제공하며, 특히 데이터 중복의 최소화가 중요함을 강조한다.\n3) 연구의 한계: 본 연구는 특정 모델과 데이터셋에 국한되어 있으며, 다양한 LLM 아키텍처에 대한 일반화 가능성에 대한 검증이 필요하다.\n4) 향후 연구 방향: 향후 연구에서는 다양한 유형의 사실적 지식과 LLM의 훈련 조건을 조합하여, 보다 포괄적인 지식 습득 메커니즘을 탐구할 필요가 있다."