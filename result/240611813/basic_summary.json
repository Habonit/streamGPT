"### 1. 기본 정보\n1) 제목: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n2) 저자: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n\n### 2. 연구 목적\n1) 문제의식: 대형 언어 모델의 사실적 지식 습득 메커니즘\n2) 설명: 최근 대형 언어 모델(LLM)이 상당한 사실적 지식을 저장할 수 있다는 관찰에도 불구하고, 이들이 사전 훈련 중 사실적 지식을 어떻게 습득하는지에 대한 이해는 제한적이다. 본 연구는 LLM이 사전 훈련 중 사실적 지식을 습득하는 과정을 분석하여 이 격차를 해소하고자 한다. 연구는 LLM의 사실적 지식 습득이 훈련 데이터의 양, 훈련 조건, 잊어버림의 경향 등 다양한 요소에 의해 어떻게 영향을 받는지를 탐구한다.\n\n### 3. 연구 방법\n1) 실험 방법: 연구진은 LLM의 중간 사전 훈련 체크포인트를 사용하여, 이전에 접하지 않은 사실적 지식을 주입하고, 다양한 조건에서 지식 습득의 진행 상황을 모니터링하였다. \n2) 데이터: FICTIONAL KNOWLEDGE 데이터셋을 구성하여, 허구적이지만 현실적인 개체에 대한 설명을 포함한 문장을 주입하였다. 이 데이터셋은 LLM이 훈련 중 접하지 않은 지식을 포함한다.\n3) 모델 및 분석 방법: OLMo 모델을 사용하여, 주입된 지식에 대한 로그 확률을 평가하고, 메모리화, 의미적 일반화, 조합적 일반화의 세 가지 깊이에서 지식 습득을 분석하였다. 또한, 효과성 및 유지 가능성을 측정하기 위한 지표를 정의하였다.\n\n### 4. 주요 결과\n1) 연구의 주요 발견: LLM의 사실적 지식 습득은 미세한 확률 증가를 통해 이루어지며, 훈련 단계가 진행됨에 따라 효과성은 크게 개선되지 않는 것으로 나타났다. 또한, 훈련 단계와 잊어버림 간의 파워-로우 관계가 발견되었고, 중복된 훈련 데이터로 훈련된 모델은 더 빠른 잊어버림을 보였다.\n2) 기여 및 성과: 본 연구는 LLM의 사실적 지식 습득 동역학을 세밀하게 분석하여, 기존 연구에서 간과된 LLM의 행동을 설명할 수 있는 기초를 제공하였다. 또한, 데이터 중복 제거의 중요성과 대규모 배치 크기의 이점을 강조하였다.\n\n### 5. 결론 및 시사점\n1) 결론: LLM의 사실적 지식 습득은 점진적인 미세 습득을 통해 이루어지며, 잊어버림이 이 과정을 방해한다는 것을 확인하였다. \n2) 시사점: 연구 결과는 LLM의 훈련 데이터 구성 및 훈련 조건이 모델의 성능에 미치는 영향을 이해하는 데 중요한 통찰을 제공하며, LLM의 활용 가능성을 높인다.\n3) 연구의 한계: 본 연구는 특정 데이터셋과 모델에 국한되어 있으며, 다양한 유형의 지식에 대한 일반화 가능성에 대한 추가 연구가 필요하다.\n4) 향후 연구 방향: LLM의 지식 습득 메커니즘을 더욱 깊이 이해하기 위해, 다양한 데이터셋과 모델을 활용한 추가 연구가 필요하며, 장기적인 지식 유지 및 일반화 능력에 대한 연구도 중요하다."