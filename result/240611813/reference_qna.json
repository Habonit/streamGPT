{
    "27": {
        "Title": "Scaling laws for neural language models",
        "Authors": "Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei",
        "Counter": 22,
        "Context": [
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete.",
            "Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.",
            "The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.",
            "Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.",
            "Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model’s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.",
            "These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.",
            "We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.",
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in §4.3.",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.",
            "We propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.",
            "We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.",
            "In such cases, tw should be 1 and tLAM will reduce to t + 1.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.",
            "However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different.",
            "There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization."
        ],
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "pdf_url": "http://arxiv.org/pdf/2001.08361v1",
        "Questions": "1. Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.\n2. These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete.\n3. Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.\n4. The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\n5. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\n6. Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model’s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.\n7. These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).\n8. The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\n9. There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\n10. We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.\n11. However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different.\n12. We propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.",
        "Summary": "인용 논문 제목 (Title): Scaling laws for neural language models\n\n1. **질문 :** 최근 LLM에 대한 관심이 급증하고 있으며, [23]과 [27]은 LLM의 성능이 모델 크기와 사전 훈련 코퍼스의 크기와 긍정적으로 상관관계가 있는 스케일링 법칙을 따름을 보고했습니다.\n   - **답변 :** LLM의 성능은 모델 크기와 사전 훈련 데이터의 양에 따라 증가하며, 이는 스케일링 법칙에 의해 설명됩니다. 즉, 모델의 크기와 데이터 양이 증가할수록 성능이 향상된다는 것입니다.\n   - **근거 :** \"The loss scales as a power-law with model size, dataset size, and the amount of compute used for training.\"\n\n2. **질문 :** 이러한 연구들은 주로 사전 훈련이 완료된 후 LLM에 인코딩된 사실적 지식을 조사하는 데 초점을 맞추었습니다.\n   - **답변 :** 연구들은 LLM이 사전 훈련 후에 얼마나 많은 사실적 지식을 인코딩하고 있는지를 분석하는 데 중점을 두고 있습니다. 이는 LLM의 지식 기반을 이해하는 데 중요한 요소입니다.\n   - **근거 :** \"These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete.\"\n\n3. **질문 :** 다음으로, 모델이 i번째로 지식을 제시받은 후 사실적 지식의 로그 확률에서 즉각적인 개선을 정량화하기 위한 메트릭을 정의합니다.\n   - **답변 :** 모델이 특정 지식을 반복적으로 접할 때, 그 지식에 대한 로그 확률의 즉각적인 개선을 측정하기 위한 메트릭을 설정하는 것이 중요합니다. 이는 모델의 학습 효과를 평가하는 데 유용합니다.\n   - **근거 :** \"Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.\"\n\n4. **질문 :** 정의된 메트릭의 측정은 그림 1에 설명되어 있습니다. 효과성과 유지 가능성의 측정을 위해 IQR 방법을 사용하여 이상치를 탐지합니다.\n   - **답변 :** 효과성과 유지 가능성을 평가하기 위해 IQR 방법을 사용하여 이상치를 탐지하는 방식으로 메트릭을 측정합니다. 이는 모델의 성능을 정량적으로 분석하는 데 도움을 줍니다.\n   - **근거 :** \"For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\"\n\n5. **질문 :** 결과는 중복(상단), 패러프레이즈(중앙), 한 번(하단) 주입 시나리오에 대해 보여집니다.\n   - **답변 :** 다양한 주입 시나리오에서 모델의 성능 결과를 비교하여, 각 시나리오에서의 성능 차이를 분석합니다. 이는 모델의 지식 주입 효과를 평가하는 데 유용합니다.\n   - **근거 :** \"Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\"\n\n6. **질문 :** 획득 깊이에 관계없이(기억, 의미 일반화 및 조합 일반화), 주입된 지식이 포함된 배치로 모델이 업데이트된 후 프로브에서 측정된 모델의 로그 확률은 즉각적이고 뚜렷한 증가를 보입니다.\n   - **답변 :** 모델이 주입된 지식으로 업데이트된 후, 로그 확률이 즉각적으로 증가하는 현상은 기억, 의미 일반화 및 조합 일반화와 같은 다양한 깊이에서 일관되게 나타납니다.\n   - **근거 :** \"Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model’s log probability measured on the probes shows an immediate and distinctive increase.\"\n\n7. **질문 :** 그림 5의 추정된 x-절편은 훈련으로 획득한 사실적 지식의 완전한 손실로 이어지는 추가 훈련 토큰의 수를 나타냅니다.\n   - **답변 :** x-절편의 추정치는 훈련 과정에서 사실적 지식의 손실을 완전히 상실하기 위해 필요한 추가 훈련 토큰의 수를 나타내며, 이는 모델의 학습 효율성을 평가하는 데 중요한 지표입니다.\n   - **근거 :** \"The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\"\n\n8. **질문 :** 훈련 단계와 획득한 사실적 지식의 망각 사이에는 거듭된 관계가 있습니다.\n   - **답변 :** 훈련 단계가 증가함에 따라 모델이 획득한 사실적 지식을 잊어버리는 경향이 있으며, 이는 기억과 일반화 측면에서 모두 관찰됩니다.\n   - **근거 :** \"There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\"\n\n9. **질문 :** LLM이 비인기 지식을 습득하는 데 어려움을 겪는 이유는 무엇인가요?\n   - **답변 :** LLM은 비인기 지식을 습득하는 데 필요한 충분한 노출이 부족하여, 학습 가능성의 임계값보다 짧은 간격으로 사실적 지식을 접해야 합니다.\n   - **근거 :** \"We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.\"\n\n10. **질문 :** 모델 크기와 사전 훈련 토큰 수를 확장하는 이점은 질적으로 다르다는 것을 시사하는 발견은 무엇인가요?\n    - **답변 :** 모델 크기와 사전 훈련 토큰 수를 늘릴 때 즉각적인 개선의 양은 증가하지만, 사전 훈련 진행 중에는 그 양이 크게 증가하지 않으며, 이는 두 가지 확장 이점이 질적으로 다르다는 것을 나타냅니다.\n    - **근거 :** \"However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining.\"\n\n11. **질문 :** LLM의 성능 향상은 데이터 스케일링의 결과로 일관된 개선에서 비롯된다고 제안하는 이유는 무엇인가요?\n    - **답변 :** LLM의 성능 향상은 사전 훈련 중 사실적 지식을 더 빠르게 습득하는 능력의 출현이 아니라, 데이터 스케일링을 통해 일관된 개선이 이루어지기 때문입니다.\n    - **근거 :** \"We propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining.\"\n\n### 답변 요약 (Summary of Answers)\n이 논문에서는 LLM의 성능이 모델 크기와 사전 훈련 데이터의 양에 따라 증가하는 스케일링 법칙을 제시하고 있습니다. 연구들은 LLM이 사전 훈련 후 인코딩된 사실적 지식을 분석하는 데 중점을 두고 있으며, 이를 정량화하기 위한 메트릭을 정의합니다. 또한, 다양한 주입 시나리오에서 모델의 성능을 비교하고, 주입된 지식으로 업데이트된 후 로그 확률의 즉각적인 증가를 관찰합니다. 훈련 단계와 사실적 지식의 망각 사이의 관계를 설명하며, LLM이 비인기 지식을 습득하는 데 어려움을 겪는 이유를 제시합니다. 마지막으로, 모델 크기와 사전 훈련 토큰 수의 확장 이점이 질적으로 다르다는 점을 강조하며, 데이터 스케일링을 통한 성능 향상을 제안합니다. 이러한 결과들은 LLM의 성능을 이해하고 향상시키는 데 중요한 통찰을 제공합니다.",
        "Summary_QnA": "해당 논문은 LLM의 사실적 지식 습득 메커니즘을 탐구하며, 인용 논문에서 제시된 스케일링 법칙과 LLM의 성능 향상 간의 관계를 심화시킵니다. 특히, 모델 크기와 사전 훈련 데이터 양의 증가가 성능에 미치는 영향을 분석하고, 훈련 단계와 지식 망각 간의 파워-로 관계를 밝혀내어 LLM의 지식 유지 및 일반화 능력에 대한 새로운 통찰을 제공합니다. 또한, 비인기 지식 습득의 어려움과 데이터 스케일링의 질적 차이를 강조함으로써, LLM의 성능 향상에 대한 보다 정교한 이해를 제시합니다. 이러한 연구 결과는 LLM의 훈련 및 활용에 있어 중요한 기초 자료로 작용할 것입니다."
    },
    "29": {
        "Title": "Deduplicating training data makes language models better",
        "Authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini",
        "Counter": 16,
        "Context": [
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.",
            "Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "It is widely observed that deduplication of pretraining data is an important factor in improving model performance.",
            "Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences."
        ],
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "pdf_url": "http://arxiv.org/pdf/2107.06499v2",
        "Questions": "1. ...the importance of dataset deduplication [29, 52] can be explained.\n2. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].\n3. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\n4. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.\n5. It is widely observed that deduplication of pretraining data is an important factor in improving model performance.",
        "Summary": "인용 논문 제목 (Title): Deduplicating training data makes language models better\n\n1. **질문 :** 데이터셋 중복 제거의 중요성에 대해 설명해 주세요.\n   - **답변 :** 데이터셋 중복 제거는 언어 모델의 성능을 향상시키는 중요한 요소입니다. 중복된 데이터는 모델이 특정 예제에 과도하게 적합하게 만들고, 이는 모델의 일반화 능력을 저하시킬 수 있습니다. 중복 제거를 통해 모델은 더 다양한 데이터를 학습하게 되어, 더 나은 일반화 성능을 발휘할 수 있습니다.\n   - **근거 :** \"We show that one particular source of bias, duplicated training examples, is pervasive... deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences.\"\n\n2. **질문 :** LLM이 훈련 데이터를 상당량 기억한다는 것은 무엇을 의미하나요?\n   - **답변 :** LLM은 훈련 데이터의 상당 부분을 기억하는 경향이 있으며, 모델의 크기가 커질수록 이러한 경향이 증가합니다. 그러나 이는 지식의 일반화 능력을 해치지 않습니다. 즉, 모델은 훈련 데이터의 특정 패턴을 기억하면서도 새로운 데이터에 대한 일반화 능력을 유지할 수 있습니다.\n   - **근거 :** \"LLMs memorize a significant amount of training data... the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge.\"\n\n3. **질문 :** 중복 제거된 데이터로 LLM을 사전 훈련하면 사실적 지식 습득이 어떻게 향상되나요?\n   - **답변 :** 중복 제거된 데이터와 더 큰 배치 크기로 LLM을 사전 훈련하면 사실적 지식의 습득이 향상됩니다. 이는 모델이 학습한 사실적 지식을 잊어버리는 것을 방지하여 더 강력한 성능을 발휘하게 합니다.\n   - **근거 :** \"pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\"\n\n4. **질문 :** 사전 훈련 코퍼스의 중복 제거가 LLM 성능에 미치는 영향은 무엇인가요?\n   - **답변 :** 사전 훈련 코퍼스의 중복 제거는 LLM의 성능을 향상시킵니다. 이는 모델이 중복된 시퀀스에 더 높은 확률을 부여하는 것을 방지하고, 습득한 일반화를 더 오래 유지할 수 있도록 돕습니다.\n   - **근거 :** \"our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences.\"\n\n5. **질문 :** 사전 훈련 데이터의 중복 제거가 모델 성능 향상에 중요한 요소로 관찰되는 이유는 무엇인가요?\n   - **답변 :** 사전 훈련 데이터의 중복 제거는 모델 성능 향상에 중요한 요소로 관찰됩니다. 이는 모델이 중복된 데이터를 학습하는 것을 줄여, 더 다양한 데이터에 대한 학습을 가능하게 하여 성능을 개선합니다.\n   - **근거 :** \"It is widely observed that deduplication of pretraining data is an important factor in improving model performance.\"\n\n답변 요약: 데이터셋 중복 제거는 언어 모델의 성능을 향상시키는 중요한 요소로, 중복된 데이터는 모델의 일반화 능력을 저하시킬 수 있습니다. LLM은 훈련 데이터의 상당 부분을 기억하는 경향이 있으며, 이는 모델의 크기가 커질수록 증가하지만, 일반화 능력에는 영향을 미치지 않습니다. 중복 제거된 데이터로 사전 훈련된 LLM은 사실적 지식 습득이 향상되고, 중복된 시퀀스에 대한 확률 부여를 줄여 성능을 개선합니다. 이러한 이유로 사전 훈련 데이터의 중복 제거는 모델 성능 향상에 중요한 요소로 관찰됩니다.",
        "Summary_QnA": "인용 논문 제목 (Title): Deduplicating training data makes language models better\n\n해당 논문은 LLM의 사실적 지식 습득 메커니즘을 탐구하며, 중복 제거된 데이터의 중요성을 강조합니다. 중복된 데이터는 모델의 일반화 능력을 저하시킬 수 있으며, 이는 LLM이 특정 예제에 과도하게 적합하게 만드는 원인이 됩니다. 연구 결과, 중복 제거와 더 큰 배치 크기로 훈련된 LLM은 사실적 지식의 습득이 향상되고, 잊어버림에 대한 저항력이 증가함을 보여줍니다. 이러한 발견은 LLM의 성능 향상에 있어 중복 제거가 필수적임을 입증하며, 모델이 더 다양한 데이터를 학습할 수 있도록 돕는다는 점에서 중요한 기여를 합니다."
    },
    "46": {
        "Title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
        "Authors": "Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan",
        "Counter": 14,
        "Context": [
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "Training steps and the forgetting of acquired factual knowledge have a power-law relationship. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining.",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining."
        ],
        "abstract": "Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.",
        "pdf_url": "http://arxiv.org/pdf/2205.10770v2",
        "Questions": "1. [44] and [46] focused on the dynamics of memorization in language model pretraining.\n2. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].\n3. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.\n4. Training steps and the forgetting of acquired factual knowledge have a power-law relationship.",
        "Summary": "인용 논문 제목 (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n\n1. **질문 :** [44]와 [46]은 언어 모델 사전 훈련에서의 암기 동역학에 초점을 맞추었다.\n   - **답변 :** [44]와 [46]은 언어 모델의 사전 훈련 과정에서의 암기 동역학을 분석하였으며, 특히 모델 크기와 훈련 과정 전반에 걸쳐 암기 속도를 측정하였다. 이 연구들은 대형 언어 모델이 훈련 데이터를 더 빠르게 암기하는 경향이 있음을 보여준다.\n   - **근거 :** \"We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings.\"\n\n2. **질문 :** 잊어버림의 기하급수적 경향은 사전 훈련에서의 암기와 지속적인 학습에서의 작업 성능을 포함한 다양한 LLM 훈련 측면에서 보고되었다.\n   - **답변 :** 연구에서는 잊어버림의 기하급수적 경향이 사전 훈련과 지속적인 학습에서의 작업 성능에 영향을 미친다고 보고하였다. 이는 모델이 훈련 데이터를 잊는 속도가 훈련이 진행됨에 따라 증가함을 나타낸다.\n   - **근거 :** \"Our empirical studies show that forgetting curves have lower bounds — we coin this as the forgetting baseline — and that this baseline increases with model scale.\"\n\n3. **질문 :** 특히, [46]은 다양한 사전 훈련 조건에서 LLM의 암기 및 잊어버림 행동에 대한 광범위한 분석을 수행하였다.\n   - **답변 :** [46]의 연구는 다양한 사전 훈련 조건에서 LLM의 암기 및 잊어버림 행동을 심층적으로 분석하였으며, 이는 모델의 크기와 훈련 데이터의 특성이 암기 동역학에 미치는 영향을 이해하는 데 기여하였다.\n   - **근거 :** \"Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.\"\n\n4. **질문 :** 훈련 단계와 습득한 사실 지식의 잊어버림은 거듭제곱 법칙 관계를 가진다.\n   - **답변 :** 훈련 단계와 습득한 사실 지식의 잊어버림 사이에는 거듭제곱 법칙 관계가 존재하며, 이는 모델이 훈련을 진행함에 따라 잊어버림의 속도가 증가함을 나타낸다.\n   - **근거 :** \"We find that larger language models memorize training data faster... forgetting curves have lower bounds.\"\n\n답변 요약 \n: 이 연구는 대형 언어 모델의 암기 및 잊어버림 동역학을 분석하며, 모델 크기가 클수록 훈련 데이터를 더 빠르게 암기하고 잊어버림의 속도가 증가하는 경향을 보인다고 설명한다. [44]와 [46]의 연구는 이러한 동역학을 심층적으로 분석하였으며, 잊어버림의 기하급수적 경향이 사전 훈련과 지속적인 학습에서의 성능에 영향을 미친다고 보고하였다. 또한, 훈련 단계와 습득한 지식의 잊어버림 사이에는 거듭제곱 법칙 관계가 존재하여, 모델의 크기와 훈련 조건이 암기 동역학에 미치는 영향을 이해하는 데 중요한 통찰을 제공한다.",
        "Summary_QnA": "인용 논문 제목 (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n\n해당 논문은 대형 언어 모델의 사전 훈련 과정에서의 암기 및 잊어버림 동역학을 심층적으로 분석한 [44]와 [46]의 연구를 바탕으로, 훈련 단계와 습득한 사실 지식의 잊어버림 사이의 거듭제곱 법칙 관계를 밝혀내며 연구를 발전시켰다. 특히, 더 많은 데이터로 훈련하더라도 사실 지식의 습득 및 유지에 유의미한 개선이 없음을 발견하고, 훈련 조건에 따른 모델의 암기 및 잊어버림 행동을 설명함으로써, LLM의 성능 저하 원인과 개선 방안을 제시하였다. 이러한 통찰은 LLM의 훈련 및 활용에 있어 중요한 기초 자료로 작용할 수 있다."
    },
    "41": {
        "Title": "Are emergent abilities of large language models a mirage?",
        "Authors": "Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo",
        "Counter": 10,
        "Context": [
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.",
            "Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold.",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8]."
        ],
        "abstract": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.",
        "pdf_url": "http://arxiv.org/pdf/2304.15004v2",
        "Questions": "1. To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.\n2. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold.\n3. ...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].\n4. ...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].\n5. ...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].",
        "Summary": "인용 논문 제목 (Title): Are emergent abilities of large language models a mirage?\n\n1. **질문 :** LLM의 사실적 지식 습득을 상세히 분석하기 위해, 우리는 로그 확률을 검토하여 모델의 상태를 평가합니다. \n   - **답변 :** LLM의 사실적 지식 습득을 분석하기 위해 로그 확률을 통해 모델의 상태를 평가하는 방법은 모델이 훈련 중에 얼마나 많은 지식을 습득했는지를 세밀하게 파악할 수 있게 해줍니다. 이는 모델의 출력에서 지식의 축적 정도를 정량적으로 측정할 수 있는 기초를 제공합니다.\n   - **근거 :** \"To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.\"\n\n2. **질문 :** 대부분의 잘 알려진 사실은 학습 가능성 임계값보다 짧은 훈련 단계 간격으로 모델에 제시될 가능성이 높습니다.\n   - **답변 :** 잘 알려진 사실들은 모델이 학습할 수 있는 임계값보다 짧은 간격으로 제공되기 때문에, 모델이 이러한 사실들을 더 쉽게 습득할 수 있습니다. 이는 모델의 성능 향상에 기여합니다.\n   - **근거 :** \"Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold.\"\n\n3. **질문 :** 지식의 누적 로그 확률이 모델의 디코딩 출력으로 지식을 생성할 만큼 충분히 높을 것입니다.\n   - **답변 :** 모델의 지식이 디코딩 출력으로 생성되기 위해서는 지식의 누적 로그 확률이 충분히 높아야 하며, 이는 모델이 훈련 중에 지식을 효과적으로 축적하고 있음을 나타냅니다.\n   - **근거 :** \"...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model.\"\n\n4. **질문 :** 이러한 지식의 습득은 모델의 상위 k 출력 시퀀스 생성에서 상대적으로 초기 훈련 단계에 반영될 것입니다.\n   - **답변 :** 모델이 지식을 습득하는 과정은 초기 훈련 단계에서 상위 k 출력 시퀀스 생성에 반영되며, 이는 모델이 훈련 초기에 이미 특정 지식을 갖추고 있음을 시사합니다.\n   - **근거 :** \"...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage.\"\n\n5. **질문 :** 이러한 지식의 습득은 모델의 상위 k 출력 시퀀스 생성에서 상대적으로 초기 훈련 단계에 반영될 것입니다.\n   - **답변 :** 이 질문은 4번 질문과 유사하며, 모델의 초기 훈련 단계에서 지식의 습득이 상위 k 출력 시퀀스 생성에 반영된다는 점을 강조합니다.\n   - **근거 :** \"...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage.\"\n\n### 답변 요약 (Summary of Answers)\n이 논문은 대형 언어 모델(LLM)의 사실적 지식 습득 과정을 분석하며, 로그 확률을 통해 모델의 상태를 평가하는 방법을 제시합니다. 잘 알려진 사실들은 학습 가능성 임계값보다 짧은 훈련 단계 간격으로 모델에 제공되어, 모델이 이러한 사실들을 쉽게 습득할 수 있도록 합니다. 모델의 지식이 디코딩 출력으로 생성되기 위해서는 지식의 누적 로그 확률이 충분히 높아야 하며, 이는 모델이 훈련 중에 효과적으로 지식을 축적하고 있음을 나타냅니다. 또한, 이러한 지식의 습득은 모델의 초기 훈련 단계에서 상위 k 출력 시퀀스 생성에 반영되어, 모델이 훈련 초기에 이미 특정 지식을 갖추고 있음을 시사합니다. 이러한 분석은 LLM의 emergent abilities에 대한 기존의 주장을 재검토하는 데 중요한 기초 자료를 제공합니다.",
        "Summary_QnA": "인용 논문 제목 (Title): Are emergent abilities of large language models a mirage?\n\n해당 논문은 대형 언어 모델(LLM)의 사실적 지식 습득 과정을 심층적으로 분석하며, 로그 확률을 통해 모델의 상태를 평가하는 방법을 제시합니다. 이는 LLM이 훈련 중 얼마나 많은 지식을 축적했는지를 정량적으로 측정할 수 있는 기초를 제공합니다. 또한, 잘 알려진 사실들이 짧은 훈련 단계 간격으로 제공될 때 모델이 더 쉽게 습득할 수 있음을 강조하며, 이러한 지식의 누적 로그 확률이 디코딩 출력에 반영된다는 점을 통해 초기 훈련 단계에서의 지식 습득을 시사합니다. 이러한 통찰은 LLM의 emergent abilities에 대한 기존 주장을 재검토하는 데 중요한 기초 자료를 제공합니다."
    },
    "52": {
        "Title": "To repeat or not to repeat: Insights from scaling llm under token-crisis",
        "Authors": "Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You",
        "Counter": 10,
        "Context": [
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "The models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts.",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]."
        ],
        "abstract": "Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.",
        "pdf_url": "http://arxiv.org/pdf/2305.13230v2",
        "Questions": "1. The models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts.  \n2. ...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].  \n3. ...the importance of dataset deduplication [29, 52] can be explained.  ",
        "Summary": "인용 논문 제목 (Title): To repeat or not to repeat: Insights from scaling llm under token-crisis\n\n1. **질문 :** 모델은 중복된 텍스트를 제시할 때 사실적 지식을 일반화하는 데 있어 더 빠른 망각을 보인다.  \n   - **답변 :** 중복된 데이터를 사용하여 훈련할 경우, 모델은 과적합에 취약해지며, 이는 다중 에폭 훈련 시 성능 저하로 이어진다. 특히, 큰 모델일수록 이러한 경향이 두드러진다.  \n   - **근거 :** \"Larger models are more prone to overfitting when training with repeated data.\"\n\n2. **질문 :** 사전 훈련 데이터의 중복 제거가 모델 성능 향상에 중요한 요소라는 것이 널리 관찰된다.  \n   - **답변 :** 사전 훈련 데이터의 중복 제거는 모델의 성능을 향상시키는 중요한 요소로, 이는 데이터의 질과 양을 최적화하는 데 기여한다.  \n   - **근거 :** \"The importance of dataset deduplication can be explained.\"\n\n3. **질문 :** 데이터셋 중복 제거의 중요성은 어떻게 설명될 수 있는가?  \n   - **답변 :** 데이터셋 중복 제거는 모델이 다양한 정보를 학습할 수 있도록 하여, 과적합을 방지하고 일반화 능력을 향상시킨다. 이는 모델의 성능을 극대화하는 데 필수적이다.  \n   - **근거 :** \"Training LLM with a larger dataset for multiple epochs can alleviate the multi-epoch degradation.\"\n\n답변 요약: 이 논문에서는 대형 언어 모델(LLM)의 훈련에서 중복된 데이터를 사용하는 것이 과적합을 초래하고, 이는 모델의 성능 저하로 이어진다는 점을 강조한다. 특히, 큰 모델일수록 중복된 데이터로 인해 더 빠르게 망각하는 경향이 있다. 또한, 사전 훈련 데이터의 중복 제거는 모델 성능 향상에 중요한 요소로 작용하며, 이는 데이터의 질과 양을 최적화하는 데 기여한다. 데이터셋 중복 제거는 모델이 다양한 정보를 학습할 수 있도록 하여 과적합을 방지하고 일반화 능력을 향상시키는 데 필수적이다. 이러한 통찰은 LLM의 효율적인 개발과 성능 향상에 중요한 기초를 제공한다.",
        "Summary_QnA": "인용 논문 제목 (Title): To repeat or not to repeat: Insights from scaling llm under token-crisis\n\n해당 논문은 대형 언어 모델(LLM)의 사전 훈련에서 중복된 데이터 사용이 과적합을 초래하고, 이는 모델의 성능 저하로 이어진다는 점을 강조하며, 중복 제거의 중요성을 설명한다. 이 연구는 LLM의 사실적 지식 습득 과정에서 중복된 데이터가 망각을 가속화하고, 더 큰 배치 크기가 망각에 대한 저항력을 높인다는 점을 밝혀내어, 중복 제거가 모델의 일반화 능력을 향상시키는 데 필수적임을 입증한다. 이러한 통찰은 LLM의 효율적인 개발과 성능 향상에 중요한 기초를 제공하며, 사전 훈련 데이터의 질과 양을 최적화하는 방향으로 연구를 발전시킨다."
    },
    "9": {
        "Title": "Language models are few-shot learners",
        "Authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
        "Counter": 9,
        "Context": [
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "...as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ],
        "abstract": "CLIP has shown a remarkable zero-shot capability on a wide range of vision\ntasks. Previously, CLIP is only regarded as a powerful visual encoder. However,\nafter being pre-trained by language supervision from a large amount of\nimage-caption pairs, CLIP itself should also have acquired some few-shot\nabilities for vision-language tasks. In this work, we empirically show that\nCLIP can be a strong vision-language few-shot learner by leveraging the power\nof language. We first evaluate CLIP's zero-shot performance on a typical visual\nquestion answering task and demonstrate a zero-shot cross-modality transfer\ncapability of CLIP on the visual entailment task. Then we propose a\nparameter-efficient fine-tuning strategy to boost the few-shot performance on\nthe vqa task. We achieve competitive zero/few-shot results on the visual\nquestion answering and visual entailment tasks without introducing any\nadditional pre-training procedure.",
        "pdf_url": "http://arxiv.org/pdf/2203.07190v1",
        "Questions": "1. Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].\n2. Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].\n3. Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
        "Summary": "인용 논문 제목 (Title): Language models are few-shot learners\n\n1. **질문 :** 최근의 사전 훈련 코퍼스는 철저하게 중복 제거되었는가?\n   - **답변 :** 최근의 사전 훈련 코퍼스는 중복 제거가 철저하게 이루어졌으며, 이는 모델 성능을 향상시키는 데 기여하는 것으로 널리 관찰되고 있다. 데이터 중복 제거는 모델이 더 다양한 정보를 학습할 수 있도록 하여 성능을 높이는 데 중요한 역할을 한다.\n   - **근거 :** \"Recent pretraining corpora are thoroughly deduplicated, as it is widely observed that data deduplication can improve model performance.\"\n\n2. **질문 :** 최근 LLM(대형 언어 모델)에 대한 관심이 급증하고 있는가?\n   - **답변 :** 최근 LLM에 대한 관심이 급증하고 있으며, 이는 다양한 비전-언어 작업에서의 성능 향상과 관련이 있다. LLM은 자연어 처리 및 비전-언어 이해 작업에서 강력한 성능을 보여주고 있다.\n   - **근거 :** \"Recently, there has been a surge in interest in LLMs.\"\n\n3. **질문 :** 최근의 사전 훈련 코퍼스는 철저하게 중복 제거되었는가?\n   - **답변 :** 최근의 사전 훈련 코퍼스는 중복 제거가 철저하게 이루어졌으며, 이는 모델 성능을 향상시키는 데 기여하는 것으로 널리 관찰되고 있다. 데이터 중복 제거는 모델이 더 다양한 정보를 학습할 수 있도록 하여 성능을 높이는 데 중요한 역할을 한다.\n   - **근거 :** \"Recent pretraining corpora are thoroughly deduplicated, as it is widely observed that data deduplication can improve model performance.\"\n\n답변 요약 \n: 최근의 사전 훈련 코퍼스는 철저하게 중복 제거되어 있으며, 이는 모델 성능 향상에 기여하는 것으로 관찰되고 있다. 데이터 중복 제거는 모델이 다양한 정보를 학습할 수 있도록 도와주며, 이는 LLM에 대한 관심이 급증하는 배경이 되고 있다. LLM은 비전-언어 작업에서 강력한 성능을 보여주고 있으며, 이러한 경향은 앞으로도 계속될 것으로 예상된다.",
        "Summary_QnA": "인용 논문 제목 (Title): Language models are few-shot learners  \n해당 논문은 LLM의 사전 훈련 과정에서 데이터 중복 제거의 중요성을 강조하며, 중복 제거가 모델 성능 향상에 기여한다는 점을 명확히 하고 있다. 본 연구는 이러한 관찰을 바탕으로, LLM이 사실적 지식을 습득하는 과정에서 중복된 데이터가 빠른 망각을 초래한다는 사실을 밝혀내어, 데이터의 다양성이 모델의 지식 유지에 미치는 영향을 심층적으로 분석하였다. 이로 인해 LLM의 성능 향상과 관련된 메커니즘에 대한 이해가 더욱 깊어졌으며, 향후 연구 방향에 대한 중요한 통찰을 제공하고 있다."
    },
    "36": {
        "Title": "Language models as knowledge bases?",
        "Authors": "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller",
        "Counter": 9,
        "Context": [
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]."
        ],
        "abstract": "Large language models (LLMs) outperform information retrieval techniques for\ndownstream knowledge-intensive tasks when being prompted to generate world\nknowledge. However, community concerns abound regarding the factuality and\npotential implications of using this uncensored knowledge. In light of this, we\nintroduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to\nsystematically and automatically evaluate generated knowledge from six\nimportant perspectives -- Factuality, Relevance, Coherence, Informativeness,\nHelpfulness and Validity. We conduct an extensive empirical analysis of the\ngenerated knowledge from three different types of LLMs on two widely studied\nknowledge-intensive tasks, i.e., open-domain question answering and\nknowledge-grounded dialogue. Surprisingly, our study reveals that the\nfactuality of generated knowledge, even if lower, does not significantly hinder\ndownstream tasks. Instead, the relevance and coherence of the outputs are more\nimportant than small factual mistakes. Further, we show how to use CONNER to\nimprove knowledge-intensive tasks by designing two strategies: Prompt\nEngineering and Knowledge Selection. Our evaluation code and LLM-generated\nknowledge with human annotations will be released to facilitate future\nresearch.",
        "pdf_url": "http://arxiv.org/pdf/2310.07289v1",
        "Questions": "1. Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].\n2. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
        "Summary": "인용 논문 제목 (Title): Language models as knowledge bases?\n\n1. **질문 :** 최근 연구들은 LLM이 사전 훈련 데이터에서 상당한 사실적 지식을 포착할 수 있음을 보여주었다. \n   - **답변 :** 최근 연구들은 대형 언어 모델(LLM)이 사전 훈련 데이터에서 상당한 사실적 지식을 포착할 수 있는 능력을 보여주었다. LLM은 정보 검색 기술보다 지식 집약적인 작업에서 더 나은 성능을 발휘하며, 생성된 지식의 사실성은 낮더라도 하위 작업에 미치는 영향이 크지 않다는 결과가 나타났다. \n   - **근거 :** \"LLM-generated knowledge surpasses retrieved knowledge in most evaluation perspectives, while it actually suffers from the factuality issue as expected.\"\n\n2. **질문 :** LLM의 매개변수에 인코딩된 지식에 대한 광범위한 연구가 진행되었다. \n   - **답변 :** LLM의 매개변수에 인코딩된 지식에 대한 연구는 LLM이 생성하는 지식의 질과 신뢰성을 평가하는 데 중점을 두고 있다. CONNER 프레임워크를 통해 LLM이 생성한 지식의 내재적 품질과 외재적 신뢰성을 평가할 수 있으며, 이는 LLM의 성능 향상에 기여할 수 있다. \n   - **근거 :** \"We conduct the first empirical analysis focusing on both intrinsic quality and extrinsic reliability of the generated knowledge from LLMs.\"\n\n답변 요약 \n: 최근 연구들은 대형 언어 모델(LLM)이 사전 훈련 데이터에서 상당한 사실적 지식을 포착할 수 있는 능력을 보여주며, LLM이 생성한 지식은 정보 검색 기술보다 더 나은 성능을 발휘한다. 그러나 생성된 지식의 사실성 문제는 여전히 존재하며, 이는 하위 작업에 미치는 영향이 크지 않다는 점이 강조된다. 또한, LLM의 매개변수에 인코딩된 지식에 대한 연구는 LLM의 성능 향상에 기여할 수 있는 중요한 요소로, CONNER 프레임워크를 통해 LLM이 생성한 지식의 질과 신뢰성을 평가하는 방법이 제안되었다. 이러한 연구 결과는 LLM을 지식 생성기로 활용하는 데 있어 중요한 통찰을 제공한다.",
        "Summary_QnA": "인용 논문 제목 (Title): Language models as knowledge bases?\n\n해당 논문은 대형 언어 모델(LLM)이 사전 훈련 데이터에서 사실적 지식을 어떻게 획득하는지를 탐구하며, LLM의 지식 생성 능력과 그 신뢰성에 대한 기존 연구를 발전시켰다. 특히, LLM이 정보 검색 기술보다 더 나은 성능을 발휘하지만, 생성된 지식의 사실성 문제는 여전히 존재한다는 점을 강조하였다. 또한, LLM의 매개변수에 인코딩된 지식의 질과 신뢰성을 평가하는 CONNER 프레임워크를 제안함으로써, LLM의 성능 향상에 기여할 수 있는 새로운 분석 방법론을 제공하였다. 이러한 통찰은 LLM을 지식 생성기로 활용하는 데 중요한 기초 자료가 된다."
    },
    "5": {
        "Title": "Physics of language models: Part 3.2, knowledge manipulation",
        "Authors": "Zeyuan Allen-Zhu, Yuanzhi Li",
        "Counter": 8,
        "Context": [
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).",
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.",
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3)."
        ],
        "abstract": "Language models can store vast factual knowledge, yet their ability to\nflexibly use this knowledge for downstream tasks (e.g., via instruction\nfinetuning) remains questionable. This paper investigates four fundamental\nknowledge manipulation tasks: retrieval (e.g., \"What is person A's attribute\nX?\"), classification (e.g., \"Is A's attribute X even or odd?\"), comparison\n(e.g., \"Is A greater than B in attribute X?\"), and inverse search (e.g., \"Which\nperson's attribute X equals T?\").\n  We show that language models excel in knowledge retrieval but struggle even\nin the simplest classification or comparison tasks unless Chain of Thoughts\n(CoTs) are employed during both training and inference. Moreover, their\nperformance in inverse knowledge search is virtually 0%, regardless of the\nprompts. Our primary contribution is a controlled, synthetic experiment that\nconfirms these weaknesses are inherent to language models: they cannot\nefficiently manipulate knowledge from pre-training data, even when such\nknowledge is perfectly stored in the models, despite adequate training and\nsufficient model size. Our findings also apply to modern pretrained language\nmodels such as GPT-4, thus giving rise to many Turing tests to distinguish\nHumans from contemporary AIs.",
        "pdf_url": "http://arxiv.org/pdf/2309.14402v2",
        "Questions": "1. In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).\n2. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.",
        "Summary": "인용 논문 제목 (Title): Physics of language models: Part 3.2, knowledge manipulation\n\n1. **질문 :** 모든 주입 시나리오에서 모델 크기가 1B에서 7B로 확장될 때 효과성이 향상되는가?\n   - **답변 :** 모델 크기를 1B에서 7B로 확장할 때 효과성이 향상된다는 관찰은 사실이다. 이는 모델이 더 많은 매개변수를 가질수록 더 많은 지식을 저장하고 활용할 수 있는 능력이 증가함을 나타낸다. 그러나 이러한 확장은 지식 조작의 효율성을 보장하지는 않는다.\n   - **근거 :** \"We discover that language models, through controlled experiments and pre-trained on synthetic data, perform poorly at basic knowledge manipulation tasks.\"\n\n2. **질문 :** 이 관찰은 사실적 지식 습득 메커니즘을 직접적으로 보여주는가?\n   - **답변 :** 이 관찰은 사실적 지식 습득 메커니즘을 보여준다. 대형 언어 모델(LLM)은 사전 훈련 중 지식을 접할 때마다 미세한 습득을 통해 사실적 지식을 축적하지만, 이후 잊어버리는 과정을 겪는다. 이는 LLM이 지식을 효과적으로 조작하는 데 어려움을 겪는 이유 중 하나이다.\n   - **근거 :** \"LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\"\n\n답변 요약 \n: 이 논문에서는 대형 언어 모델의 지식 조작 능력에 대한 연구 결과를 제시하고 있다. 모델 크기를 1B에서 7B로 확장할 때 효과성이 향상되지만, 이는 지식 조작의 효율성을 보장하지 않는다. LLM은 사전 훈련 중 지식을 축적하지만, 이 과정에서 잊어버리는 경향이 있어 지식 조작에 어려움을 겪는다. 이러한 결과는 LLM의 지식 습득 메커니즘을 이해하는 데 중요한 통찰을 제공하며, 향후 모델 개선을 위한 기초 자료로 활용될 수 있다.",
        "Summary_QnA": "인용 논문 제목 (Title): Physics of language models: Part 3.2, knowledge manipulation\n\n해당 논문은 대형 언어 모델(LLM)의 사실적 지식 습득 메커니즘을 심층적으로 탐구하며, 모델 크기 확장과 지식 조작의 효율성 간의 관계를 분석하였다. 연구 결과, 모델 크기를 1B에서 7B로 확장할 때 효과성이 향상되지만, 이는 지식 조작의 효율성을 보장하지 않는다는 점을 강조한다. 또한, LLM이 사전 훈련 중 지식을 축적하면서도 잊어버리는 경향이 있음을 밝혀내어, 지식 조작의 어려움을 설명한다. 이러한 통찰은 LLM의 지식 습득 과정을 이해하는 데 기여하며, 향후 모델 개선을 위한 중요한 기초 자료로 활용될 수 있다."
    },
    "33": {
        "Title": "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
        "Authors": "Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang",
        "Counter": 8,
        "Context": [
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including task performances in continual learning.",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "Training steps and the forgetting of acquired factual knowledge have a power-law relationship."
        ],
        "abstract": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge for achieving a satisfactory performance in downstream tasks. As\nlarge language models (LLMs) have demonstrated remarkable performance, it is\nintriguing to investigate whether CF exists during the continual instruction\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\nLLMs' knowledge during continual instruction tuning from the perspectives of\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\n7b parameters. Surprisingly, as the model scale increases, the severity of\nforgetting intensifies in such a model sale range which may result from the\nmuch significant initial performance in the larger LLM. Comparing the\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\nless forgetting and retains more knowledge. Interestingly, we also observe that\nLLMs can mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\nfine-tuning.",
        "pdf_url": "http://arxiv.org/pdf/2308.08747v5",
        "Questions": "1. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].\n2. Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].\n3. The exponential trend of forgetting has been reported in various aspects of LLM training, including task performances in continual learning.\n4. Training steps and the forgetting of acquired factual knowledge have a power-law relationship.",
        "Summary": "인용 논문 제목 (Title): An empirical study of catastrophic forgetting in large language models during continual fine-tuning\n\n1. **질문 :** 다양한 LLM 훈련의 측면에서 기억 상실의 기하급수적 경향이 보고되었는데, 여기에는 사전 훈련에서의 암기와 지속적 학습에서의 작업 성능이 포함된다. \n   - **답변 :** LLM 훈련에서 기억 상실의 기하급수적 경향은 사전 훈련 중 암기와 지속적 학습에서의 작업 성능 모두에서 관찰되었다. 이는 모델이 새로운 작업을 학습하는 과정에서 이전에 학습한 정보를 잃어버리는 현상으로, 특히 지속적 훈련 중에 더욱 두드러진다.\n   - **근거 :** \"Catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters.\"\n\n2. **질문 :** 여러 연구가 LLM의 훈련 역학을 조사했으며, 특히 훈련 중 어떻게 발전하는지를 다루었다. \n   - **답변 :** LLM의 훈련 역학에 대한 연구는 모델이 훈련 중 어떻게 변화하는지를 분석하며, 이는 모델의 성능과 기억 상실 현상에 대한 이해를 돕는다. 이러한 연구들은 LLM의 지속적 훈련 과정에서 발생하는 다양한 문제를 해결하기 위한 기초 자료를 제공한다.\n   - **근거 :** \"Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training.\"\n\n3. **질문 :** 지속적 학습에서 작업 성능을 포함한 LLM 훈련의 다양한 측면에서 기억 상실의 기하급수적 경향이 보고되었다. \n   - **답변 :** LLM 훈련에서 기억 상실의 기하급수적 경향은 지속적 학습에서의 작업 성능 저하와 관련이 있으며, 이는 모델이 새로운 작업을 학습하면서 이전의 지식을 잃어버리는 현상을 나타낸다.\n   - **근거 :** \"The forgetting problem is generally present in LLMs.\"\n\n4. **질문 :** 훈련 단계와 습득한 사실적 지식의 기억 상실은 거듭제곱 법칙 관계가 있다. \n   - **답변 :** 훈련 단계와 기억 상실 간의 관계는 거듭제곱 법칙으로 설명될 수 있으며, 이는 훈련이 진행될수록 모델이 이전에 학습한 지식을 잃어버리는 경향이 강해짐을 나타낸다.\n   - **근거 :** \"The severity of forgetting intensifies as the model scale increases.\"\n\n답변 요약 \n: 이 연구는 LLM의 지속적 훈련 중 발생하는 기억 상실 현상에 대한 실증적 분석을 제공한다. 연구 결과, LLM은 훈련 단계가 증가함에 따라 기억 상실이 심화되며, 특히 모델의 크기가 클수록 이러한 경향이 두드러진다. 또한, LLM의 아키텍처에 따라 기억 상실의 정도가 달라지며, 디코더 전용 모델이 인코더-디코더 모델보다 더 나은 기억 유지 능력을 보인다. 일반적인 지침 조정이 기억 상실 문제를 완화하는 데 도움이 될 수 있다는 점도 강조되었다. 이러한 결과는 LLM의 지속적 훈련에서 기억 상실 문제를 해결하기 위한 향후 연구 방향을 제시한다.",
        "Summary_QnA": "인용 논문 제목 (Title): An empirical study of catastrophic forgetting in large language models during continual fine-tuning\n\n해당 논문은 LLM의 사전 훈련 중 사실적 지식의 습득과 기억 상실 현상 간의 관계를 탐구하며, 기억 상실이 훈련 단계와 관련이 있음을 강조한다. 특히, 훈련 단계가 증가할수록 기억 상실이 심화된다는 점은 인용 논문에서 보고된 기하급수적 경향과 일치한다. 또한, LLM의 아키텍처에 따른 기억 유지 능력의 차이를 분석함으로써, 지속적 학습에서의 기억 상실 문제를 해결하기 위한 기초 자료를 제공하고, 향후 연구 방향을 제시하는 데 기여한다."
    },
    "18": {
        "Title": "Does fine-tuning llms on new knowledge encourage hallucinations?",
        "Authors": "Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig",
        "Counter": 7,
        "Context": [
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "Recently, [53] explored the relationship between the data size and grokking [37].",
            "Knowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)",
            "The results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge.",
            "However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining.",
            "We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability."
        ],
        "abstract": "When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.",
        "pdf_url": "http://arxiv.org/pdf/2405.05904v3",
        "Questions": "1. [44] and [46] focused on the dynamics of memorization in language model pretraining.\n2. Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].\n3. Recently, [53] explored the relationship between the data size and grokking [37].\n4. Knowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)\n5. The results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge.\n6. However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining.\n7. We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.",
        "Summary": "인용 논문 제목 (Title): Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\n\n1. **질문 :** [44]와 [46]은 언어 모델 사전 훈련에서 기억의 동역학에 초점을 맞췄다.\n   - **답변 :** 이 연구는 언어 모델의 사전 훈련 과정에서 기억의 동역학을 분석하며, 모델이 어떻게 정보를 기억하고 잊는지를 탐구한다. 특히, 사전 훈련 중에 모델이 학습한 지식의 양과 질이 모델의 성능에 미치는 영향을 강조한다.\n   - **근거 :** \"We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization...\"\n\n2. **질문 :** 여러 연구들이 LLM의 훈련 동역학을 조사했으며, 특히 훈련 중 어떻게 진화하는지를 다루었다 [12, 18, 22, 32, 33, 45, 51].\n   - **답변 :** LLM의 훈련 동역학에 대한 연구는 모델이 훈련 중에 어떻게 지식을 습득하고 활용하는지를 분석하며, 이는 모델의 성능 향상에 중요한 요소로 작용한다.\n   - **근거 :** \"We study how learning new factual knowledge through fine-tuning impacts the model’s tendency to hallucinate...\"\n\n3. **질문 :** 최근 [53]은 데이터 크기와 grokking 간의 관계를 탐구했다 [37].\n   - **답변 :** 데이터 크기와 grokking의 관계를 탐구하는 연구는 모델이 얼마나 많은 데이터를 통해 지식을 효과적으로 습득하는지를 분석하며, 이는 모델의 일반화 능력에 영향을 미친다.\n   - **근거 :** \"We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure...\"\n\n4. **질문 :** 사전 훈련 중 지식 주입에 대해 논의한다.\n   - **답변 :** LLM이 사전 훈련 중에 새로운 사실적 지식을 어떻게 습득하고 유지하는지를 분석하며, 이는 모델의 기억력과 일반화 능력에 중요한 영향을 미친다.\n   - **근거 :** \"We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization...\"\n\n5. **질문 :** 작은 배치 크기로 사전 훈련할 경우 학습 가능한 지식의 양이 줄어들고, 이는 사실적 지식의 조합 일반화 성능을 저하시킨다.\n   - **답변 :** 작은 배치 크기로 훈련할 경우 모델이 지식을 잊는 속도가 빨라져, 결과적으로 학습한 지식의 활용도가 떨어진다.\n   - **근거 :** \"The results suggest that pretraining with a small batch size reduces the set of learnable knowledge...\"\n\n6. **질문 :** 그러나, 더 큰 모델의 경우 지식 관찰 후 로그 확률의 즉각적인 개선량은 증가하지만, 사전 훈련 진행 중에는 크게 증가하지 않는다.\n   - **답변 :** 모델의 크기가 커질수록 새로운 지식을 관찰한 후의 성능 개선이 있지만, 이는 사전 훈련의 진행에 따라 크게 변화하지 않는다.\n   - **근거 :** \"the amount does not significantly increase throughout the progress of pretraining...\"\n\n7. **질문 :** LLM이 비인기 지식을 습득하는 데 어려움을 겪는 이유는 무엇인가?\n   - **답변 :** LLM은 비인기 지식을 습득하기 위해서는 충분한 노출이 필요하며, 이는 학습 가능성의 임계값보다 짧은 간격으로 이루어져야 한다.\n   - **근거 :** \"we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure...\"\n\n답변 요약 \n: 이 연구는 LLM의 사전 훈련과 새로운 지식 주입이 모델의 기억력과 일반화 능력에 미치는 영향을 분석한다. 연구 결과, LLM은 새로운 사실적 지식을 습득하는 데 어려움을 겪으며, 이는 모델의 성능 저하와 관련이 있다. 특히, 작은 배치 크기로 훈련할 경우 학습 가능한 지식의 양이 줄어들고, 이는 사실적 지식의 조합 일반화 성능을 저하시킨다. 또한, 비인기 지식의 습득은 충분한 노출이 필요하며, 이는 모델의 학습 가능성에 영향을 미친다. 이러한 결과는 LLM의 지식 습득 및 활용 방식에 대한 중요한 통찰을 제공한다.",
        "Summary_QnA": "인용 논문 제목 (Title): <<Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?>>\n\n해당 논문은 LLM의 사전 훈련 과정에서 기억의 동역학과 새로운 지식의 주입이 모델의 성능에 미치는 영향을 심층적으로 분석함으로써, LLM이 사실적 지식을 습득하고 유지하는 방식에 대한 이해를 발전시켰다. 특히, 작은 배치 크기로 훈련할 경우 지식의 잊힘 속도가 빨라지고, 비인기 지식의 습득에는 충분한 노출이 필요하다는 점을 강조하여, LLM의 일반화 능력과 기억력 간의 복잡한 상관관계를 명확히 했다. 이러한 통찰은 LLM의 훈련 전략을 개선하고, 모델의 성능을 최적화하는 데 기여할 수 있다."
    },
    "26": {
        "Title": "Large language models struggle to learn long-tail knowledge",
        "Authors": "Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, Colin Raffel",
        "Counter": 7,
        "Context": [
            "...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication...",
            "...the failure to acquire long-tail knowledge [26, 34]...",
            "...the failure to acquire long-tail knowledge [26, 34]...",
            "...the failure to acquire long-tail knowledge [26, 34]...",
            "However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].",
            "...the failure to acquire long-tail knowledge [26, 34]...",
            "However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]."
        ],
        "abstract": "The Internet contains a wealth of knowledge -- from the birthdays of\nhistorical figures to tutorials on how to code -- all of which may be learned\nby language models. However, while certain pieces of information are ubiquitous\non the web, others appear extremely rarely. In this paper, we study the\nrelationship between the knowledge memorized by large language models and the\ninformation in pre-training datasets scraped from the web. In particular, we\nshow that a language model's ability to answer a fact-based question relates to\nhow many documents associated with that question were seen during pre-training.\nWe identify these relevant documents by entity linking pre-training datasets\nand counting documents that contain the same entities as a given\nquestion-answer pair. Our results demonstrate strong correlational and causal\nrelationships between accuracy and relevant document count for numerous\nquestion answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,\nROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models\nare better at learning long-tail knowledge, we estimate that today's models\nmust be scaled by many orders of magnitude to reach competitive QA performance\non questions with little support in the pre-training data. Finally, we show\nthat retrieval-augmentation can reduce the dependence on relevant pre-training\ninformation, presenting a promising approach for capturing the long-tail.",
        "pdf_url": "http://arxiv.org/pdf/2211.08411v2",
        "Questions": "1. ...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication...\n2. However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].",
        "Summary": "인용 논문 제목 (Title): Large language models struggle to learn long-tail knowledge\n\n1. **질문 :** ...롱테일 지식을 습득하지 못하는 실패와 데이터셋 중복 제거의 중요성에 대해 설명해 주세요.\n   - **답변 :** 대형 언어 모델(LLM)은 인터넷에서 방대한 양의 지식을 학습하지만, 특정 정보는 드물게 나타나며, 이러한 드문 정보를 '롱테일 지식'이라고 합니다. 연구에 따르면, LLM은 이러한 롱테일 지식을 효과적으로 습득하지 못하며, 이는 모델의 성능에 부정적인 영향을 미칩니다. 데이터셋 중복 제거는 이러한 문제를 완화할 수 있는 방법 중 하나로, 중복된 데이터를 제거함으로써 모델이 더 다양한 정보를 학습할 수 있도록 도와줍니다.\n   - **근거 :** \"Moreover, while larger models are better at learning long-tail knowledge, we estimate that today’s models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data.\"\n\n2. **질문 :** 최근 LLM에 대한 연구에서 LLM이 롱테일 지식을 잘 습득하지 못한다는 사실이 밝혀졌습니다. 이에 대한 설명을 해주세요.\n   - **답변 :** LLM은 훈련 데이터에서 자주 나타나는 정보에 대해서는 높은 정확도를 보이지만, 드물게 나타나는 정보에 대해서는 성능이 저하됩니다. 연구 결과, LLM의 질문 응답 능력은 훈련 데이터에서 관련 문서의 수와 강한 상관관계를 가지며, 관련 문서가 적을수록 정확도가 낮아지는 경향이 있습니다. 이는 LLM이 롱테일 지식을 효과적으로 학습하지 못함을 나타냅니다.\n   - **근거 :** \"We observe a strong correlation between an LM’s ability to answer a question and the number of pre-training documents relevant to that question.\"\n\n답변 요약 \n: 대형 언어 모델(LLM)은 방대한 양의 지식을 학습할 수 있지만, 드물게 나타나는 롱테일 지식에 대한 습득이 부족합니다. 이는 모델의 성능에 부정적인 영향을 미치며, 데이터셋 중복 제거가 이러한 문제를 완화할 수 있는 방법으로 제안됩니다. 연구에 따르면, LLM의 질문 응답 능력은 훈련 데이터에서 관련 문서의 수와 강한 상관관계를 가지며, 관련 문서가 적을수록 정확도가 낮아지는 경향이 있습니다. 따라서, LLM의 성능을 향상시키기 위해서는 롱테일 지식의 습득을 개선할 필요가 있으며, 이는 모델의 크기나 데이터셋의 다양성을 증가시키는 것만으로는 충분하지 않을 수 있습니다.",
        "Summary_QnA": "인용 논문 제목 (Title): Large language models struggle to learn long-tail knowledge\n\n해당 논문은 대형 언어 모델(LLM)이 롱테일 지식을 효과적으로 습득하지 못하는 문제를 다루며, 데이터셋 중복 제거의 중요성을 강조합니다. 본 연구는 LLM의 사실적 지식 습득 메커니즘을 분석하여, 훈련 데이터의 중복이 모델의 기억력과 일반화 능력에 미치는 영향을 밝혀냈습니다. 특히, 중복된 데이터로 훈련된 LLM이 더 빠르게 잊어버리는 경향이 있음을 보여주며, 이는 롱테일 지식 습득의 어려움과 관련이 있습니다. 따라서, 이 연구는 롱테일 지식의 습득을 개선하기 위한 데이터셋의 다양성과 중복 제거의 필요성을 뒷받침하는 중요한 통찰을 제공합니다."
    }
}