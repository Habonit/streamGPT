{
    "27": {
        "Title": "Scaling laws for neural language models",
        "Authors": "Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei",
        "Counter": 14,
        "Context": [
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.",
            "The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.",
            "Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.",
            "This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.",
            "These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.",
            "Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.",
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in § 4.4.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in §4.3.",
            "Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times."
        ],
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "pdf_url": "http://arxiv.org/pdf/2001.08361v1",
        "Questions": "1. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.\n2. We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.\n3. The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\n4. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\n5. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.\n6. These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).\n7. The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\n8. There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\n9. Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.\n10. This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in § 4.4.\n11. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times.",
        "Summary": "이 논문에서는 대형 언어 모델(LLM)의 성능이 모델 크기와 사전 훈련 데이터의 크기에 따라 비례적으로 증가하는 스케일링 법칙을 다룹니다. 연구 결과에 따르면, LLM은 사실적 지식을 획득하는 과정에서 미세한 습득과 잊어버림을 반복하며, 이는 모든 사전 훈련 단계에서 일관된 패턴을 보입니다. 또한, 모델의 성능은 훈련 단계와 기억된 사실 지식의 망각 간의 파워-로 관계를 따르며, 이는 모델의 메모리와 일반화 능력에 영향을 미칩니다. 최적의 성능을 위해서는 모델 크기, 데이터 크기, 훈련 컴퓨팅 자원을 동시에 증가시켜야 하며, 대형 모델이 더 적은 샘플로도 높은 성능을 달성할 수 있음을 보여줍니다. 이러한 발견은 LLM의 행동을 이해하는 데 중요한 통찰을 제공하며, 향후 연구의 유망한 방향을 제시합니다."
    },
    "29": {
        "Title": "Deduplicating training data makes language models better",
        "Authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini",
        "Counter": 11,
        "Context": [
            "...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.",
            "the importance of dataset deduplication",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]."
        ],
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "pdf_url": "http://arxiv.org/pdf/2107.06499v2",
        "Questions": "1. The failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.\n2. Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.\n3. Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.",
        "Summary": "이 논문은 대규모 언어 모델(LLM)의 성능을 향상시키기 위해 훈련 데이터의 중복을 제거하는 방법을 제안합니다. 연구 결과, 중복된 데이터가 있는 훈련 세트에서 모델이 메모리된 텍스트를 자주 생성하는 경향이 있으며, 이는 모델의 일반화 능력을 저하시킵니다. 데이터 중복을 제거하면 모델이 중복된 시퀀스에 더 낮은 확률을 부여하고, 학습한 사실적 지식을 더 오래 유지할 수 있어 성능이 향상됩니다. 또한, 중복 제거된 데이터로 훈련된 모델은 더 많은 사실적 지식을 습득하고, 잊어버리는 경향이 줄어들어 더 강력한 성능을 보입니다. 따라서, 데이터 중복 제거는 LLM의 훈련 효율성을 높이고, 모델의 평가 정확성을 개선하는 데 기여합니다."
    },
    "46": {
        "Title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
        "Authors": "Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan",
        "Counter": 9,
        "Context": [
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]."
        ],
        "abstract": "Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.",
        "pdf_url": "http://arxiv.org/pdf/2205.10770v2",
        "Questions": "1. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.\n2. Training steps and the forgetting of acquired factual knowledge have a power-law relationship.\n3. The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
        "Summary": "이 논문은 대형 언어 모델(LLM)의 훈련 동역학을 분석하여 기억과 망각의 과정을 연구합니다. 연구에 따르면, LLM은 훈련 데이터의 기억 속도가 모델 크기에 비례하여 증가하며, 이는 과적합이 발생하기 전에 더 많은 데이터를 기억할 수 있음을 보여줍니다. 또한, 훈련 단계와 기억된 사실 지식의 망각은 파워 법칙 관계를 가지며, 망각 곡선은 모델 크기가 증가함에 따라 낮은 경계값을 가지게 됩니다. 다양한 훈련 조건에서 LLM의 기억과 망각 행동을 분석한 결과, 명사와 숫자가 다른 품사보다 더 빨리 기억되는 경향이 있음을 발견했습니다. 이러한 발견은 LLM의 훈련 동역학을 이해하는 데 중요한 통찰을 제공합니다."
    },
    "41": {
        "Title": "Are emergent abilities of large language models a mirage?",
        "Authors": "Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo",
        "Counter": 6,
        "Context": [
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.",
            "In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model.",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]."
        ],
        "abstract": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.",
        "pdf_url": "http://arxiv.org/pdf/2304.15004v2",
        "Questions": "1. To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.\n2. In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model.\n3. ...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].\n4. ...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41].",
        "Summary": "이 논문은 대형 언어 모델(LLM)의 이른바 \"출현 능력\"이 연구자의 선택한 메트릭에 의해 유도된 환상일 수 있음을 주장합니다. 연구자들이 비선형 또는 불연속적인 메트릭을 선택할 경우, 모델의 성능이 급격하고 예측 불가능하게 변화하는 것처럼 보일 수 있으며, 이는 모델의 실제 성능이 증가하는 동안 부드럽고 연속적으로 변화하는 것과 대조적입니다. 예를 들어, 특정 작업에서 LLM의 지식이 축적되면, 이 지식의 로그 확률이 증가하여 모델의 출력으로 생성될 수 있지만, 이는 메트릭의 선택에 따라 달라질 수 있습니다. 또한, 이 논문은 다양한 실험을 통해 메트릭을 변경하면 출현 능력이 사라지거나 새롭게 유도될 수 있음을 보여줍니다. 따라서, 출현 능력은 모델의 본질적인 특성이 아니라 연구자의 분석 방식에 의해 형성된 결과일 수 있다는 결론을 내립니다."
    },
    "52": {
        "Title": "To repeat or not to repeat: Insights from scaling llm under token-crisis",
        "Authors": "Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You",
        "Counter": 6,
        "Context": [
            "...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "the importance of dataset deduplication",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]."
        ],
        "abstract": "Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.",
        "pdf_url": "http://arxiv.org/pdf/2305.13230v2",
        "Questions": "1. ...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.\n2. ...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].\n3. the importance of dataset deduplication.",
        "Summary": "이 논문은 대규모 언어 모델(LLM)의 사전 훈련 데이터 반복 사용의 효과를 조사하며, 데이터 중복 제거의 중요성과 긴 꼬리 지식의 획득 실패를 설명합니다. 연구 결과, 사전 훈련 데이터의 중복 제거가 모델 성능 향상에 중요한 요소로 작용하며, 데이터 크기와 모델 파라미터가 다중 에폭 퇴화에 미치는 영향이 크다는 것을 발견했습니다. 특히, 데이터 품질은 상대적으로 덜 중요한 요소로 나타났습니다. 또한, 드롭아웃과 같은 정규화 기법이 다중 에폭 퇴화를 완화하는 데 효과적임을 보여주었으며, Mixture-of-Experts(MoE) 모델을 활용하여 효율적인 하이퍼파라미터 조정이 가능하다는 점도 강조했습니다. 이러한 통찰은 LLM 개발의 효율성을 높이는 데 기여할 수 있습니다."
    }
}