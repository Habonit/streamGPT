{
    "27": {
        "Title": "Scaling laws for neural language models",
        "Authors": "Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei",
        "Counter": 14,
        "Context": [
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.",
            "The measurement of the defined metrics are illustrated in Figure 1. For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.",
            "Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.",
            "This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.",
            "These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs.",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.",
            "Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.",
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in § 4.4.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in §4.3.",
            "Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times."
        ],
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "pdf_url": "http://arxiv.org/pdf/2001.08361v1"
    },
    "21": {
        "Title": "Olmo: Accelerating the science of language models",
        "Authors": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi",
        "Counter": 11,
        "Context": [
            "To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...",
            "Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing.",
            "To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained...",
            "These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1).",
            "It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [21].",
            "The results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge.",
            "To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43])...",
            "Next, we measure effectivity (Eq. 2) to quantify the improvement of the LLMs’ log probability after being trained with the injected knowledge, averaged across all probes (q) and encounters (i).",
            "It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing.",
            "Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size.",
            "This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold."
        ],
        "abstract": null,
        "pdf_url": null
    },
    "29": {
        "Title": "Deduplicating training data makes language models better",
        "Authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini",
        "Counter": 11,
        "Context": [
            "...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.",
            "the importance of dataset deduplication",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]."
        ],
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "pdf_url": "http://arxiv.org/pdf/2107.06499v2"
    },
    "46": {
        "Title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
        "Authors": "Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan",
        "Counter": 9,
        "Context": [
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]."
        ],
        "abstract": "Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.",
        "pdf_url": "http://arxiv.org/pdf/2205.10770v2"
    },
    "23": {
        "Title": "Training compute-optimal large language models",
        "Authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, L. Sifre",
        "Counter": 8,
        "Context": [
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "We use the window size tw = 50.23 Next, we define a metric to quantify the immediate improvement in the model’s log probability of factual knowledge after it is presented with the knowledge for the i-th time.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in §4.3.",
            "[23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.",
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.",
            "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in §4.3."
        ],
        "abstract": null,
        "pdf_url": null
    },
    "41": {
        "Title": "Are emergent abilities of large language models a mirage?",
        "Authors": "Rylan Schaeffer, Brando Miranda, Oluwasanmi Koyejo",
        "Counter": 6,
        "Context": [
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information.",
            "In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model.",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [41].",
            "To conduct a detailed analysis of the LLMs’ acquisition of factual knowledge during pretraining, we evaluate the model’s state by examining log probabilities to obtain fine-grained information [41].",
            "...the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]."
        ],
        "abstract": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.",
        "pdf_url": "http://arxiv.org/pdf/2304.15004v2"
    },
    "52": {
        "Title": "To repeat or not to repeat: Insights from scaling llm under token-crisis",
        "Authors": "Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You",
        "Counter": 6,
        "Context": [
            "...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "the importance of dataset deduplication",
            "it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52].",
            "...the importance of dataset deduplication [29, 52] can be explained.",
            "...it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]."
        ],
        "abstract": "Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.",
        "pdf_url": "http://arxiv.org/pdf/2305.13230v2"
    },
    "9": {
        "Title": "Language models are few-shot learners",
        "Authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
        "Counter": 5,
        "Context": [
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ]
    },
    "26": {
        "Title": "Large language models struggle to learn long-tail knowledge",
        "Authors": "Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, Colin Raffel",
        "Counter": 5,
        "Context": [
            "...the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication...",
            "the failure to acquire long-tail knowledge",
            "However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34].",
            "...the failure to acquire long-tail knowledge [26, 34]...",
            "However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]."
        ]
    },
    "36": {
        "Title": "Language models as knowledge bases?",
        "Authors": "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller",
        "Counter": 5,
        "Context": [
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]."
        ]
    },
    "40": {
        "Title": "How much knowledge can you pack into the parameters of a language model?",
        "Authors": "Adam Roberts, Colin Raffel, Noam M. Shazeer",
        "Counter": 5,
        "Context": [
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]."
        ]
    },
    "43": {
        "Title": "Dolma: An open corpus of three trillion tokens for language model pretraining research",
        "Authors": "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al.",
        "Counter": 5,
        "Context": [
            "...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...",
            "...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "...using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps...",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ]
    },
    "4": {
        "Title": "Physics of language models: Part 3.1, knowledge storage and extraction",
        "Authors": "Zeyuan Allen-Zhu, Yuanzhi Li",
        "Counter": 4,
        "Context": [
            "...which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].",
            "This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.",
            "...which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4].",
            "This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining."
        ]
    },
    "7": {
        "Title": "Emergent and predictable memorization in large language models",
        "Authors": "Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, Edward Raf",
        "Counter": 4,
        "Context": [
            "First, when the model is updated after seeing the factual knowledge, the most significant improvement in log probability is observed for memorization, followed by semantic generalization, and the least improvement is seen in compositional generalization.",
            "In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance.",
            "Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model’s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.",
            "This theoretical threshold may not be equal to the estimated x-intercepts presented in Figure 5, as we estimate the threshold based on the controlled experiment of injecting factual knowledge."
        ]
    },
    "10": {
        "Title": "Extracting training data from large language models",
        "Authors": "Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, Úlfar Erlingsson, Alina Oprea, Colin Raffel",
        "Counter": 4,
        "Context": [
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11].",
            "The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.",
            "LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11]."
        ]
    },
    "33": {
        "Title": "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
        "Authors": "Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yuechen Zhang",
        "Counter": 4,
        "Context": [
            "Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]."
        ]
    },
    "49": {
        "Title": "Llama 2: Open foundation and fine-tuned chat models",
        "Authors": "Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom",
        "Counter": 4,
        "Context": [
            "Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing.",
            "It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [49].",
            "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49].",
            "Pretraining with a larger batch size helps LLMs acquire more knowledge."
        ]
    },
    "2": {
        "Title": "Gpt-4 technical report",
        "Authors": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.",
        "Counter": 3,
        "Context": [
            "An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.",
            "An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.",
            "An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases."
        ]
    },
    "5": {
        "Title": "Physics of language models: Part 3.2, knowledge manipulation",
        "Authors": "Zeyuan Allen-Zhu, Yuanzhi Li",
        "Counter": 3,
        "Context": [
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5].",
            "In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5]."
        ]
    },
    "6": {
        "Title": "A closer look at memorization in deep networks",
        "Authors": "Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, Simon Lacoste-Julien",
        "Counter": 3,
        "Context": [
            "Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].",
            "Memorization and forgetting are closely related to knowledge acquisition in neural networks [6].",
            "Memorization and forgetting are closely related to knowledge acquisition in neural networks [6]."
        ]
    },
    "8": {
        "Title": "Pythia: A suite for analyzing large language models across training and scaling",
        "Authors": "Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",
        "Counter": 3,
        "Context": [
            "...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8].",
            "...the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be ‘revealed’ in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in §4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge (more knowledge will be presented with an interval shorter than the learnability threshold) since the skewness of the distribution of factual knowledge popularity is likely to be mitigated as the data becomes larger and more diverse.",
            "...the acquisition of such knowledge will be reflected in the model’s top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8]."
        ]
    },
    "13": {
        "Title": "Palm: Scaling language modeling with pathways",
        "Authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",
        "Counter": 3,
        "Context": [
            "Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing.",
            "It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13].",
            "Pretraining with a larger batch size helps LLMs acquire more knowledge."
        ]
    },
    "14": {
        "Title": "Analyzing commonsense emergence in few-shot knowledge models",
        "Authors": "Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut",
        "Counter": 3,
        "Context": [
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40].",
            "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40]."
        ]
    },
    "19": {
        "Title": "Transformer feed-forward layers are key-value memories",
        "Authors": "Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy",
        "Counter": 3,
        "Context": [
            "[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data.",
            "This x-intercept of R(p, t) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in § 4.4.",
            "These patterns are consistent across all pretraining stages of OLMo-7B we investigate (§E.1)."
        ]
    },
    "25": {
        "Title": "Mistral 7b",
        "Authors": "Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed",
        "Counter": 3,
        "Context": [
            "Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing.",
            "It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [25].",
            "Pretraining with a larger batch size helps LLMs acquire more knowledge."
        ]
    },
    "28": {
        "Title": "The bigscience roots corpus: A 1.6 tb composite multilingual dataset",
        "Authors": "Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al.",
        "Counter": 3,
        "Context": [
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ]
    },
    "30": {
        "Title": "Starcoder: may the source be with you!",
        "Authors": "Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm de Vries",
        "Counter": 3,
        "Context": [
            "Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing.",
            "It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [30].",
            "It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing."
        ]
    },
    "35": {
        "Title": "Entity cloze by date: What lms know about unseen entities",
        "Authors": "Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, Greg Durrett",
        "Counter": 3,
        "Context": [
            "An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.",
            "An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases.",
            "An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases."
        ]
    },
    "47": {
        "Title": "D4: Improving llm pretraining via document de-duplication and diversification",
        "Authors": "Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos",
        "Counter": 3,
        "Context": [
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance.",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ]
    },
    "50": {
        "Title": "Emergent abilities of large language models",
        "Authors": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.",
        "Counter": 3,
        "Context": [
            "We suggest a plausible hypothesis based on further observations in §4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training.",
            "Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times.",
            "Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times."
        ]
    },
    "53": {
        "Title": "Critical data size of language models from a grokking perspective",
        "Authors": "Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin",
        "Counter": 3,
        "Context": [
            "Recently, [53] explored the relationship between the data size and grokking [37].",
            "Recently, [53] explored the relationship between the data size and grokking [37].",
            "Recently, [53] explored the relationship between the data size and grokking [37]."
        ]
    },
    "12": {
        "Title": "Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs",
        "Authors": "Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, Naomi Saphra",
        "Counter": 2,
        "Context": [
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51].",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]."
        ]
    },
    "18": {
        "Title": "Does fine-tuning llms on new knowledge encourage hallucinations?",
        "Authors": "Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig",
        "Counter": 2,
        "Context": [
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]."
        ]
    },
    "31": {
        "Title": "How pre-trained language models capture factual knowledge? a causal-inspired analysis",
        "Authors": "Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu",
        "Counter": 2,
        "Context": [
            "The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably.",
            "[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data."
        ]
    },
    "34": {
        "Title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
        "Authors": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi",
        "Counter": 2,
        "Context": [
            "...the failure to acquire long-tail knowledge [26, 34]...",
            "However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]."
        ]
    },
    "39": {
        "Title": "Anatomy of catastrophic forgetting: Hidden representations and task semantics",
        "Authors": "Vinay Venkatesh Ramasesh, Ethan Dyer, Maithra Raghu",
        "Counter": 2,
        "Context": [
            "Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39].",
            "The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]."
        ]
    },
    "44": {
        "Title": "Memorisation versus generalisation in pre-trained language models",
        "Authors": "Michael Tänzer, Sebastian Ruder, Marek Rei",
        "Counter": 2,
        "Context": [
            "[44] and [46] focused on the dynamics of memorization in language model pretraining.",
            "[44] and [46] focused on the dynamics of memorization in language model pretraining."
        ]
    },
    "48": {
        "Title": "Llama: Open and efficient foundation language models",
        "Authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.",
        "Counter": 2,
        "Context": [
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52].",
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ]
    },
    "1": {
        "Title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication",
        "Authors": "Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, Ari S. Morcos",
        "Counter": 1,
        "Context": [
            "as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ]
    },
    "3": {
        "Title": "Tracing knowledge in language models back to the training data",
        "Authors": "Ekin Akyürek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu",
        "Counter": 1,
        "Context": [
            "[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data."
        ]
    },
    "15": {
        "Title": "Knowledge neurons in pretrained transformers",
        "Authors": "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei",
        "Counter": 1,
        "Context": [
            "[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data."
        ]
    },
    "16": {
        "Title": "Measuring causal effects of data statistics on language model’s ’factual’ predictions",
        "Authors": "Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, Yoav Goldberg",
        "Counter": 1,
        "Context": [
            "[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data."
        ]
    },
    "17": {
        "Title": "Does learning require memorization? a short tale about a long tail",
        "Authors": "Vitaly Feldman",
        "Counter": 1,
        "Context": [
            "In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance."
        ]
    },
    "20": {
        "Title": "Dissecting recall of factual associations in auto-regressive language models",
        "Authors": "Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson",
        "Counter": 1,
        "Context": [
            "[3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data."
        ]
    },
    "22": {
        "Title": "Investigating learning dynamics of bert fine-tuning",
        "Authors": "Yaru Hao, Li Dong, Furu Wei, Ke Xu",
        "Counter": 1,
        "Context": [
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]."
        ]
    },
    "32": {
        "Title": "Probing across time: What does RoBERTa know and when?",
        "Authors": "Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith",
        "Counter": 1,
        "Context": [
            "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]."
        ]
    },
    "38": {
        "Title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
        "Counter": 1,
        "Context": [
            "Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]."
        ]
    },
    "11": {
        "Title": "Quantifying memorization across neural language models",
        "Authors": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, Chiyuan Zhang",
        "Counter": 0,
        "Context": []
    },
    "24": {
        "Title": "The surprising simplicity of the early-time learning dynamics of neural networks",
        "Authors": "Wei Hu, Lechao Xiao, Ben Adlam, Jeffrey Pennington",
        "Counter": 0,
        "Context": []
    },
    "37": {
        "Title": "Grokking: Generalization beyond overfitting on small algorithmic datasets",
        "Authors": "Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra",
        "Counter": 0,
        "Context": []
    },
    "42": {
        "Title": "Noise-robust de-duplication at scale",
        "Authors": "Emily Silcock, Luca D’Amico-Wong, Jinglin Yang, Melissa Dell",
        "Counter": 0,
        "Context": []
    },
    "45": {
        "Title": "Emergent structures and training dynamics in large language models",
        "Authors": "Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan",
        "Counter": 0,
        "Context": []
    },
    "51": {
        "Title": "Training trajectories of language models across scales",
        "Authors": "Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov",
        "Counter": 0,
        "Context": []
    }
}